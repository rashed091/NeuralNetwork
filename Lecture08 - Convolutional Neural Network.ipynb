{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\n",
    "\n",
    "<img src=\"images/cnn.jpeg\" width=650>\n",
    "\n",
    "So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n",
    "\n",
    "### Architecture Overview\n",
    "Recall: Regular Neural Nets. As we saw in the previous chapter, Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.\n",
    "\n",
    "Regular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
    "\n",
    "3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. Here is a visualization:\n",
    "\n",
    "\n",
    "### Layers used to build ConvNets\n",
    "As we described above, a simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.\n",
    "\n",
    "Example Architecture: Overview. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:\n",
    "\n",
    "- INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n",
    "- CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "- RELU layer will apply an elementwise activation function, such as the max(0,x) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "- POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "\n",
    "In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n",
    "\n",
    "### Spatial arrangement. \n",
    "We have explained the connectivity of each neuron in the Conv Layer to the input volume, but we haven’t yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding. We discuss these next:\n",
    "\n",
    "1. First, the depth of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre).\n",
    "2. Second, we must specify the stride with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n",
    "3. As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n",
    "\n",
    "We can compute the spatial size of the output volume as a function of the input volume size (W), the receptive field size of the Conv Layer neurons (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. You can convince yourself that the correct formula for calculating how many neurons “fit” is given by (W−F+2P)/S+1. For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. \n",
    "\n",
    "$$\n",
    "Output = \\frac{W - F + 2 * P}{S} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "\n",
    "Let's assume a grayscale image (single channel). A convolution is a function that takes an input 2D image $I(x,y)$ and outputs a filtered imaged $I'(x,y)$ such that $I'(x, y) = f(\\text{neighborhood of } I(x, y))$. This means that in order to compute the value of the output pixel $I'(x, y)$ we will need not only the input pixel value $I(x, y)$ (as we did for brightness) but also its neighboring pixel values. This is not a strict definition but this is the basic idea.\n",
    "\n",
    "Most convolution operations we will be using during the course will use an $n\\text{x}n$ number of neighboring pixels, and will be of the form:\n",
    "\n",
    "$$I'(u,v) = \\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1}{w_{i,j}I(u - \\lfloor n / 2 \\rfloor + i, v-\\lfloor n / 2 \\rfloor + j)} + b$$\n",
    "\n",
    "For the purpose of this lab tutorial $b$ will be zero, so the only parameters of our convolution operations will be the size of our neighborhood region $n$ and the weights $w_{i,j}$. Moreover, for the first few examples here the neighborhod size will be $3\\text{x}3$, thus we will be dealing with the following operation to compute the output pixels $I'(u, v)$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "I'(u,v) =\\quad  &w_{0,0}I(u - 1, v - 1)\\quad  &+\\quad &w_{0,1}I(u - 1, v)\\quad &+\\quad &w_{0,2}I(u - 1, v + 1) \\quad+ \\\\\n",
    "          &w_{1,0}I(u, v - 1)\\quad  &+\\quad &w_{1,1}I(u, v)\\quad &+\\quad &w_{1,2}I(u, v + 1) \\quad+ \\\\\n",
    "          &w_{2,0}I(u + 1, v - 1)\\quad  &+\\quad &w_{2,1}I(u + 1, v)\\quad &+\\quad &w_{2,2}I(u + 1, v + 1) \\quad+\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We are effectively computing a sliding window as illustrated here:\n",
    "\n",
    "<img src=\"images/animation.gif\" style=\"width:520px\"/>\n",
    "\n",
    "In practice, there are various ways of making the convolution operations faster by realizing that some computations are shared by consecutive windows, or by sacrificing memory and expressing the convolution as a single matrix multiplication, GPU acceleration is also possible and fortunately all of these are already implemented in pytorch. Convolutions are essential to obtain information from images as well as to apply many common effects to images like blurring, sharpening, etc. In pytorch we can use F.conv2d function to an input image.\n",
    "\n",
    "Most image processing libraries implement a convolution operation, they are also referred to as image filtering operations in some libraries including PIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Filters\n",
    "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevant features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as one-hot encoded vector which gives us our 2D input. The intuition here is that each filter represents a feature and we will use this filter on other inputs to capture the same feature. This is known as parameter sharing.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/madewithml/images/master/02_Basics/07_Convolutional_Neural_Networks/convolution.gif\" width=\"500\">\n",
    "\n",
    "Our inputs are a batch of 2D text data. Let's make an input with 64 samples, where each sample has 8 words and each word is represented by a array of 10 values (one hot encoded with vocab size of 10). This gives our inputs the size (64, 8, 10). The [PyTorch CNN modules](https://pytorch.org/docs/stable/nn.html#convolution-functions) prefer inputs to have the channel dim (one hot vector dim in our case) to be in the second position, so our inputs are of shape (64, 10, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Assume all our inputs have the same # of words\n",
    "batch_size = 64\n",
    "sequence_size = 8 # words per input\n",
    "one_hot_size = 10 # vocab size (num_input_channels)\n",
    "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
    "print(\"Size: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convolve on this input using filters. For simplicity we will use just 5 filters that is of size (1, 2) and has the same depth as the number of channels (one_hot_size). This gives our filter a shape of (5, 2, 10) but recall that PyTorch CNN modules prefer to have the channel dim (one hot vector dim in our case) to be in the second position so the filter is of shape (5, 10, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([5, 10, 2])\n",
      "Filter size: 2\n",
      "Padding: 0\n",
      "Stride: 1\n"
     ]
    }
   ],
   "source": [
    "# Create filters for a conv layer\n",
    "out_channels = 5 # of filters\n",
    "kernel_size = 2 # filters size 2\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "print(\"Size: {}\".format(conv1.weight.shape))\n",
    "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
    "print(\"Padding: {}\".format(conv1.padding[0]))\n",
    "print(\"Stride: {}\".format(conv1.stride[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride and padding\n",
    "When we apply this filter on our inputs, we receive an output of shape (64, 5, 7). We get 64 for the batch size, 5 for the channel dim because we used 5 filters and 7 for the conv outputs because:\n",
    "\n",
    "$\\frac{W - F + 2P}{S} + 1 = \\frac{8 - 2 + 2(0)}{1} + 1 = 7$\n",
    "\n",
    "where:\n",
    "  * W: width of each input\n",
    "  * F: filter size\n",
    "  * P: padding\n",
    "  * S: stride\n",
    " \n",
    "\n",
    "#### Padding types:\n",
    "* **VALID**: no padding, the filters only use the \"valid\" values in the input. If the filter cannot reach all the input values (filters go left to right), the extra values on the right are dropped.\n",
    "* **SAME**: adds padding evenly to the right (preferred) and left sides of the input so that all values in the input are processed.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/madewithml/images/master/02_Basics/07_Convolutional_Neural_Networks/padding.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "# Convolve using filters\n",
    "conv_output = conv1(x)\n",
    "print(\"Size: {}\".format(conv_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "In addition to strides, another way to reduce the size of feature maps between the convolutional layers, is by adding **pooling** layers. The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field. \n",
    "\n",
    "A pooling layer has the following hyperparameters (but **no trainable parameters**):\n",
    "\n",
    "1. Spatial extent (size) of each pooling kernel, $F \\geq 2$. \n",
    "1. Stride $S\\geq 2$: spatial distance between consecutive applications.\n",
    "1. Operation (e.g. max, average, $p$-norm)\n",
    "\n",
    "**Example**: $\\max$-pooling with $F=2,~S=2$ performing a factor-2 downsample:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/madewithml/images/master/02_Basics/07_Convolutional_Neural_Networks/pooling.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Max pooling\n",
    "kernel_size = 2\n",
    "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "pool_output = pool1(conv_output)\n",
    "print(\"Size: {}\".format(pool_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{W-F}{S} + 1 = \\frac{7-2}{2} + 1 =  \\text{floor }(2.5) + 1 = 3$\n",
    "\n",
    "### Hyperparameters & dimentions\n",
    "\n",
    "Assume an input volume of shape $(C_{\\mathrm{in}}, H_{\\mathrm{in}}, W_{\\mathrm{in}})$, i.e. channels, height, width.\n",
    "Define,\n",
    "\n",
    "1. Number of kernels, $K \\geq 1$.\n",
    "2. Spatial extent (size) of each kernel, $F \\geq 1$. \n",
    "3. Stride $S\\geq 1$: spatial distance between consecutive applications of a kernel.\n",
    "4. Padding $P\\geq 0$: Number of \"pixels\" to zero-pad around each input feature map.\n",
    "5. Dilation $D \\geq 1$: Spacing between kernel elements when applying to input.\n",
    "\n",
    "In the following animations, **blue** maps are inputs,\n",
    "**green** maps are outputs and\n",
    "the **shaded** area is the kernel with $F=3$.\n",
    "\n",
    "| $P=0,~S=1,~D=1$ | $P=1,~S=1,~D=1$ | $P=1,~S=2,~D=1$ | $P=0,~S=1,~D=2$ |\n",
    "|-----------------|-----------------|-----------------| --------------- |\n",
    "|<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"250\"/>| <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" width=\"250\"/> |\n",
    "\n",
    "\n",
    "We can see that the second combination, $F=3,~P=1,~S=1,~D=1$, leads to identical sizes of input and output feature maps.\n",
    "\n",
    "Then, given a set of hyperparameters,\n",
    "\n",
    "- Each convolution kernel will (usually) be a tensor of shape $(C_{\\mathrm{in}}, F, F)$.\n",
    "- The ouput volume dimensions will be:\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{\\mathrm{out}} &= \\left\\lfloor \\frac{H_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  W_{\\mathrm{out}} &= \\left\\lfloor \\frac{W_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  C_{\\mathrm{out}} &= K\\\\\n",
    "  \\end{align}$$\n",
    "  \n",
    "- The number of parameters in a convolutional layer will be:\n",
    "\n",
    "$$\n",
    "\\underbrace{K}_{\\mathrm{kernels}} \\cdot \\left(\n",
    "\\underbrace{C_{\\mathrm{in}} \\cdot F^2}_{\\mathrm{kernel\\ size}} + \\underbrace{1}_{\\mathrm{bias\\ term}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "**Example**: Input image is 1000x1000x3, and the first conv layer has $10$ kernels of size 5x5.\n",
    "The number of parameters in the first layer will be: $ 10 \\cdot 3 \\cdot 5^2 + 10 = 760 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Convolutional Neural Network with PyTorch\n",
    "\n",
    "Output Formula for Convolution\n",
    "    \n",
    "- $O = \\frac {W - K + 2P}{S} + 1$\n",
    "- $O$: output height/length\n",
    "- $W$: input height/length\n",
    "- $K$: filter size (kernel size) = 5\n",
    "- $P$: same padding (non-zero)\n",
    "- $P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2$\n",
    "- $S$: stride = 1\n",
    "\n",
    "Output Formula for Pooling\n",
    "\n",
    "- $O = \\frac {W - K}{S} + 1$\n",
    "- W: input height/width\n",
    "- K: filter size = 2\n",
    "- S: stride size = filter size, PyTorch defaults the stride to kernel filter size\n",
    "\n",
    "If using PyTorch default stride, this will result in the formula $O = \\frac {W}{K}$\n",
    "By default, in our tutorials, we do this for simplicity.\n",
    "\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/cnn10-2n.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = data.MNIST(root='./data', train=True,\n",
    "                           transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = data.MNIST(root='./data', train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 1000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x11d7ed678>\n",
      "6\n",
      "torch.Size([16, 1, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([32, 16, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 1568])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# Convolution 1: 16 Kernels\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# Convolution 1 Bias: 16 Kernels\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# Convolution 2: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# Convolution 2 Bias: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[3].size())\n",
    "\n",
    "# Fully Connected Layer 1\n",
    "print(list(model.parameters())[4].size())\n",
    "\n",
    "# Fully Connected Layer Bias\n",
    "print(list(model.parameters())[5].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Loss: 0.2117665410041809. Accuracy: 94\n",
      "Iteration: 200. Loss: 0.13519540429115295. Accuracy: 95\n",
      "Iteration: 300. Loss: 0.07478510588407516. Accuracy: 96\n",
      "Iteration: 400. Loss: 0.0675332322716713. Accuracy: 97\n",
      "Iteration: 500. Loss: 0.074933260679245. Accuracy: 97\n",
      "Iteration: 600. Loss: 0.2683143615722656. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images\n",
    "                images = images.requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(\n",
    "                iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
    "\n",
    "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
    "\n",
    "where:\n",
    "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
    "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $epsilon$ = noise\n",
    "* $\\gamma$ = scale parameter (learned parameter)\n",
    "* $\\beta$ = shift parameter (learned parameter)\n",
    "\n",
    "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_output_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3074,  1.6045, -0.2154,  0.1486, -0.5794,  0.8766,  0.1486],\n",
       "          [-1.6714, -0.9434, -1.3074,  1.6045,  1.2405,  0.8766, -0.9434],\n",
       "          [ 0.8766,  0.1486, -0.2154,  0.1486, -0.9434,  1.6045,  1.2405],\n",
       "          [ 0.1486, -0.5794,  0.1486,  1.2405, -1.3074, -0.2154, -0.2154],\n",
       "          [-1.6714,  1.6045,  1.6045, -0.9434, -1.3074, -0.2154,  0.8766],\n",
       "          [ 0.1486,  1.2405, -0.9434, -0.2154, -1.3074,  0.8766, -0.2154],\n",
       "          [ 1.6045,  0.1486, -0.9434, -1.3074, -0.2154,  0.5126, -0.9434]],\n",
       "\n",
       "         [[-0.0284, -1.0737,  0.6684, -1.4221, -0.3768, -0.7252, -1.4221],\n",
       "          [ 1.3652,  1.7136,  0.3200, -0.0284,  0.6684,  0.3200, -1.0737],\n",
       "          [-1.0737, -1.4221, -0.7252,  1.0168,  1.7136, -0.3768, -0.0284],\n",
       "          [-1.4221,  0.6684,  1.0168, -0.3768, -1.4221, -1.4221, -0.3768],\n",
       "          [-1.0737,  1.0168, -0.0284, -1.4221,  0.6684, -0.0284,  1.7136],\n",
       "          [ 1.0168,  1.7136,  1.0168, -0.7252,  0.6684,  1.7136, -0.7252],\n",
       "          [ 0.3200, -0.3768,  0.6684,  0.3200, -0.3768, -1.4221,  0.6684]],\n",
       "\n",
       "         [[ 0.0415,  1.2611,  0.8545,  1.6676,  0.0415, -1.1781, -0.3650],\n",
       "          [ 0.0415,  1.2611,  0.0415,  2.0741,  0.0415,  1.2611, -0.3650],\n",
       "          [ 0.0415,  0.8545,  0.4480,  0.4480, -1.5846,  0.0415,  1.2611],\n",
       "          [ 1.2611, -1.5846, -1.1781, -1.1781, -0.3650,  0.0415,  1.2611],\n",
       "          [-1.1781,  0.4480, -0.3650,  0.8545, -1.1781,  0.0415,  0.0415],\n",
       "          [-0.7716, -1.5846,  0.0415,  0.8545, -0.3650, -1.5846,  0.0415],\n",
       "          [ 0.8545, -1.1781, -0.3650, -1.5846, -0.3650,  2.0741, -1.1781]]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm2d(3, affine=False)\n",
    "m(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting (pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/dropout.jpeg\" style=\"border:none;width:70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000, 11.2500,  5.0000,  0.0000,  3.7500,  8.7500,  6.2500],\n",
       "          [ 0.0000,  2.5000,  1.2500,  0.0000, 10.0000,  8.7500,  2.5000],\n",
       "          [ 8.7500,  0.0000,  5.0000,  6.2500,  2.5000, 11.2500, 10.0000],\n",
       "          [ 6.2500,  0.0000,  6.2500, 10.0000,  1.2500,  5.0000,  0.0000],\n",
       "          [ 0.0000, 11.2500, 11.2500,  2.5000,  1.2500,  5.0000,  8.7500],\n",
       "          [ 6.2500, 10.0000,  2.5000,  5.0000,  1.2500,  8.7500,  0.0000],\n",
       "          [11.2500,  0.0000,  2.5000,  0.0000,  5.0000,  7.5000,  2.5000]],\n",
       "\n",
       "         [[ 5.0000,  1.2500,  0.0000,  0.0000,  3.7500,  2.5000,  0.0000],\n",
       "          [10.0000, 11.2500,  6.2500,  5.0000,  7.5000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  8.7500, 11.2500,  0.0000,  5.0000],\n",
       "          [ 0.0000,  7.5000,  8.7500,  3.7500,  0.0000,  0.0000,  3.7500],\n",
       "          [ 1.2500,  8.7500,  5.0000,  0.0000,  7.5000,  5.0000, 11.2500],\n",
       "          [ 8.7500, 11.2500,  8.7500,  2.5000,  0.0000, 11.2500,  0.0000],\n",
       "          [ 6.2500,  3.7500,  7.5000,  6.2500,  0.0000,  0.0000,  7.5000]],\n",
       "\n",
       "         [[ 5.0000,  0.0000,  7.5000, 10.0000,  5.0000,  0.0000,  0.0000],\n",
       "          [ 5.0000,  8.7500,  5.0000,  0.0000,  0.0000,  8.7500,  3.7500],\n",
       "          [ 5.0000,  0.0000,  6.2500,  0.0000,  0.0000,  0.0000,  8.7500],\n",
       "          [ 8.7500,  0.0000,  1.2500,  1.2500,  3.7500,  5.0000,  8.7500],\n",
       "          [ 1.2500,  6.2500,  3.7500,  7.5000,  1.2500,  5.0000,  5.0000],\n",
       "          [ 2.5000,  0.0000,  0.0000,  7.5000,  3.7500,  0.0000,  5.0000],\n",
       "          [ 7.5000,  0.0000,  3.7500,  0.0000,  3.7500, 11.2500,  1.2500]]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "m(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
