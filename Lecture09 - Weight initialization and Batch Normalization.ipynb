{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "In this lesson, you'll learn how to find good initial weights for a neural network. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker. \n",
    "\n",
    "\n",
    "### Dataset\n",
    "To see how different weights perform, we'll test on the same dataset and neural network. Let's go over the dataset and neural network.\n",
    "\n",
    "We'll be using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) to demonstrate the different initial weights. As a reminder, the MNIST dataset contains images of handwritten numbers, 0-9, with normalized input (0.0 - 1.0).  Run the cell below to download and load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data so we have something for our experiments\n",
    "train_dataset = torchvision.datasets.MNIST(root='.data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_dist(title, values, hist_range=(-4, 4)):\n",
    "    plt.title(title)\n",
    "    plt.hist(values, np.linspace(*hist_range, num=len(values)/2))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Weight Initialization\n",
    "\n",
    "PyTorch layers are initialized by default in their respective `reset_parameters()` method. For example:\n",
    "\n",
    "- `nn.Linear`\n",
    "    - `weight` and `bias`: uniform distribution [-limit, +limit] where `limit` is `1. / sqrt(fan_in)` and `fan_in` is the number of input units in the weight tensor.\n",
    "- `nn.Conv2D`\n",
    "    - `weight` and `bias`: uniform distribution [-limit, +limit] where `limit` is `1. / sqrt(fan_in)` and `fan_in` is the number of input units in the weight tensor.\n",
    "\n",
    "With this implementation, the variance of the layer outputs is equal to `Var(W) = 1 / 3 * sqrt(fan_in)` which isn't the best initialization strategy out there.\n",
    "\n",
    "Note that PyTorch provides convenience functions for some of the initializations. The input and output shapes are computed using the method `_calculate_fan_in_and_fan_out()` and a `gain()` method scales the standard deviation to suit a particular activation.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu')) \n",
    "```\n",
    "\n",
    "#### Xavier Initialization\n",
    "\n",
    "This initialization is general-purpose and meant to \"work\" pretty well for any activation in practice.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform(m.weight)\n",
    "```\n",
    "\n",
    "You can tailor this initialization to your specific activation by using the `nn.init.calculate_gain(act)` argument.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform(m.weight(), gain=nn.init.calculate_gain('relu'))\n",
    "```\n",
    "\n",
    "- [arXiv](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "#### He et. al Initialization\n",
    "\n",
    "This is a similarly derived initialization tailored specifically for ReLU activations since they do not exhibit zero mean.\n",
    "\n",
    "```python\n",
    "# he initialization\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal(m.weight, mode='fan_in')\n",
    "```\n",
    "\n",
    "For `mode=fan_in`, the variance of the distribution is ensured in the forward pass, while for `mode=fan_out`, it is ensured in the backwards pass.\n",
    "\n",
    "- [arXiv](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "#### SELU Initialization\n",
    "\n",
    "Again, this initialization is specifically derived for the SELU activation function. The authors use the `fan_in` strategy. They mention that there is no significant difference between sampling from a Gaussian, a truncated Gaussian or a Uniform distribution.\n",
    "\n",
    "```python\n",
    "# selu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "        nn.init.normal(m.weight, 0, sqrt(1. / fan_in))\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        fan_in = m.in_features\n",
    "        nn.init.normal(m.weight, 0, sqrt(1. / fan_in))\n",
    "```\n",
    "\n",
    "- [arXiv](https://arxiv.org/abs/1706.02515)\n",
    "\n",
    "#### Orthogonal Initialization\n",
    "\n",
    "Orthogonality is a desirable quality in NN weights in part because it is norm preserving, i.e. it rotates the input matrix, but cannot change its norm (scale/shear). This property is valuable in deep or recurrent networks, where repeated matrix multiplication can result in signals vanishing or exploding.\n",
    "\n",
    "```python\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.orthogonal(m.weight)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_init_weights(dataset, title, weight_init_list, plot_n_batches=100):\n",
    "    \"\"\"\n",
    "    Plot loss and print stats of weights using an example neural network\n",
    "    \"\"\"\n",
    "    colors = ['r', 'b', 'g', 'c', 'y', 'k']\n",
    "    label_accs = []\n",
    "    label_loss = []\n",
    "\n",
    "    assert len(weight_init_list) <= len(colors), 'Too many inital weights to plot'\n",
    "\n",
    "    for i, (weights, label) in enumerate(weight_init_list):\n",
    "        loss, val_acc = _get_loss_acc(dataset, weights)\n",
    "\n",
    "        plt.plot(loss[:plot_n_batches], colors[i], label=label)\n",
    "        label_accs.append((label, val_acc))\n",
    "        label_loss.append((label, loss[-1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    print('After 858 Batches (2 Epochs):')\n",
    "    print('Validation Accuracy')\n",
    "    for label, val_acc in label_accs:\n",
    "        print('  {:7.3f}% -- {}'.format(val_acc*100, label))\n",
    "    print('Loss')\n",
    "    for label, loss in label_loss:\n",
    "        print('  {:7.3f}  -- {}'.format(loss, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network, we'll test on a 3 layer neural network with ReLU activations and an Adam optimizer.  The lessons you learn apply to other neural networks, including different activations and optimizers.\n",
    "\n",
    "## Initialize Weights\n",
    "Let's start looking at some initial weights.\n",
    "### All Zeros or Ones\n",
    "If you follow the principle of [Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor), you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.\n",
    "\n",
    "With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.\n",
    "\n",
    "Let's compare the loss with all ones and all zero weights using `helper.compare_init_weights`.  This function will run two different initial weights on the neural network above for 2 epochs.  It will plot the loss for the first 100 batches and print out stats after the 2 epochs (~860 batches). We plot the first 100 batches to better judge which weights performed better at the start.\n",
    "\n",
    "Run the cell below to see the difference between weights of all zeros against all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.Tensor(1000).uniform_(-3, 3)\n",
    "\n",
    "hist_dist('Distribution', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the accuracy is close to guessing for both zeros and ones, around 10%.\n",
    "\n",
    "The neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer.  To avoid neurons with the same output, let's use unique weights.  We can also randomly select these weights to avoid being stuck in a local minimum for each run.\n",
    "\n",
    "A good solution for getting these random weights is to sample from a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "A [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous%29) has the equal probability of picking any number from a set of numbers. We'll be picking from a continous distribution, so the chance of picking the same number is low. We'll use TensorFlow's `tf.random_uniform` function to pick random numbers from a uniform distribution.\n",
    "\n",
    ">#### [`torch.Tensor.uniform`](https://www.tensorflow.org/api_docs/python/tf/random_uniform)\n",
    ">Outputs random values from a uniform distribution.\n",
    "\n",
    ">The generated values follow a uniform distribution in the range [minval, maxval). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n",
    "\n",
    ">- **shape:** A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "- **minval:** A 0-D Tensor or Python value of type dtype. The lower bound on the range of random values to generate. Defaults to 0.\n",
    "- **maxval:** A 0-D Tensor or Python value of type dtype. The upper bound on the range of random values to generate. Defaults to 1 if dtype is floating point.\n",
    "- **dtype:** The type of the output: float32, float64, int32, or int64.\n",
    "- **seed:** A Python integer. Used to create a random seed for the distribution. See tf.set_random_seed for behavior.\n",
    "- **name:** A name for the operation (optional).\n",
    "\n",
    "We can visualize the uniform distribution by using a histogram. Let's map the values from `tf.random_uniform([1000], -3, 3)` to a histogram using the `helper.hist_dist` function. This will be `1000` random float values from `-3` to `3`, excluding the value `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = torch.Tensor(1000).uniform_(-3, 3)\n",
    "\n",
    "hist_dist('Distribution', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram used 500 buckets for the 1000 values.  Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That's exactly what we see with the histogram.  Some buckets have more and some have less, but they trend around 2.\n",
    "\n",
    "Now that you understand the `tf.random_uniform` function, let's apply it to some initial weights.\n",
    "\n",
    "### Baseline\n",
    "\n",
    "\n",
    "Let's see how well the neural network trains using the default values for `tf.random_uniform`, where `minval=0.0` and `maxval=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss graph is showing the neural network is learning, which it didn't with all zeros or all ones. We're headed in the right direction.\n",
    "\n",
    "### General rule for setting weights\n",
    "The general rule for setting the weights in a neural network is to be close to zero without being too small. A good pracitce is to start your weights in the range of $[-y, y]$ where\n",
    "$y=1/\\sqrt{n}$ ($n$ is the number of inputs to a given neuron).\n",
    "\n",
    "Let's see if this holds true, let's first center our range over zero.  This will give us the range [-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too small\n",
    "Let's compare [-0.1, 0.1), [-0.01, 0.01), and [-0.001, 0.001) to see how small is too small.  We'll also set `plot_n_batches=None` to show all the batches in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range we found and $y=1/\\sqrt{n}$ are really close.\n",
    "\n",
    "Since the uniform distribution has the same chance to pick anything in the range, what if we used a distribution that had a higher chance of picking numbers closer to 0.  Let's look at the normal distribution.\n",
    "### Normal Distribution\n",
    "Unlike the uniform distribution, the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) has a higher likelihood of picking number close to it's mean. To visualize it, let's plot values from TensorFlow's `torch.Tensor.normal_` function to a histogram.\n",
    "\n",
    ">[torch.Tensor.normal_](https://www.tensorflow.org/api_docs/python/tf/random_normal)\n",
    "\n",
    ">Outputs random values from a normal distribution.\n",
    "\n",
    ">- **shape:** A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "- **mean:** A 0-D Tensor or Python value of type dtype. The mean of the normal distribution.\n",
    "- **stddev:** A 0-D Tensor or Python value of type dtype. The standard deviation of the normal distribution.\n",
    "- **dtype:** The type of the output.\n",
    "- **seed:** A Python integer. Used to create a random seed for the distribution. See tf.set_random_seed for behavior.\n",
    "- **name:** A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = torch.Tensor(1000).normal_(-3, 3)\n",
    "\n",
    "hist_dist('Distribution', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution gave a slight increasse in accuracy and loss.  Let's move closer to 0 and drop picked numbers that are `x` number of standard deviations away.  This distribution is called [Truncated Normal Distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution%29).\n",
    "### Truncated Normal Distribution\n",
    ">[tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)\n",
    "\n",
    ">Outputs random values from a truncated normal distribution.\n",
    "\n",
    ">The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "\n",
    ">- **shape:** A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
    "- **mean:** A 0-D Tensor or Python value of type dtype. The mean of the truncated normal distribution.\n",
    "- **stddev:** A 0-D Tensor or Python value of type dtype. The standard deviation of the truncated normal distribution.\n",
    "- **dtype:** The type of the output.\n",
    "- **seed:** A Python integer. Used to create a random seed for the distribution. See tf.set_random_seed for behavior.\n",
    "- **name:** A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Batch Normalization?<a id='theory'></a>\n",
    "\n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. It's called \"batch\" normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current mini-batch.\n",
    "\n",
    "Why might this help? Well, we know that normalizing the inputs to a _network_ helps the network learn. But a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the _first_ layer of a smaller network.\n",
    "\n",
    "For example, imagine a 3 layer network. Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network. \n",
    "\n",
    "Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.\n",
    "\n",
    "When you think of it like that - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at every layer (sub-network).\n",
    "\n",
    "Beyond the intuitive reasons, there are good mathematical reasons why it helps the network learn better, too. It helps combat what the authors call _internal covariate shift_. This discussion is best handled [in the paper](https://arxiv.org/pdf/1502.03167.pdf) and in [Deep Learning](http://www.deeplearningbook.org) a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of [Chapter 8: Optimization for Training Deep Models](http://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "### Benefits of Batch Normalization<a id=\"benefits\"></a>\n",
    "\n",
    "Batch normalization optimizes network training. It has been shown to have several benefits:\n",
    "1. **Networks train faster** – Each training _iteration_ will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall. \n",
    "2. **Allows higher learning rates** – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train. \n",
    "3. **Makes weights easier to initialize** – Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.  \n",
    "4. **Makes more activation functions viable** – Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.  \n",
    "5. **Simplifies the creation of deeper networks** – Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.\n",
    "6. **Provides a bit of regularlization** – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network. \n",
    "7. **May give better results overall** – Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
    "\n",
    "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
    "\n",
    "where:\n",
    "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
    "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $epsilon$ = noise\n",
    "* $\\gamma$ = scale parameter (learned parameter)\n",
    "* $\\beta$ = shift parameter (learned parameter)\n",
    "\n",
    "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_output_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exmaple\n",
    "\n",
    "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
    "\n",
    "The following cell imports the packages we need in the notebook and loads the MNIST dataset to use in our experiments. However, the `pytorch` package contains all the code you'll actually need for batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
