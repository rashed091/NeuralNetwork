{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "In this lesson, you'll learn how to find good initial weights for a neural network. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker. \n",
    "\n",
    "\n",
    "### Dataset\n",
    "To see how different weights perform, we'll test on the same dataset and neural network. Let's go over the dataset and neural network.\n",
    "\n",
    "We'll be using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) to demonstrate the different initial weights. As a reminder, the MNIST dataset contains images of handwritten numbers, 0-9, with normalized input (0.0 - 1.0).  Run the cell below to download and load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data so we have something for our experiments\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_dist(title, values, hist_range=(-4, 4)):\n",
    "    plt.title(title)\n",
    "    plt.hist(values, np.linspace(*hist_range, num=len(values)/2))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAECdJREFUeJzt3Xms5WV9x/H3xxksIq6ZW6uAXqyESKhbbq2WLhawHYVC29iKEYpbJk2roqWxIK5dSTHUNrY1E6S4EGyLGhWrgopBE4veQVBg3II4DLJcalRAWx359o9zpl6vdznL795z5pn3K7mZ81vO83zP9pnfec7vnCdVhSRp33e/SRcgSeqGgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXVMtyVuTvLajth6d5J4km/rLn0zyki7a7rf34SSnd9WeNKzNky5A+7ckNwOPAPYAPwJuBN4BbK+q+6rqj4Zo5yVV9bGV9qmqXcDB49bc7+8NwOOq6tRF7T+ri7alUXmErmnw21X1IOAxwLnAnwNv67KDJB68qHkGuqZGVX2nqj4APBc4PcnRSS5K8lcASbYkuSzJt5N8K8mnktwvyTuBRwMf7A+pvCrJbJJK8uIku4BPLFq3ONx/Pslnk3w3yfuTPLzf1zOS7F5cX5KbkxyfZCvwauC5/f6u62///yGcfl2vSfKNJHcmeUeSh/S37a3j9CS7ktyV5Jz1vXe1PzDQNXWq6rPAbuBXl2w6s79+ht4wzat7u9dpwC56R/oHV9XfLbrOrwOPB35rhe7+EHgR8Eh6wz7/OEB9HwH+Bvi3fn9PXGa3F/T/fgN4LL2hnrcs2edXgCOB44DXJXn8Wn1LqzHQNa2+CTx8ybof0gvex1TVD6vqU7X2jxG9oarurarvr7D9nVV1fVXdC7wW+IO9H5qO6fnA+VV1U1XdA5wNnLLk3cEbq+r7VXUdcB2w3H8M0sAMdE2rQ4BvLVl3HvA14PIkNyU5a4B2bhli+zeAA4AtA1e5skf121vc9mZ67yz2un3R5e/R0Qe22n8Z6Jo6SX6RXqB/evH6qrq7qs6sqscCJwF/muS4vZtXaG6tI/jDFl1+NL13AXcB9wIHLappE72hnkHb/Sa9D3kXt70HuGON60kjM9A1NZI8OMmJwLuBd1XVF5dsPzHJ45IE+A690xzv62++g95Y9bBOTXJUkoOAvwAuraofAV8BDkxyQpIDgNcAP7PoencAs0lWeg1dArwyyeFJDubHY+57RqhRGoiBrmnwwSR30xv+OAc4H3jhMvsdAXwMuAf4DPDPVXVlf9vfAq/pnwHzZ0P0/U7gInrDHwcCL4feGTfAHwMXALfSO2JffNbLf/T//e8k1yzT7oX9tq8Cvg78D/CyIeqShhYnuJCkNniELkmNMNAlqREGuiQ1wkCXpEZs6A8WbdmypWZnZzeyS0na5+3YseOuqppZa78NDfTZ2Vnm5+c3sktJ2ucl+cbaeznkIknNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhqxZqAnubA/J+L1i9Y9PMkVSb7a//dh61umJGktgxyhXwRsXbLuLODjVXUE8PH+siRpgtYM9Kq6ip+eCuxk4O39y28HfqfjuiRJQxp1DP0RVXVb//Lt/OQ8iT8hybYk80nmFxYWRuxOkrSWsT8U7c+6vuIsGVW1varmqmpuZmbNnyKQJI1o1EC/I8kjAfr/3tldSZKkUYwa6B8ATu9fPh14fzflSJJGNchpi5fQm5D3yCS7k7wYOBd4ZpKvAsf3lyVJE7Tmz+dW1fNW2HRcx7VIksbgN0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRgr0JO8MskNSa5PckmSA7sqTJI0nJEDPckhwMuBuao6GtgEnNJVYZKk4Yw75LIZeECSzcBBwDfHL0mSNIqRA72qbgXeBOwCbgO+U1WXL90vybYk80nmFxYWRq9UkrSqcYZcHgacDBwOPAp4YJJTl+5XVduraq6q5mZmZkavVJK0qnGGXI4Hvl5VC1X1Q+C9wC93U5YkaVjjBPou4GlJDkoS4DhgZzdlSZKGNc4Y+tXApcA1wBf7bW3vqC5J0pA2j3Plqno98PqOapEkjcFvikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA70Ds2d9aNIlaB0M+7gu3n9/fk7sz7d90gx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ijxgr0JA9NcmmSLyXZmeTpXRUmSRrO5jGv/w/AR6rqOUnuDxzUQU2SpBGMHOhJHgL8GvACgKr6AfCDbsqSJA1rnCGXw4EF4F+TfD7JBUke2FFdkqQhjRPom4GnAP9SVU8G7gXOWrpTkm1J5pPMLywsjNHdvm0aJ85draZB69273zTevrVs5KTO++L9M6gunkfqxjiBvhvYXVVX95cvpRfwP6GqtlfVXFXNzczMjNGdJGk1Iwd6Vd0O3JLkyP6q44AbO6lKkjS0cc9yeRlwcf8Ml5uAF45fkiRpFGMFelVdC8x1VIskaQx+U1SSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOhTbvasDzkvY9+w85eu55yhGz2XaivPgUFuRyu3dRIMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrE2IGeZFOSzye5rIuCJEmj6eII/QxgZwftSJLGMFagJzkUOAG4oJtyJEmjGvcI/c3Aq4D7VtohybYk80nmFxYWxuxu+nUxefCwkyCvtP9y69d7cuPF7Q8zwfW49WxUP132sdrjM+r1Vrrfh7ndw0yu3cXjttbzd63b4qTSPzZyoCc5Ebizqnastl9Vba+quaqam5mZGbU7SdIaxjlCPwY4KcnNwLuBY5O8q5OqJElDGznQq+rsqjq0qmaBU4BPVNWpnVUmSRqK56FLUiM2d9FIVX0S+GQXbUmSRuMRuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0Duy3MS6g074O+ikvINuW2ty5uX2HaWGrifrXe5+W62PlbYNun6t9gdtYzXj9Ll021qXu659uXYGbWuQCcJHuQ1r9be/TxhtoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGjFyoCc5LMmVSW5MckOSM7osTJI0nM1jXHcPcGZVXZPkQcCOJFdU1Y0d1SZJGsLIR+hVdVtVXdO/fDewEzikq8IkScPpZAw9ySzwZODqZbZtSzKfZH5hYaGL7qbKIHNxDjpf57D9dbVttf0Wz9U4yPyWqy0vbWfceSQH3Wet66w27+eo28Zpf7X1S/fp8vk07OM77HNqmFrXml/WOUSXN3agJzkYeA/wiqr67tLtVbW9quaqam5mZmbc7iRJKxgr0JMcQC/ML66q93ZTkiRpFOOc5RLgbcDOqjq/u5IkSaMY5wj9GOA04Ngk1/b/nt1RXZKkIY182mJVfRpIh7VIksbgN0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqRKpqwzqbm5ur+fn5DetvI4wzSe3N556w4vVvPveEFdtf7Xrj7DuoLtrc28bSttZavxFW6mvY9eP2u1y74/a1nrdhIx6jQfrY+9ppSZIdVTW31n4eoUtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ijxgr0JFuTfDnJ15Kc1VVRkqThjRzoSTYB/wQ8CzgKeF6So7oqTJI0nHGO0J8KfK2qbqqqHwDvBk7upixJ0rBGniQ6yXOArVX1kv7yacAvVdVLl+y3DdjWXzwS+PKItW4B7hrxuuvJuoZjXcOxruG0WtdjqmpmrZ02j9HBQKpqO7B93HaSzA8y6/VGs67hWNdwrGs4+3td4wy53Aoctmj50P46SdIEjBPonwOOSHJ4kvsDpwAf6KYsSdKwRh5yqao9SV4KfBTYBFxYVTd0VtlPG3vYZp1Y13CsazjWNZz9uq6RPxSVJE0XvykqSY0w0CWpEftkoCc5M0kl2TLpWgCS/GWSLyS5NsnlSR416ZoAkpyX5Ev92t6X5KGTrgkgye8nuSHJfUkmforZNP6ERZILk9yZ5PpJ17JYksOSXJnkxv5jeMakawJIcmCSzya5rl/XGydd02JJNiX5fJLL1rOffS7QkxwG/Cawa9K1LHJeVT2hqp4EXAa8btIF9V0BHF1VTwC+Apw94Xr2uh74PeCqSRcyxT9hcRGwddJFLGMPcGZVHQU8DfiTKbm//hc4tqqeCDwJ2JrkaROuabEzgJ3r3ck+F+jA3wOvAqbm09yq+u6ixQcyJbVV1eVVtae/+F/0viswcVW1s6pG/cZw16byJyyq6irgW5OuY6mquq2qrulfvpteSB0y2aqgeu7pLx7Q/5uK12GSQ4ETgAvWu699KtCTnAzcWlXXTbqWpZL8dZJbgOczPUfoi70I+PCki5hChwC3LFrezRQE1L4gySzwZODqyVbS0x/WuBa4E7iiqqaiLuDN9A5C71vvjtb9q//DSvIx4OeW2XQO8Gp6wy0bbrW6qur9VXUOcE6Ss4GXAq+fhrr6+5xD763yxRtR06B1ad+V5GDgPcArlrxDnZiq+hHwpP5nRe9LcnRVTfQziCQnAndW1Y4kz1jv/qYu0Kvq+OXWJ/kF4HDguiTQGz64JslTq+r2SdW1jIuB/2SDAn2tupK8ADgROK428EsHQ9xfk+ZPWAwpyQH0wvziqnrvpOtZqqq+neRKep9BTPpD5WOAk5I8GzgQeHCSd1XVqevR2T4z5FJVX6yqn62q2aqapffW+CkbEeZrSXLEosWTgS9NqpbFkmyl91bvpKr63qTrmVL+hMUQ0juaehuws6rOn3Q9eyWZ2XsWV5IHAM9kCl6HVXV2VR3az6xTgE+sV5jDPhToU+7cJNcn+QK9IaGpOJULeAvwIOCK/imVb510QQBJfjfJbuDpwIeSfHRStfQ/NN77ExY7gX9f55+wGEiSS4DPAEcm2Z3kxZOuqe8Y4DTg2P5z6tr+0eekPRK4sv8a/By9MfR1PUVwGvnVf0lqhEfoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ14v8AUf/W5GYBq8IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = torch.Tensor(1000).uniform_(-3, 3)\n",
    "\n",
    "hist_dist('Distribution', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Weight Initialization\n",
    "\n",
    "PyTorch layers are initialized by default in their respective `reset_parameters()` method. For example:\n",
    "\n",
    "- `nn.Linear`\n",
    "    - `weight` and `bias`: uniform distribution [-limit, +limit] where `limit` is `1. / sqrt(fan_in)` and `fan_in` is the number of input units in the weight tensor.\n",
    "- `nn.Conv2D`\n",
    "    - `weight` and `bias`: uniform distribution [-limit, +limit] where `limit` is `1. / sqrt(fan_in)` and `fan_in` is the number of input units in the weight tensor.\n",
    "\n",
    "With this implementation, the variance of the layer outputs is equal to `Var(W) = 1 / 3 * sqrt(fan_in)` which isn't the best initialization strategy out there.\n",
    "\n",
    "Note that PyTorch provides convenience functions for some of the initializations. The input and output shapes are computed using the method `_calculate_fan_in_and_fan_out()` and a `gain()` method scales the standard deviation to suit a particular activation.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu')) \n",
    "```\n",
    "\n",
    "#### Xavier Initialization\n",
    "\n",
    "This initialization is general-purpose and meant to \"work\" pretty well for any activation in practice.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform(m.weight)\n",
    "```\n",
    "\n",
    "You can tailor this initialization to your specific activation by using the `nn.init.calculate_gain(act)` argument.\n",
    "\n",
    "```python\n",
    "# default xavier init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform(m.weight(), gain=nn.init.calculate_gain('relu'))\n",
    "```\n",
    "\n",
    "- [arXiv](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "#### He et. al Initialization\n",
    "\n",
    "This is a similarly derived initialization tailored specifically for ReLU activations since they do not exhibit zero mean.\n",
    "\n",
    "```python\n",
    "# he initialization\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal(m.weight, mode='fan_in')\n",
    "```\n",
    "\n",
    "For `mode=fan_in`, the variance of the distribution is ensured in the forward pass, while for `mode=fan_out`, it is ensured in the backwards pass.\n",
    "\n",
    "- [arXiv](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "#### SELU Initialization\n",
    "\n",
    "Again, this initialization is specifically derived for the SELU activation function. The authors use the `fan_in` strategy. They mention that there is no significant difference between sampling from a Gaussian, a truncated Gaussian or a Uniform distribution.\n",
    "\n",
    "```python\n",
    "# selu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "        nn.init.normal(m.weight, 0, sqrt(1. / fan_in))\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        fan_in = m.in_features\n",
    "        nn.init.normal(m.weight, 0, sqrt(1. / fan_in))\n",
    "```\n",
    "\n",
    "- [arXiv](https://arxiv.org/abs/1706.02515)\n",
    "\n",
    "#### Orthogonal Initialization\n",
    "\n",
    "Orthogonality is a desirable quality in NN weights in part because it is norm preserving, i.e. it rotates the input matrix, but cannot change its norm (scale/shear). This property is valuable in deep or recurrent networks, where repeated matrix multiplication can result in signals vanishing or exploding.\n",
    "\n",
    "```python\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.orthogonal(m.weight)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_init_weights(dataset, title, weight_init_list, plot_n_batches=100):\n",
    "    \"\"\"\n",
    "    Plot loss and print stats of weights using an example neural network\n",
    "    \"\"\"\n",
    "    colors = ['r', 'b', 'g', 'c', 'y', 'k']\n",
    "    label_accs = []\n",
    "    label_loss = []\n",
    "\n",
    "    assert len(weight_init_list) <= len(colors), 'Too many inital weights to plot'\n",
    "\n",
    "    for i, (weights, label) in enumerate(weight_init_list):\n",
    "        loss, val_acc = _get_loss_acc(dataset, weights)\n",
    "\n",
    "        plt.plot(loss[:plot_n_batches], colors[i], label=label)\n",
    "        label_accs.append((label, val_acc))\n",
    "        label_loss.append((label, loss[-1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    print('After 858 Batches (2 Epochs):')\n",
    "    print('Validation Accuracy')\n",
    "    for label, val_acc in label_accs:\n",
    "        print('  {:7.3f}% -- {}'.format(val_acc*100, label))\n",
    "    print('Loss')\n",
    "    for label, loss in label_loss:\n",
    "        print('  {:7.3f}  -- {}'.format(loss, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network, we'll test on a 3 layer neural network with ReLU activations and an Adam optimizer.  The lessons you learn apply to other neural networks, including different activations and optimizers.\n",
    "\n",
    "## Initialize Weights\n",
    "Let's start looking at some initial weights.\n",
    "### All Zeros or Ones\n",
    "If you follow the principle of [Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor), you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.\n",
    "\n",
    "With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.\n",
    "\n",
    "Let's compare the loss with all ones and all zero weights using `helper.compare_init_weights`.  This function will run two different initial weights on the neural network above for 2 epochs.  It will plot the loss for the first 100 batches and print out stats after the 2 epochs (~860 batches). We plot the first 100 batches to better judge which weights performed better at the start.\n",
    "\n",
    "Run the cell below to see the difference between weights of all zeros against all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
