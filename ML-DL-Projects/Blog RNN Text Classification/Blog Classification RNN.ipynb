{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogging with RNNs\n",
    "\n",
    "By Karl Heyer\n",
    "\n",
    "This project is a follow up to my last [RNN project](https://github.com/kheyer/ML-DL-Projects/tree/master/Shakespeare%20RNN) where I trained a character level RNN to write Shakespeare.\n",
    "\n",
    "This time around we're going to make a word level RNN using  the [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm), a collection of blogs. The blogs are categorized by the author's gender, age, profession and astrology sign. First we will train a language model on a section of blogs. Then we will try to fine tune the model to the task of classifying blog authors by gender.\n",
    "\n",
    "There were a lot of things I wanted to try with this project, but unfortunately I ran into the limitations of my hardware (namely my 8 gigs of GPU memory). But more on that later. For now lets look at the model we're going to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Model Architecture\n",
    "2. Project Setup\n",
    "    * 2.1 Import Libraries\n",
    "    * 2.2 Data Overview\n",
    "3. Model Training\n",
    "4. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Architecture\n",
    "\n",
    "This project will use a recurrent neural network (RNN) to build a language model. Lets break down what these mean.\n",
    "\n",
    "## Language Model\n",
    "\n",
    "This model will try to predict the next word in a string of words. For example, if the last sequence of words the RNN has seen were \"The sky is\", it should predict \"blue\" as the next character.\n",
    "\n",
    "This is different from a character level model that tries to predict one character at a time.\n",
    "\n",
    "The fundamental way we will represent language in this model is through a word embedding matrix representing the vocabulary of our corpus. When we process text for feeding into the model, each word is mapped to a number value. Each word/number in the corpus has a corresponding row of weights in the embedding matrix. This creates a higher dimensional representation of the word. When the RNN processes sentences of text, it is really processing vectors of weight values for each word.\n",
    "\n",
    "\n",
    "## Recurrent Neural Network\n",
    "\n",
    "Most deep learning architectures have a series of layers that process an input into an output. Each layer has a set of weights that perform some functional operation to convert inputs to outputs. RNNs, rather than having a deep stack of layers, have a single hidden layer that is updated every time an input comes in. This allows the activations of a previous input to affect the activations of the next input. This allows RNNs to learn complex relationships along a series of inputs.\n",
    "\n",
    "This has made RNNs very effective architectures for NLP problems, where understanding a word in a sentence requires understanding what came before it.\n",
    "\n",
    "For example, lets say the RNN is processing the sentence \"The sky is blue\". Since this is a word level language model, each word (represented by a vector of weights from the embedding matrix) is fed into the network one word/vector at a time.\n",
    "\n",
    "First the netword processes \"The\" and updates the hidden state accordingly.\n",
    "Then the network processes \"sky\", using the activations generated by the previous word \"the\" to process \"sky\" and generate a new set of activations. \n",
    "Then the network processes \"is\" using the activations generated by processing \"sky\" and \"the\". And so on.\n",
    "\n",
    "\n",
    "In Pytorch, the standard RNN module processes each input into a new set of activations using the following equation:\n",
    "\n",
    "$h_t = tanh(w_{ih} x_t + b_{ih}  +  w_{hh} h_{(t-1)} + b_{hh})$\n",
    "\n",
    "The network starts with an existing hidden state $h_{(t-1)}$. When a new input $x_t$ enters the network, it is processed by a linear layer with weights $w_{ih}$ and bias $b_{ih}$. The hidden state $h_{(t-1)}$ is also processed by a linear layer with weights $w_{hh}$ and bias $b_{hh}$. The outputs of these two linear layers are summed together and processed by the $tanh$ activation function to create the new hidden state $h_t$.\n",
    "\n",
    "For this project, we'll use a more advanced version of an RNN called an LSTM. The key improvement of LSTMs over regular RNNs is LSTMs introduce a set of gates that control how much information from the old hidden state and the current input move into the new hidden state. The parameters that control these gates are learned by the network. This allows LSTMs to learn and understand information over longer periods.\n",
    "\n",
    "The equations governing LSTMs:\n",
    "\n",
    "$\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}$\n",
    "\n",
    "$i_{t}$ is the input gate. This controls how much of the new input is taken into the cell state.\n",
    "\n",
    "$f_{t}$ is the forget gate. This gate controls how much of the old cell state passes on to the new cell state.\n",
    "\n",
    "$g_{t}$ is the cell gate, which is the same equation we used to define our new hidden state in the standard RNN.\n",
    "\n",
    "$o_{t}$ is the output gate which controls how much of the cell state passes on to the hidden state.\n",
    "\n",
    "$c_{t}$ is the current cell state. You will notice $c_{t}$ is composed of the old cell state, the current forget gate, the current input gate, and the current cell gate. This shows how the various gates are used to filter how much of the input and previous cell state pass on to the new hidden state.\n",
    "\n",
    "$h_{t}$ is the new hidden state. It is composed of the current cell state activated by $tanh$ and filtered by the output gate.\n",
    "\n",
    "Visually:[](attachment:LSTM.png)\n",
    "\n",
    "<img src=\"LSTM.png\">\n",
    "\n",
    "\n",
    "When the optimizer updates the weights by backpropagation, the loss is propagated through all the hidden activations, essentially \"unrolling\" the hidden state into each set of activations created over the course of generating the final activations.\n",
    "\n",
    "Visually:[](attachment:RNN.png)\n",
    "\n",
    "<img src=\"RNN.png\">\n",
    "\n",
    "[Image source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "\n",
    "This leads to one of the most important parameters in training RNNs: backprop through time (bptt). The bptt value sets how many hidden activations are saved and propagated through during backpropagation. Bptt is a double edged sword - on the one hand, a larger bptt value allows the network to train over longer sequences. This can allow the network to understand language over a longer sequence length. On the other, the larger the bptt value, the deeper the network becomes and the harder it is to train. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Project Setup\n",
    "\n",
    "## 2.1 Load Libraries\n",
    "\n",
    "This project uses FastAI, Pytorch, and their associated dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Overview\n",
    "\n",
    "The files for this corpus are all in xml format. The first thing to do is to convert them to txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000331.female.37.indUnk.Leo.xml',\n",
       " '1000866.female.17.Student.Libra.xml',\n",
       " '1004904.male.23.Arts.Capricorn.xml',\n",
       " '1005076.female.25.Arts.Cancer.xml',\n",
       " '1005545.male.25.Engineering.Sagittarius.xml',\n",
       " '1007188.male.48.Religion.Libra.xml',\n",
       " '100812.female.26.Architecture.Aries.xml',\n",
       " '1008329.female.16.Student.Pisces.xml',\n",
       " '1009572.male.25.indUnk.Cancer.xml',\n",
       " '1011153.female.27.Technology.Virgo.xml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH='data/blogs/blogs/'\n",
    "\n",
    "files = os.listdir(PATH)\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2 = 'data/blogs/blogs_txt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works to convert everything to txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in files:\n",
    "    try:\n",
    "        infile = open(f'{PATH}{fn}', encoding=\"ISO-8859-1\")\n",
    "        contents = infile.read()\n",
    "        soup = BeautifulSoup(contents, 'html5lib')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        with open(f'{PATH2}{fn[:-4]}.txt', 'w', encoding='utf-8') as out:\n",
    "            for post in posts:\n",
    "                out.write(post.get_text()) \n",
    "                \n",
    "        if os.path.getsize(f'{PATH2}{fn[:-4]}.txt') == 0:\n",
    "            print(f'{PATH2}{fn[:-4]}.txt')\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        #print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = os.listdir(PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of files in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19320"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/blogs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In experimenting with the model, I found I had to severely limit the selection of the corpus to avoid CUDA memory errors. Our training data set will be 400 blogs from men aged 23-27 and 400 blogs from women aged 23-27. Our validation set will be 200 total blogs from the same subset of writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{file_path}trn/male', exist_ok=True)\n",
    "os.makedirs(f'{file_path}trn/female', exist_ok=True)\n",
    "os.makedirs(f'{file_path}trn/all', exist_ok=True)\n",
    "os.makedirs(f'{file_path}val/male', exist_ok=True)\n",
    "os.makedirs(f'{file_path}val/female', exist_ok=True)\n",
    "os.makedirs(f'{file_path}val/all', exist_ok=True)\n",
    "os.makedirs(f'{file_path}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = 0\n",
    "tf = 0\n",
    "vm = 0\n",
    "vf = 0\n",
    "\n",
    "for fn in txt_files:\n",
    "    copy_file = False\n",
    "    gender = fn.split('.')[1]\n",
    "    age = int(fn.split('.')[2])\n",
    "    \n",
    "    if age in range(23,28):\n",
    "        if random.random() > 0.1:\n",
    "            dset = 'trn/'\n",
    "            \n",
    "            if gender == 'male':\n",
    "                if tm < 400:\n",
    "                    tm += 1\n",
    "                    copy_file = True\n",
    "                    \n",
    "            else:\n",
    "                if tf < 400:\n",
    "                    tf += 1\n",
    "                    copy_file = True          \n",
    "            \n",
    "        else:\n",
    "            dset = 'val/'\n",
    "            \n",
    "            if gender == 'male':\n",
    "                if vm < 100:\n",
    "                    vm += 1\n",
    "                    copy_file = True\n",
    "            else:\n",
    "                if vf < 100:\n",
    "                    vf += 1\n",
    "                    copy_file = True\n",
    "        \n",
    "        if copy_file:\n",
    "            copy2(f'{PATH2}{fn}', f'{file_path}{dset}{gender}')\n",
    "            copy2(f'{PATH2}{fn}', f'{file_path}{dset}all')\n",
    "        \n",
    "    if all((tm>=400, tf>=400, vm>=100, vf>=100)):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization \n",
    "\n",
    "An important step in creating language models is to process the text into tokens. Tokenization breaks text into chunks that will be fed into the network. For character level models, we would tokenize by making each character a single input. For word level language models, we will use spacy tokenizer to process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_PATH = 'trn/all'\n",
    "VAL_PATH = 'val/all'\n",
    "TRN = f'{file_path}trn/all'\n",
    "VAL = f'{file_path}val/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/blogs/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{TRN}/{trn_files[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of blogs in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\t \\n      cupid,please hear my cry, cupid, please let your arrow fly  straight into my fucking head.\\n     \\n\\n    \\n\\n\\n\\t \\n      Ijust got back from LA. I needed something to do so I looked up shows and ended up getting to see Deathray Davies that same night. I brought back shirts and stickers.my brother in law was impressed enough to purchase one of there albums.they were followed by this stoner/boogie band called Honky which allegedly contains and ex-Butthole Surfer.I didnt dig them too well.they were like the melvins and ZZ top combined. they even had redneck cowboy hats and silly 3 inch long goatees. their antics consisted of tipping there hats and torturing the audiance with between song chatter. they have a small lesbian following.  I didnt know the davies had like 6 members. I met the singer, his brother, and the second keyboardist.he didnt much care for honkey either.  thanx to colt from poulain for the correspondence. I hope we meet again except a show where WE are playing too.  j'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same blog tokenized. The difference isn't drastic, but you can see the effects of tokenization. Contractions like \"didn't\" become two words - \"did\", \"nt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' cupid , please hear my cry , cupid , please let your arrow fly  straight into my fucking head .  Ijust got back from LA . I needed something to do so I looked up shows and ended up getting to see Deathray Davies that same night . I brought back shirts and stickers.my brother in law was impressed enough to purchase one of there albums.they were followed by this stoner / boogie band called Honky which allegedly contains and ex - Butthole Surfer . I did nt dig them too well.they were like the melvins and ZZ top combined . they even had redneck cowboy hats and silly 3 inch long goatees . their antics consisted of tipping there hats and torturing the audiance with between song chatter . they have a small lesbian following .  I did nt know the davies had like 6 members . I met the singer , his brother , and the second keyboardist.he did nt much care for honkey either .  thanx to colt from poulain for the correspondence . I hope we meet again except a show where WE are playing too .  just go'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([sent.string.strip() for sent in spacy_tok(blog)])[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a torchtext field to process the text and work with our dataloader. For this model we will use a batch size of 70 and a bptt value of 55. This runs really slow due to GPU memory limitations. During training I never saw GPU utilization go above 30%, but memory utilization was around 94%. Really riding the CUDA memory error line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=\"spacy\")\n",
    "#TEXT = pickle.load(open(f'{file_path}models/TEXT_2327_partial_2.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=70; bptt=55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create our dataloader, we set min_freq to 20. This cuts out any word that appears less than 20 times. I don't like doing this. I'd much prefer to have the cutoff around 5 or 10, since less frequently used words likely hold a lot of meaning. However this keeps the vocabulary smaller (and therefore the size of the embedding matrix). It's a compromise to make this thing train in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(file_path, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{file_path}models/TEXT_2327_partial_2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after processing our corpus has a vocabulary of 25378 words and a total corpus length of 22593561 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5378, 25378, 1, 22593561)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 10 most used words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '.', ',', 'the', 'i', ' ', 'to', 'and', 'a']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word maps to a number, like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of numericalizing text input. The tensor of numbers is what is actually processed by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t ',\n",
       " '      ',\n",
       " 'cupid',\n",
       " ',',\n",
       " 'please',\n",
       " 'hear',\n",
       " 'my',\n",
       " 'cry',\n",
       " ',',\n",
       " 'cupid',\n",
       " ',',\n",
       " 'please']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0].text[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    93\n",
       "    56\n",
       " 16939\n",
       "     3\n",
       "   423\n",
       "   412\n",
       "    15\n",
       "   963\n",
       "     3\n",
       " 16939\n",
       "     3\n",
       "   423\n",
       "[torch.cuda.LongTensor of size 12x1 (GPU 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.numericalize([md.trn_ds[0].text[:12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what an input training minibatch looks like. We have 70 columns, corresponding to our batch size of 70. Each column has 55 rows representing 55 word tokens in sequential order.\n",
    "\n",
    "The y values we want to predict against are also word token values. For each value in the x matrix, there is a correct y value corresponding to the next word in the sequence. Due to how torchtext processes things, the y tensor is flattened. The 3850 (55x70) values in the y tensor correspond to predictions for each word in the x matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "     93    248      4  ...     128     51    153\n",
       "     56      2    113  ...    1763     37    623\n",
       "  16939      6    151  ...       3     25      7\n",
       "         ...            ⋱           ...         \n",
       "    218    139     30  ...       3      9     20\n",
       "    135    117      2  ...      39   1573     47\n",
       "      2  10364      6  ...       0      2      7\n",
       " [torch.cuda.LongTensor of size 55x70 (GPU 0)], torch.Size([55, 70]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "     56\n",
       "      2\n",
       "    113\n",
       "   ⋮   \n",
       "      2\n",
       "      6\n",
       "  10229\n",
       " [torch.cuda.LongTensor of size 3850 (GPU 0)], torch.Size([3850]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "Here we create the model. em_sz sets the size (columns) of the embedding matrix. The embedding matrix will have size vocabulary x em_sz.\n",
    "\n",
    "nh is the number of hidden activations per layer.\n",
    "\n",
    "nl is the number of stacked LSTM modules in the model.\n",
    "\n",
    "The model will be trained with ADAM optimizer.\n",
    "\n",
    "The model itself has 5 dropout parameters.\n",
    "\n",
    "dropouth is applied to the activations going from one LSTM block to another.\n",
    "\n",
    "dropouti is applied to the input layer.\n",
    "\n",
    "dropoute is applied to the embedding layer.\n",
    "\n",
    "wdrop is applied to the LSTM's hidden weights.\n",
    "\n",
    "dropout is applied to the linear decoder that determines the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model structure. We start with an embedding matrix encoding our vocabulary. Then we have three LSTM modules, followed by a linear decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(25378, 200, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(25378, 200, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(200, 500)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 200)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=200, out_features=25378, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a5707664d84e4983e22785cc0522a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      4.522975   4.572558  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.57256])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9a37b963464069a6503b79906bed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      4.436817   4.419866  \n",
      "    1      4.38615    4.405467                                                                                         \n",
      "    2      4.343145   4.338641                                                                                         \n",
      "    3      4.366256   4.404881                                                                                         \n",
      "    4      4.310186   4.345111                                                                                         \n",
      "    5      4.270685   4.294953                                                                                         \n",
      "    6      4.238749   4.275535                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.27554])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('cyc1')\n",
    "learner.save_encoder('cyc1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0b0c79d78942ab9046dfc5d9d5c157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      4.270131   4.278947  \n",
      "    1      4.267469   4.28397                                                                                          \n",
      "    2      4.237581   4.26392                                                                                          \n",
      "    3      4.26826    4.291307                                                                                         \n",
      "    4      4.237706   4.270288                                                                                         \n",
      "    5      4.233086   4.255018                                                                                         \n",
      "    6      4.218849   4.248353                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.24835])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('cyc2')\n",
    "learner.save_encoder('cyc2_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('cyc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d8f677dcab4d7c8c33c6af09a56c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      4.257507   4.289395  \n",
      "    1      4.249072   4.280441                                                                                         \n",
      "    2      4.214001   4.271849                                                                                         \n",
      "    3      4.224365   4.265314                                                                                         \n",
      "    4      4.200969   4.254217                                                                                         \n",
      "    5      4.189088   4.246558                                                                                         \n",
      "    6      4.181067   4.238831                                                                                         \n",
      "    7      4.166237   4.233099                                                                                         \n",
      "    8      4.170617   4.229943                                                                                         \n",
      "    9      4.179696   4.229322                                                                                         \n",
      "    10     4.237349   4.272693                                                                                         \n",
      "    11     4.21351    4.266871                                                                                         \n",
      "    12     4.213797   4.260933                                                                                         \n",
      "    13     4.213826   4.253283                                                                                         \n",
      "    14     4.180337   4.243208                                                                                         \n",
      "    15     4.176092   4.234854                                                                                         \n",
      "    16     4.169602   4.228431                                                                                         \n",
      "    17     4.17969    4.223591                                                                                         \n",
      "    18     4.144632   4.219408                                                                                         \n",
      "    19     4.156156   4.218518                                                                                         \n",
      "    20     4.226132   4.264277                                                                                         \n",
      "    21     4.204057   4.257808                                                                                         \n",
      "    22     4.194831   4.250195                                                                                         \n",
      "    23     4.192355   4.24538                                                                                          \n",
      "    24     4.179968   4.236595                                                                                         \n",
      "    25     4.181911   4.228919                                                                                         \n",
      "    26     4.160677   4.220929                                                                                         \n",
      "    27     4.155351   4.215293                                                                                         \n",
      "    28     4.140531   4.212418                                                                                         \n",
      "    29     4.125684   4.211467                                                                                         \n",
      "    30     4.215521   4.258662                                                                                         \n",
      "    31     4.197198   4.252695                                                                                         \n",
      "    32     4.202867   4.246789                                                                                         \n",
      "    33     4.176861   4.238846                                                                                         \n",
      "    34     4.165935   4.230301                                                                                         \n",
      "    35     4.15247    4.22235                                                                                          \n",
      "    36     4.148989   4.216126                                                                                         \n",
      "    37     4.13467    4.21098                                                                                          \n",
      "    38     4.139014   4.207345                                                                                         \n",
      "    39     4.128546   4.206546                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.20655])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-3, 4, wds=1e-6, cycle_len=10,cycle_save_name='cyc3_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('cyc3')\n",
    "learner.save_encoder('cyc3_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bdfbabf8dd4edcbb5ccb21d49885af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      4.452277   4.280445  \n",
      "    1      4.567751   4.463188                                                                                         \n",
      "    2      4.558173   4.455255                                                                                         \n",
      "    3      4.551173   4.447328                                                                                         \n",
      "    4      4.548191   4.438882                                                                                         \n",
      "    5      4.539622   4.428327                                                                                         \n",
      "    6      4.524216   4.413176                                                                                         \n",
      "    7      4.512895   4.404466                                                                                         \n",
      "    8      4.504389   4.388278                                                                                         \n",
      "    9      4.49776    4.370295                                                                                         \n",
      "    10     4.477144   4.355243                                                                                         \n",
      "    11     4.471716   4.338208                                                                                         \n",
      "    12     4.454649   4.322878                                                                                         \n",
      "    13     4.448473   4.308649                                                                                         \n",
      "    14     4.435744   4.293766                                                                                         \n",
      "    15     4.428338   4.279539                                                                                         \n",
      "    16     4.421133   4.269408                                                                                         \n",
      "    17     4.407684   4.261616                                                                                         \n",
      "    18     4.414855   4.256861                                                                                         \n",
      "    19     4.405081   4.254661                                                                                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.25466])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='cyc4_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('cyc4')\n",
    "learner.save_encoder('cyc4_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Review\n",
    "\n",
    "This thing was a beast to train. Even with the major reductions we made to corpus size, min frequency of words and parameter values, the training epochs shown took several days.\n",
    "\n",
    "In that time, model basically went nowhere. Validation loss dropped from 4.566 to 4.254, not a huge improvement given the number of epochs and training time involved.\n",
    "\n",
    "Testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = num_str(inp)\n",
    "    res, *_ = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(res[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res_lst = inp.split(' ')\n",
    "    try:\n",
    "        for i in range(n):\n",
    "            c = get_next(inp)\n",
    "            res_lst += [c]\n",
    "            inp_lst = inp.split(' ')\n",
    "            inp_lst += [c]\n",
    "            inp = \"\"\" \"\"\".join(inp_lst[1:])\n",
    "        return \"\"\" \"\"\".join(res_lst)\n",
    "    except:\n",
    "        print(res_lst)\n",
    "        print(c)\n",
    "        print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learner.model\n",
    "m.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usually I saw never say never, but today changed  town in cousin magnets .    i 'm mouthful .    i am going to fight union .    derek caller optimism , sweet drum roll , and the songs are <unk> goal ... but the way it sounds .   diet coke can be blames the beauty is n't a sphere . what happening to a   urllink <unk> today also . asked by firewall is a great review . she 's asked me and wretched ,   urllink tucker   harris is thinking about how i feel the first time i come to their endangered affections what happened this jaime . cramp - <unk>   the meat tool , in curtis <unk> responsible for virgin <unk> frantic and often , shady being gentle with the whole system is orbit women .   time at the house , but no words will be higher goodwill .   broad , well american whitney braun at least one totally levels <unk> returns to iraq at interestingly very difficult in one . messaging is gives me perfume going on timer !   cheers , your goal of getting a david disposable chance and if you will but the same one directed half a grad student braces ( yeah buying its own menu ) - which happened last night . parenting one less retreat because of   simulation   or   <unk>   & i ran into apple 's kravitz and sums up my listening match .   this is my 1980 's career job .            this week , then camping finland late on my usual relationship with his hard <unk> our first defeats the sharma of a <unk> thanks for having to record magazine .            urllink here , i think of the murderer because the driver 's machine was called evaluated so he was not lost when it worked . so much for getting out of bed \" childrens \" with the maraming think figures . and not posting bladder and mockery oils and zilch ,   now , in the city of his , 's mini pooped . and the best thing though - it 's a sweetheart . no , not horse - drums and <unk>   >            that 's the best way ... i did n't miss wear again , ...   new stuff to blog .   karissa * tapping elevator in north has <unk> that would be go for the hospital o'reilly complaint . this treat has been an inn brooks was on there i have press paycheck . ( faulty rendezvous wisdom , so you can figure out why you love popping makes it be a jerk , cause he does n't know whether it 's even                                      or gruesome , is it just is n't really <unk> > stunt .... work 's here on your desk applauded .   cameras is still vulnerable .   but at see what it takes when i 'm / w baby a girls bunch of southwestern <unk> he futurama was a <unk> ungrateful ahmed            i ki again , in different route . give him - hole in the hopping , so split away in vietnam camcorder for a us$ 50 % of course .   sunday night out of whatever utilize jesus , ado : i have a phone number that is composed of on the thursday asses .                  \" looong omg bs \" <unk>   it 's everyday morning after sleeping with addition to the potential of the gift hailing from my doubts and cabaret chalet and about on the <unk> red a new year 's eve . )   and really beautiful and robust developed , cold , and <unk> : \" i 'm comb ; fickle !             this <unk>   \" for real ? \"   anymore ,   should i click , as well is very nice , too .            awe - sirens - load the hatred in my sep .    it was sold only by 56 pages in cheapo .   rhymes with all you can ask orchid of the manufacturing system populace mab .   to all the problems in the home world have <unk>   ladies and gentlemen , its flattering at times that when way- make you out of <unk> western manhattan drive is going to happen soon enough .   have you ever had a sort of principle , when all this aisle should be somewhere that for more cash everything is not in the footage of course .    that 's all she looks like they have proclaimed these colors . , for example , and what could get on the shoulder of the car cigarette stray from a relief in the dead hot sore has pledged . legend would be a treat on this donation ? hahaha ... <unk>   i look for brighten her .   question : ) go for christmas gifts on sunday . wish me luck !            most likely microscope are pretty quickly courtyard on the road .   after the day this morning <unk> the site would n't go off again ( do n't really think it burns down ) but this morning during the day that we are working in court . i also started by saying since abusing vic <unk>   you said more to one who kept taking their glasses to the ballet : i was driving to the grocery shopping away outside and exploring a giant house tube .   i swear went through the door lines on the web .   me and flipping out .   how horrible .   <unk> free in possible grounds to cheer the âthe away from its wrestling <unk> of the   urllink place   since her party at our later work iis . i got to ask for visit . who knew ? no ... i guess what i think dela . resident\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('Usually I saw never say never, but today changed ', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Results\n",
    "\n",
    "These results aren't that great. Some chunks sound like sentences, but most are nonsensical. The main problem seems to be that the model loss is still far too high, and the model will struggle to train with such limited data.\n",
    "\n",
    "This whole project has felt restricted by hardware issues. Makes me want to go through the effort of setting up AWS.\n",
    "\n",
    "I'm going to try fine tune the model to gender classification. I don't have a lot of hope for this to perform given how bad it still is - crap in crap out - but I want to get some practice setting up this kind of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Classification\n",
    "\n",
    "Issues about model performance aside, here we go.\n",
    "\n",
    "First we create a torchtext dataset. Then we feed the dataset to a new model. The new model is then loaded with the word embedding matrix from the language model and trained to classify author gender from text. This time the torchtext field will contain the label ('male' or 'female') that the model will try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for label in ['male', 'female']:\n",
    "            for fname in glob(os.path.join(path, label, '*.txt')):\n",
    "                with open(fname, 'r', encoding='utf-8') as f:\n",
    "                    text = ''\n",
    "                    while len(text) < 1000: #Ideally we would just grab the first line but a lot of the blogs\n",
    "                        next_line = f.readline() #start with whitespace. This ensures we get a good chunk of text to go on\n",
    "                        if not any((next_line == '', next_line == '\\n', next_line == ' ')):\n",
    "                            text += ' ' + next_line\n",
    "                    \n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.data/',\n",
    "               train='train', test='test', **kwargs):\n",
    "        return super().splits(\n",
    "            root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_LABEL = data.Field(sequential=False)\n",
    "splits = BlogDataset.splits(TEXT, BLOG_LABEL, file_path, train='trn', test='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/blogs/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "md2 = TextData.from_splits(file_path, splits, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of male and female text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = splits[0].examples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        \\n        wednesday of this week , my wife elizabeth and i found out that we will soon become parents ! soon , if march is soon for you .    funny because my wife has always been the one saying \" i want to have babies \" , while making cutesy hands , but since the news has come , really does n\\'t act all that excited , but nausea can do that to you . really , the depth of the revelation has not fully hit us yet , but we are very happy .    one of the main purposes of this blog will be to document the belly growth visually as the next 6 1/2 months pass . oh , and i guess i will put up some photos when the little one appears .      also in other news ...            i am leaving work soon   ( do n\\'t worry , i \\'ve earned this break )   to head down to noblesville near indy to see sting ! this will be my 3rd time seeing him live , and a delayed birthday present to myself . i \\'ll be meeting jenn swift and a friend of hers . i \\'ll give an update on this event next posting .      \\n      \\n     \\n        \\n          man in superman costume attacks motorists      yesterday we reported that a few dozen spider - men took over a peruvian university . today it appears that a 21-year - old man dressed as superman has attacked ann arbor motorists . be good , batman . be good ...     urllink nbc 4 - news - man in superman costume attacks motorists     a group of college students in lima , peru donned spider - man masks , took over several campus buildings , and demanded the removal of the university dean . good thinking , kids . because nothing says \" i mean business \" like halloween costumes . i \\'m going to go put on my aquaman suit and ask for a raise now ...     urllink news story    '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = splits[0].examples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\t \\n        i wrote these yesterday for work but thought you lot might like them too . they 're short and sweet as the cards we have to write on are not very big .   -----------------------------------------------------------------------       urllink lirael by garth nix    lirael returns us to the old kingdom fourteen years after the events told in sabriel . lirael is a daughter of the clayr , a race of seers who live within the glacier . but lirael has never received the gift of sight and so she spends her time working and hiding in the great library where she can learn more charter magic . when she is sent on a quest to find a boy named nicholas , she meets up with prince sameth and her whole world changes overnight . a fantastic return to the world first introduced in sabriel .   -----------------------------------------------------------------------       urllink abhorsen by garth nix    in abhorsen we return to the story of prince sameth and lirael . they are both still reeling from the discovery of their true destinies , sam as a wallmaker , and lirael as the - abhorsen - in - waiting . the race to the red lake is becoming more desperate every day , and with the news of sabriel and touchstone 's murders still ringing in their ears , lirael and sam must fight their way through many dead to reach their destination . the finale to the old kingdom trilogy is tense and exciting . very highly recommended .   -----------------------------------------------------------------------       urllink paradise fields by katie fforde    nel is pleasantly surprised by a kiss from a handsome stranger at the christmas farmers market . her excuse to her friends is that she was selling mistletoe at the time , but this does n't explain why she gets so flustered every time she thinks of him . but when the hospices playing fields are threatened , and her handsome stranger turns out to be the solicitor for the enemy , nel must fight with strength she never knew she had . katie fforde 's latest bestseller is an absolutely glorious tale of love , life and community spirit .   -----------------------------------------------------------------------       urllink a hat full of sky by terry pratchett    tiffany aching is leaving home to be apprenticed to miss level , a witch who lives in the mountain country and sometimes leaves her glasses on her other nose . tiffany is so busy trying to get used to being surrounded by trees , and to be accepted by the other apprentice witches ( one of whom prefers magick to magic ) that she does n't realise she is being hunted by a hiver , a magical creature from the beginnings of time . luckily rob any body , daft wullie and the rest of the nac mac feegle clan are on their way to help out , along with a certain mistress weatherwax . as with all discworld books , highly recommended .   \""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(t2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, \n",
    "           dropout=0.1, dropouti=0.65, wdrop=0.5, dropoute=0.1, dropouth=0.3)\n",
    "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m3.clip=25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have the same model structure as before. The main difference is the dimensionality of the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchRNN(\n",
       "    (encoder): Embedding(25378, 200, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(25378, 200, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(200, 500)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 200)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): ModuleList(\n",
       "      (0): LinearBlock(\n",
       "        (lin): Linear(in_features=600, out_features=3, bias=True)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (bn): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up the model's encoder with the saved weights from the language model and try our best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.load_encoder('cyc4_enc')\n",
    "lrs=np.array([1e-5,1e-5,1e-5,1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394667c8d8214f34be4524cf418d2a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      1.104813   1.026852   0.381657  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.02685]), 0.3816565847613915]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.freeze_to(-1)\n",
    "m3.fit(lrs/2, 1, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88c0f7aa4444123abe182ed554c18ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      1.060456   1.057459   0.37992   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.05746]), 0.3799195173540556]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e5b2c62c74de29bc4dfd6596cbff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      1.004435   1.185917   0.381821  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.18592]), 0.38182147358036267]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.unfreeze()\n",
    "m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c82c27c27c64b6fb1b8efaf5184370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      0.952848   1.104415   0.500488  \n",
      "    1      0.895149   1.159471   0.501726                                                                              \n",
      "    2      0.858103   1.571479   0.501086                                                                              \n",
      "    3      0.846589   1.683654   0.501409                                                                              \n",
      "    4      0.82649    1.329061   0.501266                                                                              \n",
      "    5      0.803777   0.979512   0.501                                                                                 \n",
      "    6      0.783979   0.953483   0.501084                                                                              \n",
      "    7      0.772295   0.935644   0.500664                                                                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.93564]), 0.5006637278107683]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(lrs, 2, metrics=[accuracy], cycle_len=4, cycle_save_name='gender_class1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2705a95b8204d1f9a2c7a8f9ff2a7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      0.732586   0.810481   0.38408   \n",
      "    1      0.732259   1.176944   0.501833                                                                              \n",
      "    2      0.721015   1.184865   0.50342                                                                               \n",
      "    3      0.719483   1.191819   0.502816                                                                              \n",
      "    4      0.716559   1.179486   0.503789                                                                              \n",
      "    5      0.709782   0.789773   0.387736                                                                              \n",
      "    6      0.70994    0.934974   0.386555                                                                              \n",
      "    7      0.707187   0.970563   0.504004                                                                              \n",
      "    8      0.709645   1.251063   0.388116                                                                              \n",
      "    9      0.708588   1.739861   0.503822                                                                              \n",
      "    10     0.707677   1.505881   0.386731                                                                              \n",
      "    11     0.704523   1.468481   0.387013                                                                              \n",
      "    12     0.707446   0.888588   0.384099                                                                              \n",
      "    13     0.703359   0.734094   0.383398                                                                              \n",
      "    14     0.702813   0.75038    0.386804                                                                              \n",
      "    15     0.701012   0.746877   0.38626                                                                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.74688]), 0.38625989366836494]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(lrs, 4, metrics=[accuracy], cycle_len=4, cycle_save_name='gender_class2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f738d1fcab4773b4c64917c71ffac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      0.702355   0.73934    0.386541  \n",
      "    1      0.701594   0.750925   0.503607                                                                              \n",
      "    2      0.697262   0.760139   0.503607                                                                              \n",
      "    3      0.697233   0.74761    0.387044                                                                              \n",
      "    4      0.698773   0.757773   0.503585                                                                              \n",
      "    5      0.697376   0.755829   0.386864                                                                              \n",
      "    6      0.70034    0.751478   0.504425                                                                              \n",
      "    7      0.697464   0.756986   0.504568                                                                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.75699]), 0.504567826586341]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "m3.fit(lrs/10, 2, metrics=[accuracy], cycle_len=4, cycle_save_name='gender_class3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4809b188ad0a4789b445fffed1ddf4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      0.691443   0.767655   0.504282  \n",
      "    1      0.694669   0.780608   0.504984                                                                              \n",
      "    2      0.698031   0.775461   0.503866                                                                              \n",
      "    3      0.699507   0.762166   0.503723                                                                              \n",
      "    4      0.696531   0.7502     0.504706                                                                              \n",
      "    5      0.69829    0.753113   0.50358                                                                               \n",
      "    6      0.696828   0.754551   0.504383                                                                              \n",
      "    7      0.698813   0.758001   0.50358                                                                               \n",
      "    8      0.699445   0.74268    0.503723                                                                              \n",
      "    9      0.701611   0.759422   0.503442                                                                              \n",
      "    10     0.699671   0.761394   0.504004                                                                              \n",
      "    11     0.699702   0.754919   0.503442                                                                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.75492]), 0.5034417833375088]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(lrs/50, 2, metrics=[accuracy], cycle_len=4, cycle_save_name='gender_class4', cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postmortem\n",
    "\n",
    "The model doesn't really go anywhere. As expected the crappy language model created a crappy classifier. I need to make some changes to really try to fix this. The language model takes way too long to train to really experiment with it.\n",
    "\n",
    "I wonder if I could create the corpus in a more intelligent way. That or just accept that any serious attempt at this project will require several days of training time.\n",
    "\n",
    "Despite my frustrations with this project, I'm glad I took the time to attempt it. I think language modeling is really interesting and it was fun to try it out. With all the talk about transfer learning in NLP going around lately, maybe I'll come back to this with a pretrained language model and focus more on the classification aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
