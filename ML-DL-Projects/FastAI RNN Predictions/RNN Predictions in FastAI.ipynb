{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with RNNs in FastAI - Avoiding Infinite Loops\n",
    "\n",
    "Some people, myself included, have run into RNNS made with FastAI spitting out the same sentence over and over again in a loop. This can be solved with a small tweak to the way the prediction vector is evaluated. This applies to any `Learner` created from a `LanguageModelData` object and works for character level or word level RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background - Character Level RNN predictions\n",
    "\n",
    "I ran into this issue trying to create a `Learner` version of the Nietzsche RNN. Compare the output predictions from the models below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: LSTM Model From Lesson 6\n",
    "\n",
    "This is directly from Lesson 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'models', 'nietzsche.txt', 'trn', 'val']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "os.listdir(f'{PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 55, 1, 482972)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=16; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a59f2de0cca42289474c47662e04e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      1.80958    1.707095  \n",
      "    1      1.687597   1.619923                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.61992])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f5f4bb55264ce28185cb40d7e64482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      1.536625   1.478161  \n",
      "    1      1.55555    1.482114                                                                                         \n",
      "    2      1.445336   1.420886                                                                                         \n",
      "    3      1.582081   1.521532                                                                                         \n",
      "    4      1.496563   1.441881                                                                                         \n",
      "    5      1.415837   1.391549                                                                                         \n",
      "    6      1.360574   1.372136                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.37214])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 7, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those aristocracy of whichdo seconsthat as even a culture \"sees?     heastand and to say for there may according required shappensary reputients.[19] perhaps almost conceptions\", the restraction, theprecessonizes--obversions, are synthesistic worth:--the present to dices in the world. the satisfacts, such no nastingly; and short. once from anlights of presumptice from attricknings(his will in a my dis repured from the uniteral degree--platory: no doubt, it is that the how is to the cultured craffined; then well only anything to decided, askmater, only present to dream to the drscerned to has reverence is something physics with some usual wanting whom hesit.\" but in them of causethe namely manage thewhatever) in fauth: there good too egoists, my devilposed 'years to theeperhams, however, preimitable commanding,womanythingfully, and sumplantly suffering, and feels how first, and kinds. the eetern: howlustom, and putfaith and actuallybecome to art of their soul? but it late hout victo) of the\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is clearly working. Compare this output to what happens when we instead create a `Learner` from the `LanguageModelData`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Model Created from LanguageModelData.get_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = n_fac  \n",
    "nh = n_hidden     \n",
    "nl = 2       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e195c0e01dc1459a92b9f67bcdd2915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      1.762447   1.660329  \n",
      "    1      1.647266   1.559843                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.55984])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 2, wds=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505fa19592b44b2e99914505324fc9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                                                         \n",
      "    0      1.513945   1.437034  \n",
      "    1      1.548548   1.463759                                                                                         \n",
      "    2      1.446684   1.400717                                                                                         \n",
      "    3      1.576475   1.487075                                                                                         \n",
      "    4      1.515214   1.437371                                                                                         \n",
      "    5      1.436331   1.388373                                                                                         \n",
      "    6      1.374034   1.368162                                                                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.36816])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 3, wds=1e-5, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'exp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3b28ce9eb6d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_next_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'for thos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-edc2f4b73f5f>\u001b[0m in \u001b[0;36mget_next_n\u001b[1;34m(inp, n)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4647cae1d09a>\u001b[0m in \u001b[0;36mget_next\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0midxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'exp'"
     ]
    }
   ],
   "source": [
    "m = learner.model\n",
    "print(get_next_n('for thos', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a prediction error due because the outputs from the Learner model aren't in the same form as the CharSeqStatefulLSTM. To get around this, we use the prediction method from Lesson 4, used with the IMDB language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m=learner.model\n",
    "ss= \"\"\" for thos\"\"\"\n",
    "t=TEXT.numericalize(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set batch size to 1\n",
    "m[0].bs=1\n",
    "# Turn off dropout\n",
    "m.eval()\n",
    "# Reset hidden state\n",
    "m.reset()\n",
    "# Get predictions from model\n",
    "res,*_ = m(t)\n",
    "# Put the batch size back to what it was\n",
    "m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'e', 't', ',', 'o', 'i', 'u', '.', '-', 'h']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for thos \n",
      "\n",
      " of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sens...\n"
     ]
    }
   ],
   "source": [
    "print(ss,\"\\n\")\n",
    "for i in range(1000):\n",
    "    n=res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0]==0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end='')\n",
    "    res,*_ = m(n[0].unsqueeze(0))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loop\n",
    "\n",
    "So what happened here? The LSTM model, which seemed to function fine when we made it from scratch, is now suddenly predicting the same thing over and over. The difference from what I have been able to determine is that in the first model, we choose the next character using `torch.multinomial`, while for the second model, we use `torch.topk`. I'm not too certain about the causes for this, but it looks like `torch.topk` is a poor choice in this scenario.\n",
    "\n",
    "We can fix the looping problem by applying `torch.multinomial` instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    res, *_ = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(res[-1].exp(), 2)\n",
    "    if r.data[0] == 0:\n",
    "        r = r[1]\n",
    "    else:\n",
    "        r = r[0]\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those and man seem power, interroganism we repro[uss, that equality, system in mankind us. as an awory\"--of it stacgorificance,too love oftheir age, we have also, andweregour of every tly, and have abyss of th--the degree at errordious things? to assimilar of the belongs?--for the retencertain \"develops.60. e: are long itbeing ill thinking an advantage ofks, very feel usual to nature whether unfavour that pathosising nature upon his thinking out of pro5ulimatic lase for its past the<pad>part with principle, every cultivity, and out of people rights of dinestates are been, always it finners tas matter, of the nature), would not also to the morality, whichin daring modesty, passian victage beinbe mlm, in    any 13gormands\"at things, that is to a [intdo youngstill have (the earing of life to character the ideas what say there is criticism must ne suchindifference to them wnehose tale[able, and cover messed. i let us real completer that they schoadiss and overcontemplations--in thesympbojectment l\n"
     ]
    }
   ],
   "source": [
    "m = learner.model\n",
    "print(get_next_n('for thos', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now everything is working again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Models\n",
    "\n",
    "The same problem and solution exists in word level models. Using the IMDB model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'imdb.vocab',\n",
       " 'imdbEr.txt',\n",
       " 'models',\n",
       " 'README',\n",
       " 'test',\n",
       " 'tmp',\n",
       " 'train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH='data/aclImdb/'\n",
    "\n",
    "TRN_PATH = 'train/all/'\n",
    "VAL_PATH = 'test/all/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48; bptt=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  \n",
    "nh = 500     \n",
    "nl = 3       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('adam3_10_enc_py03_full_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m=learner.model\n",
    "ss=\"\"\". So, it wasn't quite was I was expecting, but I really liked it anyway! The best\"\"\"\n",
    "s = [TEXT.tokenize(ss)]\n",
    "t=TEXT.numericalize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set batch size to 1\n",
    "m[0].bs=1\n",
    "# Turn off dropout\n",
    "m.eval()\n",
    "# Reset hidden state\n",
    "m.reset()\n",
    "# Get predictions from model\n",
    "res,*_ = m(t)\n",
    "# Put the batch size back to what it was\n",
    "m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". So, it wasn't quite was I was expecting, but I really liked it anyway! The best \n",
      "\n",
      "of the film 's main character . the film is a bit of a mess , but it 's a good film . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the actors were very good . i think the director did a great job of making the movie . i would recommend this movie to anyone who wants to see a good movie . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the actors were very good . i think the director did a great job of making the movie . i would recommend this movie to anyone who wants to see a good movie . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the actors were very good . i think the director did a great job of making the movie . i would recommend this movie to anyone who wants to see a good movie . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the actors were very good . i think the director did a great job of making the movie . i would recommend this movie to anyone who wants to see a good movie . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the actors were very good . i think the director did a great job of making the movie . i would recommend this movie to anyone who wants to see a good movie . <eos> i saw this movie at the toronto film festival and it was a great movie . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i was very impressed with the acting . i think the ...\n"
     ]
    }
   ],
   "source": [
    "print(ss,\"\\n\")\n",
    "for i in range(500):\n",
    "    n=res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0]==0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "    res,*_ = m(n[0].unsqueeze(0))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the repeats are less obvious but we can see most of the predictions are variants of the same sentence repeated over and over again. We can apply the same fix as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_str(s): return TEXT.preprocess(TEXT.tokenize(s))\n",
    "def num_str(s): return TEXT.numericalize([proc_str(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(m, s, l=50):\n",
    "    t = num_str(s)\n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(l):\n",
    "        r = torch.multinomial(res[-1].exp(), 2)\n",
    "        if r.data[0] == 0:\n",
    "            r = r[1]\n",
    "        else:\n",
    "            r = r[0]\n",
    "        word = TEXT.vocab.itos[to_np(r)[0]]\n",
    "        res, *_ = m(r[0].unsqueeze(0))\n",
    "        print(word, end=' ')\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...part of the film was the final scene when the son wants owen to run away from others.<br /><br cronies did work with him on this cartoon . how this man can immersed guy and his family on a roses etc . , is the real reason i 'm still saying hurt a masterson 's must do it because he keeps from going ' through the eye . which was what i especially believe it is . and anymore : it was a wonderful mix where a person on earth could watch something so difficult to make . every reasoning about 9.5/10 there was never the emotion munez after . it was like the pollution that the people died in making the world and it is dutch movie you - guess you never want to see , anyone says or does it . i 'm shoving anyone back with this show ! . ! ! ! ! ! only diy and hoo ... showcase them for that great time joke and the show .... try to please 2 bucks and down out to get back to my favorite game now ! <eos> well thought i 'll give it a 9 , at least on it 's way . take me the time . the person who wrote this film is right which says i 'm attracted to the first abomination . humor is so cry - so that it 's an above average movie that makes one wonder why it still is banish across \" hysterics \" . it 's all kind of like the bringing room of a teenager like julia roberts for mystery science theater 3000 . while it 's probably />hollywood 's film tv so he wants to leave it why the hell he should make a franchise like this , then he let the straight actors get away with what they 're doing in that.<br /><br />see this one in grit and produced a bit if you 're familiar with the scripts , which are far more exciting and fair , it 's still worth entertainment . if you can sit through it , if you want to be a child . real <eos> macabre work by the secrets of a man 's soul , a gruesome depardieu and intriguing psychological thriller with a love affair middle - noir , at least multi - wannabe north det . discharged set in chicago , with more usual emotionless acting re - vintage hailed as brett discipline . the unexplained gaps in pretentious.<br 's estate is accomplished in this stock flaw . helper of the swimming pool is an admirable , appearing , and sadly slightly ridiculous . so i do n't care what the producers ( an italian director ) , go from odd to slop with mind pic , but like bottom.<br interesting prey whose characterization of its own jason.<br /><br />the cast for this film is veiled - the fact that mel gibson is no "
     ]
    }
   ],
   "source": [
    "sample_model(m, ss, l=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're seeing much more variation in the output. On the subject of predictions, the prediction format for the language model used a single word at a time, while the character level model keeps a rolling section of the output texts for each sequential prediction. We can modify the prediction functions for the character model to work with word models to accomplish the same thing. I haven't played around with this enough to understand the effect of it, but here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = num_str(inp)\n",
    "    res, *_ = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(res[-1].exp(), 2)\n",
    "    if r.data[0] == 0:\n",
    "        r = r[1]\n",
    "    else:\n",
    "        r = r[0]\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res_lst = inp.split(' ')\n",
    "    try:\n",
    "        for i in range(n):\n",
    "            c = get_next(inp)\n",
    "            res_lst += [c]\n",
    "            inp_lst = inp.split(' ')\n",
    "            inp_lst += [c]\n",
    "            inp = \"\"\" \"\"\".join(inp_lst[1:])\n",
    "        return \"\"\" \"\"\".join(res_lst)\n",
    "    except:\n",
    "        print(res_lst)\n",
    "        print(c)\n",
    "        print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learner.model\n",
    "m.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". So, it wasn't quite was I was expecting, but I really liked it anyway! The best to feel - good heartedness . the untouchables : * * ( # ) unborn minions with the leather legs for some way ( popularity ! ) ! <eos> networks might be too free to do.<br /><br />there were so many things to be found in our world but the only reason why i liked it was because i took it first to fireball his guru . that 's the promote . i think joel fail to explain why he soured the tag line at the end of the film but it 's all set up . just ran out in 1948 , only disorientation . this film is so good its obvious what would helping the writers live their shocking.<br /><br />it 's been a long time since i 'd out to see this movie . it 's brilliant . aside from being stupid , it 's a refreshing exercise than my head belgian . the casting is bad . the story does r auspicious , out.<br /><br />too many different imagery . i feel that this will be seeing rather for correct words for someone who rises above its italian central point of view . a definite must or for fans of well photographed indie films , and anyone i 've been wild , nd and old figures have most of the gentle tension ( evelyn sorrows future.<br /><br anu minutes of celluloid , such as you think i actually can ? a car dump put into a prices amateurish.<br /><br telepods around them is not this 1/2 out of 10 . <eos> take for example \" the five - two gawd x \" it was 7/10<br /><br diaz 's time not to be judged ... or is it ? forged for people who see a film on groundbreaking dramatized cinema . they should be scared lennox . i 'll stay at home on the huge list of stars on the cast in a few parts that should not take a psychiatrist scene . if you want to see them what you compare with old hollywood people.<br /><br />watch it . ( 7 out of 10 ) <eos> in one scene you are right , with a wide fund ! the free cathedral seem to the solomon 's glass park futurama ? i did n't find much to think about , let alone in drugs . < br /><br raven was at the best of years , but the vagrant episode of ' stay night reused ' and that was the congratulated way to give antz an experimental instead of a climactic chase scene , an stag ! ! the final sequence is good />mainly dialects riff on schaeffer 's disgraced attitude , but the truth here is that the aim is no objectives to make creeps . pay off you . i am sure that the t.v admirably though coasts is top billed and is imparts to the gimmick made of using his skill to shine a\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n(ss, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what does this mean?\n",
    "\n",
    "From what I understand `torch.topk` returns the index of the n largest elements in the output vector, while `torch.multinomial` pulls n samples from the output vector using the values of the elements in the vector as weights in a probability distribution. I think the model learns that certain words come up all the time, so simply sampling the top valued outputs leads to loops. The added variance from the `torch.multinomial` method leads to a much more varied output from the RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
