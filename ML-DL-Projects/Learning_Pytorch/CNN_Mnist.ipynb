{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1178,
     "status": "ok",
     "timestamp": 1553520452195,
     "user": {
      "displayName": "Omer Sezer",
      "photoUrl": "",
      "userId": "08295833296445258983"
     },
     "user_tz": -180
    },
    "id": "3ve5cf1rBYJ-",
    "outputId": "1bd3bf00-6083-4cba-fe04-9ef424335563"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3498,
     "status": "ok",
     "timestamp": 1553520543462,
     "user": {
      "displayName": "Omer Sezer",
      "photoUrl": "",
      "userId": "08295833296445258983"
     },
     "user_tz": -180
    },
    "id": "aWZv20ldCO6k",
    "outputId": "d643e3c5-f09f-46e4-aaa3-17f0905c2ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 8792432.81it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 135716.04it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2189782.63it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 50679.80it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset= datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset= datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lB1REqGi0yDC"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once in a notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once in a notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Create & upload a text file.\n",
    "uploaded = drive.CreateFile({'title': 'Sample file.txt'})\n",
    "uploaded.SetContentString('Sample upload file content')\n",
    "uploaded.Upload()\n",
    "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xd7zyv1qCmir"
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs=10\n",
    "train_load=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_load=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1291,
     "status": "ok",
     "timestamp": 1553522392728,
     "user": {
      "displayName": "Omer Sezer",
      "photoUrl": "",
      "userId": "08295833296445258983"
     },
     "user_tz": -180
    },
    "id": "qu6NepleC8FO",
    "outputId": "d33c1fab-4dd6-4e54-d405-45bfc3c9f055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training set: 60000\n",
      "Number of images in test set: 10000\n",
      "Number of batches in the train loader: 600\n",
      "Number of batches in the test loader: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images in training set: {}\".format(len(train_dataset)))\n",
    "print(\"Number of images in test set: {}\".format(len(test_dataset)))\n",
    "print(\"Number of batches in the train loader: {}\".format(len(train_load)))\n",
    "print(\"Number of batches in the test loader: {}\".format(len(test_load)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDWXQzzbKHwV"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN,self).__init__()\n",
    "    # input_size:28, same_padding=(filter_size-1)/2, 3-1/2=1:padding\n",
    "    self.cnn1=nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    # input_size-filter_size +2(padding)/stride + 1 = 28-3+2(1)/1+1=28\n",
    "    self.batchnorm1=nn.BatchNorm2d(8)\n",
    "    # output_channel:8, batch(8)\n",
    "    self.relu=nn.ReLU()\n",
    "    self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "    #input_size=28/2=14\n",
    "    self.cnn2=nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "    # same_padding: (5-1)/2=2:padding_size. \n",
    "    self.batchnorm2=nn.BatchNorm2d(32)\n",
    "    self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "    # input_size=14/2=7\n",
    "    # 32x7x7=1568\n",
    "    self.fc1 =nn.Linear(in_features=1568, out_features=600)\n",
    "    self.dropout= nn.Dropout(p=0.5)\n",
    "    self.fc2 =nn.Linear(in_features=600, out_features=10)\n",
    "  def forward(self,x):\n",
    "    out =self.cnn1(x)\n",
    "    out =self.batchnorm1(out)\n",
    "    out =self.relu(out)\n",
    "    out =self.maxpool1(out)\n",
    "    out =self.cnn2(out)\n",
    "    out =self.batchnorm2(out)\n",
    "    out =self.relu(out)\n",
    "    out =self.maxpool2(out)\n",
    "    out =out.view(-1,1568)\n",
    "    out =self.fc1(out)\n",
    "    out =self.relu(out)\n",
    "    out =self.dropout(out)\n",
    "    out =self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rLx8eZaRmD4"
   },
   "outputs": [],
   "source": [
    "model=CNN()\n",
    "CUDA=torch.cuda.is_available()\n",
    "if CUDA:\n",
    "  model=model.cuda()\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 169535,
     "status": "ok",
     "timestamp": 1553527101813,
     "user": {
      "displayName": "Omer Sezer",
      "photoUrl": "",
      "userId": "08295833296445258983"
     },
     "user_tz": -180
    },
    "id": "51tCtmnCSJJ0",
    "outputId": "1d998904-a524-4cfb-f09f-d4504d63a3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100, Train Loss: 0.0857103243470192, Test Accuracy:98%\n",
      "Iteration: 200, Train Loss: 0.20697177946567535, Test Accuracy:98%\n",
      "Iteration: 300, Train Loss: 0.06593213230371475, Test Accuracy:98%\n",
      "Iteration: 400, Train Loss: 0.0648646429181099, Test Accuracy:98%\n",
      "Iteration: 500, Train Loss: 0.02223188430070877, Test Accuracy:98%\n",
      "Iteration: 600, Train Loss: 0.032660093158483505, Test Accuracy:98%\n",
      "Iteration: 700, Train Loss: 0.0625385269522667, Test Accuracy:98%\n",
      "Iteration: 800, Train Loss: 0.09954184293746948, Test Accuracy:98%\n",
      "Iteration: 900, Train Loss: 0.07626273483037949, Test Accuracy:98%\n",
      "Iteration: 1000, Train Loss: 0.09606733918190002, Test Accuracy:98%\n",
      "Iteration: 1100, Train Loss: 0.04067772999405861, Test Accuracy:98%\n",
      "Iteration: 1200, Train Loss: 0.07098077982664108, Test Accuracy:98%\n",
      "Iteration: 1300, Train Loss: 0.05420248582959175, Test Accuracy:98%\n",
      "Iteration: 1400, Train Loss: 0.02750409208238125, Test Accuracy:98%\n",
      "Iteration: 1500, Train Loss: 0.06596118211746216, Test Accuracy:98%\n",
      "Iteration: 1600, Train Loss: 0.017068618908524513, Test Accuracy:98%\n",
      "Iteration: 1700, Train Loss: 0.09589124470949173, Test Accuracy:98%\n",
      "Iteration: 1800, Train Loss: 0.09299355745315552, Test Accuracy:98%\n",
      "Iteration: 1900, Train Loss: 0.06871959567070007, Test Accuracy:98%\n",
      "Iteration: 2000, Train Loss: 0.04811862111091614, Test Accuracy:98%\n",
      "Iteration: 2100, Train Loss: 0.018543705344200134, Test Accuracy:98%\n",
      "Iteration: 2200, Train Loss: 0.06499851495027542, Test Accuracy:98%\n",
      "Iteration: 2300, Train Loss: 0.07531427592039108, Test Accuracy:98%\n",
      "Iteration: 2400, Train Loss: 0.10248444229364395, Test Accuracy:98%\n",
      "Iteration: 2500, Train Loss: 0.014873662032186985, Test Accuracy:98%\n",
      "Iteration: 2600, Train Loss: 0.019387083128094673, Test Accuracy:98%\n",
      "Iteration: 2700, Train Loss: 0.02942785806953907, Test Accuracy:98%\n",
      "Iteration: 2800, Train Loss: 0.06560874730348587, Test Accuracy:98%\n",
      "Iteration: 2900, Train Loss: 0.015282545238733292, Test Accuracy:98%\n",
      "Iteration: 3000, Train Loss: 0.025859808549284935, Test Accuracy:98%\n",
      "Iteration: 3100, Train Loss: 0.01010643970221281, Test Accuracy:98%\n",
      "Iteration: 3200, Train Loss: 0.04190950468182564, Test Accuracy:98%\n",
      "Iteration: 3300, Train Loss: 0.05604088306427002, Test Accuracy:98%\n",
      "Iteration: 3400, Train Loss: 0.016772184520959854, Test Accuracy:98%\n",
      "Iteration: 3500, Train Loss: 0.041676707565784454, Test Accuracy:98%\n",
      "Iteration: 3600, Train Loss: 0.035578664392232895, Test Accuracy:98%\n",
      "Iteration: 3700, Train Loss: 0.028997458517551422, Test Accuracy:98%\n",
      "Iteration: 3800, Train Loss: 0.03516755998134613, Test Accuracy:98%\n",
      "Iteration: 3900, Train Loss: 0.14350377023220062, Test Accuracy:98%\n",
      "Iteration: 4000, Train Loss: 0.033672112971544266, Test Accuracy:98%\n",
      "Iteration: 4100, Train Loss: 0.050937581807374954, Test Accuracy:98%\n",
      "Iteration: 4200, Train Loss: 0.00470767030492425, Test Accuracy:98%\n",
      "Iteration: 4300, Train Loss: 0.06932661682367325, Test Accuracy:98%\n",
      "Iteration: 4400, Train Loss: 0.028644122183322906, Test Accuracy:98%\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "for epoch in range(epochs):\n",
    "  for i, (images,labels) in enumerate(train_load):\n",
    "    iteration+=1\n",
    "    if CUDA:\n",
    "      images =Variable(images.cuda())\n",
    "      labels =Variable(labels.cuda())\n",
    "    else:\n",
    "      images =Variable(images)\n",
    "      labels =Variable(labels)\n",
    "      \n",
    "    optimizer.zero_grad()\n",
    "    outputs=model(images)\n",
    "    loss=loss_function(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i+1)%100 ==0:\n",
    "        correct =0\n",
    "        total =0\n",
    "        for images,labels in test_load:\n",
    "            if CUDA:\n",
    "              images =Variable(images.cuda())\n",
    "            else:\n",
    "              images =Variable(images)\n",
    "\n",
    "            outputs=model(images)\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            total+=labels.size(0)\n",
    "            if CUDA:\n",
    "              correct += (predicted.cpu()==labels.cpu()).sum()\n",
    "            else:\n",
    "              correct += (predicted==labels).sum()\n",
    "\n",
    "        accuracy = 100 *correct/total\n",
    "        print(\"Iteration: {}, Train Loss: {}, Test Accuracy:{}%\".format(iteration, loss.item(),accuracy))\n",
    "        \n",
    "print(\"Finished!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
