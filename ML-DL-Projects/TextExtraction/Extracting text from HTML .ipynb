{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml \n",
    "\n",
    "from time import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def html_to_text(html):\n",
    "    \"Creates a formatted text email message as a string from a rendered html template (page)\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Ignore anything in head\n",
    "    body, text = soup.body, []\n",
    "    for element in body.descendants:\n",
    "        # We use type and not isinstance since comments, cdata, etc are subclasses that we don't want\n",
    "        if type(element) == NavigableString:\n",
    "            parent_tags = (t for t in element.parents if type(t) == Tag)\n",
    "            hidden = False\n",
    "            for parent_tag in parent_tags:\n",
    "                # Ignore any text inside a non-displayed tag\n",
    "                # We also behave is if scripting is enabled (noscript is ignored)\n",
    "                # The list of non-displayed tags and attributes from the W3C specs:\n",
    "                if (parent_tag.name in ('area', 'base', 'basefont', 'datalist', 'head', 'link',\n",
    "                                        'meta', 'noembed', 'noframes', 'param', 'rp', 'script',\n",
    "                                        'source', 'style', 'template', 'track', 'title', 'noscript') or\n",
    "                    parent_tag.has_attr('hidden') or\n",
    "                    (parent_tag.name == 'input' and parent_tag.get('type') == 'hidden')):\n",
    "                    hidden = True\n",
    "                    break\n",
    "            if hidden:\n",
    "                continue\n",
    "\n",
    "            # remove any multiple and leading/trailing whitespace\n",
    "            string = ' '.join(element.string.split())\n",
    "            if string:\n",
    "                if element.parent.name == 'a':\n",
    "                    a_tag = element.parent\n",
    "                    # replace link text with the link\n",
    "                    string = a_tag['href']\n",
    "                    # concatenate with any non-empty immediately previous string\n",
    "                    if (    type(a_tag.previous_sibling) == NavigableString and\n",
    "                            a_tag.previous_sibling.string.strip() ):\n",
    "                        text[-1] = text[-1] + ' ' + string\n",
    "                        continue\n",
    "                elif element.previous_sibling and element.previous_sibling.name == 'a':\n",
    "                    text[-1] = text[-1] + ' ' + string\n",
    "                    continue\n",
    "                elif element.parent.name == 'p':\n",
    "                    # Add extra paragraph formatting newline\n",
    "                    string = '\\n' + string\n",
    "                text += [string]\n",
    "    doc = '\\n'.join(text)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def get_text_bs(html):\n",
    "    tree = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    body = tree.body\n",
    "    if body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in body.select('script'):\n",
    "        tag.decompose()\n",
    "    for tag in body.select('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = body.get_text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_selectolax(html):\n",
    "    tree = HTMLParser(html)\n",
    "\n",
    "    if tree.body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in tree.css('script'):\n",
    "        tag.decompose()\n",
    "    for tag in tree.css('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = tree.body.text(separator='\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://sambaiga.github.io/2017/05/03/hmm-intro.html'\n",
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n  sambaiga\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu:\\n\\n\\n\\n\\nAbout\\n\\n\\nBlog\\n\\n\\nProjects\\n\\n\\nResources\\n\\n\\nTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Basic of Hidden Markov Model\\n\\n\\n\\n\\n\\n          \\n\\n  May 3, 2017\\n\\n\\n        \\n\\n\\n·\\n\\n    \\n\\n    last updated on\\n    \\n\\n  Oct 1, 2017\\n\\n\\n\\n    \\n\\n    \\n\\n\\n\\n\\nThis post review basic of HMM and its implementation in Python.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\n\\n\\nHMM is a Markov model whose states are not directly observed instead each state is characterised by a probability distribution function modelling the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM.\\n\\n\\nThere are two variables in HMM: observed variables and hidden variables where the sequences of hidden variables forms a Markov process as shown in figure below. In the context of NILM, the hidden variables are used to model states(ON,OFF, standby etc) of individual appliances and the observed variables are used to model the electric usage. HMMs has been widely used in most of the recently proposed NILM approach because it represents well the individual appliance internal states which are not directly observed in the targeted energy consumption.\\n\\n\\n\\n\\n\\n\\nHMM graphical model\\n  \\n\\n\\n\\n\\nA typical HMM is characterised by the following:\\n\\n\\n\\n\\nThe finite set of hidden states  \\n (e.g ON, stand-by, OFF, etc.) of an appliance,  \\n.\\n\\n\\nThe finite set of  \\n observable symbol  \\n per states (power consumption) observed in each state,  \\n. The observable symbol  \\n can be discrete or a continuous set.\\n\\n\\nThe transition matrix   \\n represents the probability of moving from state  \\n to  \\n such that:  \\n, with  \\n and where  \\n denotes the state occupied by the system at time  \\n. The matrix  \\n is  \\n.\\n\\n\\nThe emission matrix  \\n representing the probability of emission of symbol  \\n \\n \\n when system state is  \\n such that:  \\n The matrix  \\n is an  \\n. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission.\\n\\n\\nAnd the initial state probability distribution  \\n indicating the probability of each state of the hidden variable  at  \\n such that,  \\n.\\n\\n\\n\\n\\nThe complete HMM specification requires;\\n\\n\\n\\n\\nFinite set of hidden states \\n and observation symbols \\n\\n\\n\\n\\nLength of observation seqences \\n and\\n\\n\\nSpecification of three probability measures \\n and \\n\\n\\n\\n\\n\\n\\nThe set of all HMM model parameters is represented by \\n.\\n\\n\\nSince \\n is not observed, the likelihood function \\n is given by the joint distribution of \\n and \\n over all possible state.\\n\\n\\n\\n\\nwhere\\n\\n\\n\\n\\nNote that \\n is independent and identically distributed given state sequence \\n. Also each state at time \\n depend on the state at its previous time \\n. Then\\n\\n\\n\\n\\nSimilary\\n\\n\\n\\n\\nThe joint probability is therefore:\\n\\n\\n\\n\\nThree main problems in HMMs\\n\\n\\nWhen applying HMM to a real world problem, three important problem must be solved.\\n\\n\\n\\n\\nEvaluation Problem: Given HMM parameters \\n and the observation seqence \\n, find \\n the likelihood of the observation sequence \\n given the model \\n. This problem give a score on how well a given model matches a given observation and thus allows you to choose the model that best match the observation.\\n\\n\\nDecoding Problem: Given HMM parameters \\n and the observation seqence \\n, find an optimal state sequense \\n which best explain the observation.This problem attempt to cover the hidden part of the model.\\n\\n\\nLearning Problem: Given the obseravtion seqence \\n, find the model parameters \\n that maximize \\n.This problem attempt to optimize the model parameters so as to describe the model.\\n\\n\\n\\n\\nThe first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post.\\n\\n\\nSolution to Problem 1\\n\\n\\nA straight forward way to solve this problem is to find \\n for fixed state sequences \\n and then sum up over all possible states. This is generally infeasible since it requires about \\n multiplications. However this problem can be efficiently solved by using the forward algorithm  as follows:\\n\\n\\nThe forward-backward Algorithm\\n\\n\\nLet us define the \\nforward variable\\n\\n\\n\\n\\nthe probability of the partial observation sequences \\n  up to time \\n and the state \\n at time \\n given the model \\n. We also define an emission probability given HMM state \\n at time \\n as \\n.\\n\\n\\nForward-Algorithm\\n\\n\\nInitilization\\n\\n\\nLet\\n\\n\\n\\n\\nInduction\\n\\n\\nFor \\n and \\n, compute:\\n\\n\\n\\n\\nTermination\\n\\n\\nFrom \\n, it cear that:\\n\\n\\n\\n\\nThe forward algorithm only requires about \\n multiplications and is it can be implemented in python as follows.\\n\\n\\ndef\\n \\nforward\\n(\\nobs_seq\\n):\\n\\n        \\nT\\n \\n=\\n \\nlen\\n(\\nobs_seq\\n)\\n\\n        \\nN\\n \\n=\\n \\nA\\n.\\nshape\\n[\\n0\\n]\\n\\n        \\nalpha\\n \\n=\\n \\nnp\\n.\\nzeros\\n((\\nT\\n,\\n \\nN\\n))\\n\\n        \\nalpha\\n[\\n0\\n]\\n \\n=\\n \\npi\\n*\\nB\\n[:,\\nobs_seq\\n[\\n0\\n]]\\n\\n        \\nfor\\n \\nt\\n \\nin\\n \\nrange\\n(\\n1\\n,\\n \\nT\\n):\\n\\n            \\nalpha\\n[\\nt\\n]\\n \\n=\\n \\nalpha\\n[\\nt\\n-\\n1\\n]\\n.\\ndot\\n(\\nA\\n)\\n \\n*\\n \\nB\\n[:,\\n \\nobs_seq\\n[\\nt\\n]]\\n\\n        \\nreturn\\n \\nalpha\\n\\n\\n \\ndef\\n \\nlikelihood\\n(\\nobs_seq\\n):\\n\\n        \\n# returns log P(Y  \\\\mid  model)\\n\\n        \\n# using the forward part of the forward-backward algorithm\\n\\n        \\nreturn\\n  \\nforward\\n(\\nobs_seq\\n)[\\n-\\n1\\n]\\n.\\nsum\\n()\\n      \\n\\n\\n\\nBackward Algorithm\\n\\n\\nThis is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the \\nbackward variable\\n \\n: probability of the partial observed sequence from \\n to the end at \\n given state \\n at time \\n and the model \\n.\\n\\n\\nThen \\n can be computed recursively as follows.\\n\\n\\nInitilization\\n\\n\\nLet \\n, for \\n\\n\\nInduction\\n\\n\\nFor \\n for \\n and by using the sum and product rules, we can rewrite \\n as:\\n\\n\\n\\n\\nTermination\\n\\n\\n\\n\\nPython implementation of forward algorithm is as shown below;\\n\\n\\ndef\\n \\nbackward\\n(\\nobs_seq\\n):\\n\\n        \\nN\\n \\n=\\n \\nA\\n.\\nshape\\n[\\n0\\n]\\n\\n        \\nT\\n \\n=\\n \\nlen\\n(\\nobs_seq\\n)\\n\\n\\n        \\nbeta\\n \\n=\\n \\nnp\\n.\\nzeros\\n((\\nN\\n,\\nT\\n))\\n\\n        \\nbeta\\n[:,\\n-\\n1\\n:]\\n \\n=\\n \\n1\\n\\n\\n        \\nfor\\n \\nt\\n \\nin\\n \\nreversed\\n(\\nrange\\n(\\nT\\n-\\n1\\n)):\\n\\n            \\nfor\\n \\nn\\n \\nin\\n \\nrange\\n(\\nN\\n):\\n\\n                \\nbeta\\n[\\nn\\n,\\nt\\n]\\n \\n=\\n \\nnp\\n.\\nsum\\n(\\nbeta\\n[:,\\nt\\n+\\n1\\n]\\n \\n*\\n \\nA\\n[\\nn\\n,:]\\n \\n*\\n \\nB\\n[:,\\n \\nobs_seq\\n[\\nt\\n+\\n1\\n]])\\n\\n\\n        \\nreturn\\n \\nbeta\\n\\n\\n\\n\\nPosterior Probability\\n\\n\\nThe forward variable \\n and backward variable \\n are used to calculate the posterior probability of a specific case. Now for \\n and \\n, let define posterior probability \\n the probability of being in state \\n at time \\n given the observation \\n and the model \\n.\\n\\n\\n\\n\\nConsider:\\n\\n\\n\\n\\nThus\\n\\n\\n\\n\\nwhere\\n\\n\\n\\n\\nIn python:\\n\\n\\ndef\\n \\ngamma\\n(\\nobs_seq\\n):\\n\\n    \\nalpha\\n \\n=\\n \\nforward\\n(\\nobs_seq\\n)\\n\\n    \\nbeta\\n  \\n=\\n \\nbackward\\n(\\nobs_seq\\n)\\n\\n    \\nobs_prob\\n \\n=\\n \\nlikelihood\\n(\\nobs_seq\\n)\\n\\n    \\nreturn\\n \\n(\\nnp\\n.\\nmultiply\\n(\\nalpha\\n,\\nbeta\\n.\\nT\\n)\\n \\n/\\n \\nobs_prob\\n)\\n\\n\\n\\n\\nWe can use \\n to find the most likely state at time \\n which is the state \\n for which \\n is maximum. This algorithm \\nworks fine in the case when HMM is ergodic\\n i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem \\nViterbi algorithm\\n is the most common decoding algorithms used.\\n\\n\\nViterbi Algorithm\\n\\n\\nViterbi is a kind of dynamic programming algorithm that make uses of a dynamic programming trellis.\\n\\n\\nThe virtebi algorithm offer an efficient way of finding  the single best state sequence.Let define the highest probability along a single path, at time \\n, which accounts for the first \\n observations and ends in state \\n using a new notation:\\n\\n\\n\\n\\nBy induction, a recursive formula of \\n from \\n  is derived to calculate this probability as follows:\\n\\n\\nConsider the joint distribution appearing in \\n, which can be rewritten when \\n and \\n as:\\n\\n\\n\\n\\nThus \\n  is computed recursively from \\n as:\\n\\n\\n\\n\\nWe therefore need to keep track the state that maximize the above equation so as to backtrack to the single best state sequence in the following Viterbi algorithm:\\n\\n\\nInitilization\\n\\n\\nFor \\n, let:\\n\\n\\n\\n\\nRecursion\\n\\n\\nCalculate  the ML (maximum likelihood) state sequences and their probabilities. For \\n and \\n\\n\\n\\n\\nTermination\\n:\\n\\n\\nRetrieve the most likely final state\\n\\n\\n\\n\\nState sequence backtracking\\n:\\n\\n\\nRetrieve  the most likely state sequences (virtebi path)\\n\\n\\n\\n\\nVirtebi algorithm uses the same schema as the Forward algorithm except for two differences:\\n\\n\\n\\n\\nIt uses maximization in place of summation at the recursion and termination steps.\\n\\n\\nIt keeps track of the arguments that maximize \\n for each \\n and \\n, storing them in the N by T matrix \\n. This matrix is used to retrieve the optimal state sequence at the backtracking step.\\n\\n\\n\\n\\nPython implementation of virtebi algorithm\\n\\n\\ndef\\n \\nviterbi\\n(\\nobs_seq\\n):\\n\\n        \\n# returns the most likely state sequence given observed sequence x\\n\\n        \\n# using the Viterbi algorithm\\n\\n        \\nT\\n \\n=\\n \\nlen\\n(\\nobs_seq\\n)\\n\\n        \\nN\\n \\n=\\n \\nA\\n.\\nshape\\n[\\n0\\n]\\n\\n        \\ndelta\\n \\n=\\n \\nnp\\n.\\nzeros\\n((\\nT\\n,\\n \\nN\\n))\\n\\n        \\npsi\\n \\n=\\n \\nnp\\n.\\nzeros\\n((\\nT\\n,\\n \\nN\\n))\\n\\n        \\ndelta\\n[\\n0\\n]\\n \\n=\\n \\npi\\n*\\nB\\n[:,\\nobs_seq\\n[\\n0\\n]]\\n\\n        \\nfor\\n \\nt\\n \\nin\\n \\nrange\\n(\\n1\\n,\\n \\nT\\n):\\n\\n            \\nfor\\n \\nj\\n \\nin\\n \\nrange\\n(\\nN\\n):\\n\\n                \\ndelta\\n[\\nt\\n,\\nj\\n]\\n \\n=\\n \\nnp\\n.\\nmax\\n(\\ndelta\\n[\\nt\\n-\\n1\\n]\\n*\\nA\\n[:,\\nj\\n])\\n \\n*\\n \\nB\\n[\\nj\\n,\\n \\nobs_seq\\n[\\nt\\n]]\\n\\n                \\npsi\\n[\\nt\\n,\\nj\\n]\\n \\n=\\n \\nnp\\n.\\nargmax\\n(\\ndelta\\n[\\nt\\n-\\n1\\n]\\n*\\nA\\n[:,\\nj\\n])\\n\\n\\n        \\n# backtrack\\n\\n        \\nstates\\n \\n=\\n \\nnp\\n.\\nzeros\\n(\\nT\\n,\\n \\ndtype\\n=\\nnp\\n.\\nint32\\n)\\n\\n        \\nstates\\n[\\nT\\n-\\n1\\n]\\n \\n=\\n \\nnp\\n.\\nargmax\\n(\\ndelta\\n[\\nT\\n-\\n1\\n])\\n\\n        \\nfor\\n \\nt\\n \\nin\\n \\nrange\\n(\\nT\\n-\\n2\\n,\\n \\n-\\n1\\n,\\n \\n-\\n1\\n):\\n\\n            \\nstates\\n[\\nt\\n]\\n \\n=\\n \\npsi\\n[\\nt\\n+\\n1\\n,\\n \\nstates\\n[\\nt\\n+\\n1\\n]]\\n\\n        \\nreturn\\n \\nstates\\n\\n\\n\\n\\nTo summarize, we can compute the following from HMM:\\n\\n\\n\\n\\nThe marginalized likelihood function \\n from the forward or backward algorithm.\\n\\n\\nThe posterior probability \\n from the forward–backward algorithm.\\n\\n\\nThe optimal state sequence \\nfrom the Viterbi algorithm.\\n\\n\\nThe segmental joint likelihood function \\n from the Viterbi algorithm.\\n\\n\\n\\n\\nThese values are used in the decoding step and the training step of estimating model parameters \\n.\\n\\n\\nExample 1\\n\\n\\nConside the Bob-Alice example as described \\nhere\\n. Two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.\\n\\n\\nAlice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his activities, those are the observations.\\n\\n\\nstates\\n \\n=\\n \\n(\\n\\'Rainy\\'\\n,\\n \\n\\'Sunny\\'\\n)\\n\\n\\nobservations\\n \\n=\\n \\n(\\n\\'walk\\'\\n,\\n \\n\\'shop\\'\\n,\\n \\n\\'clean\\'\\n)\\n\\n\\npi\\n \\n=\\n \\nnp\\n.\\narray\\n([\\n0.6\\n,\\n \\n0.4\\n])\\n  \\n#initial probability \\n\\nA\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n0.7\\n,\\n \\n0.3\\n],[\\n0.4\\n,\\n \\n0.6\\n]])\\n \\n#Transmission probability \\n\\nB\\n \\n=\\n \\nnp\\n.\\narray\\n([[\\n0.1\\n,\\n \\n0.4\\n,\\n \\n0.5\\n],[\\n0.6\\n,\\n \\n0.3\\n,\\n \\n0.1\\n]])\\n \\n#Emission probability\\n\\n\\n\\nSuppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears.\\n\\n\\nbob_says\\n \\n=\\n \\nnp\\n.\\narray\\n([\\n0\\n,\\n \\n2\\n,\\n \\n1\\n,\\n \\n1\\n,\\n \\n2\\n,\\n \\n0\\n])\\n\\n\\nalice_hears\\n=\\nviterbi\\n(\\nbob_says\\n)\\n\\n\\nprint\\n(\\n\"Bob says:\"\\n,\\n \\n\", \"\\n,\\nlist\\n(\\nmap\\n(\\nlambda\\n \\ny\\n:\\n \\nobserv_bob\\n[\\ny\\n],\\n \\nbob_says\\n)))\\n\\n\\nprint\\n(\\n\"Alice hears:\"\\n,\\n \\n\", \"\\n,\\n \\nlist\\n(\\nmap\\n(\\nlambda\\n \\ns\\n:\\n \\nstates_bob\\n[\\ns\\n],\\n \\nalice_hears\\n)))\\n\\n\\n\\n\\n\\n    (\\'Bob says:\\', \\'walk, clean, shop, shop, clean, walk\\')\\n    (\\'Alice hears:\\', \\'Sunny, Rainy, Rainy, Rainy, Rainy, Sunny\\')\\n\\n\\n\\nThe notebook with codes for the above example can be found in \\nhere\\n\\n\\nReferences\\n\\n\\n\\n\\nL. R. Rabiner, \\nA tutorial on hidden Markov models and selected applications in speech recognition\\n, Proceedings of the IEEE, Vol. 77, No. 2, February 1989.\\n\\n\\nShinji Watanabe, Jen-Tzung Chien, \\nBayesian Speech and Language Processing\\n, Cambridge University Press, 2015.\\n\\n\\nViterbi Algorithm in Speech Enhancement and HMM\\n\\n\\nNikolai Shokhirev, \\nHidden Markov Models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Did you like that post?\\n  \\n\\n  You can suscribe to the\\n  \\nRSS feed\\n\\n  \\n\\n  \\n    or follow\\n    \\n@sambaiga\\n\\n    on Twitter\\n  .\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Anthony Faustine\\n        \\n\\n\\n\\n  PhD machine learning researcher (IDLab, imec, University of Ghent).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     Copyright 2019 \\n    \"© 2017 A.Faustine\"\\n\\n    \\n    : sambaiga@gmail.com\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = get_text_bs(html)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <h2 id=\"introduction\">Introduction</h2>\n",
    "\n",
    "<p>HMM is a Markov model whose states are not directly observed instead each state is characterised by a probability distribution function modelling the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM.</p>\n",
    "\n",
    "<p>There are two variables in HMM: observed variables and hidden variables where the sequences of hidden variables forms a Markov process as shown in figure below. In the context of NILM, the hidden variables are used to model states(ON,OFF, standby etc) of individual appliances and the observed variables are used to model the electric usage. HMMs has been widely used in most of the recently proposed NILM approach because it represents well the individual appliance internal states which are not directly observed in the targeted energy consumption.</p>\n",
    "<figure>\n",
    "  <img src=\"/assets/img/post/hmm.png\" title=\"HMM graphical model\" alt=\"\">\n",
    "  <figcaption>HMM graphical model\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "<p>A typical HMM is characterised by the following:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>The finite set of hidden states  <script type=\"math/tex\">S</script> (e.g ON, stand-by, OFF, etc.) of an appliance,  <script type=\"math/tex\">S = \\{s_1, s_2....,s_N\\}</script>.</li>\n",
    "  <li>The finite set of  <script type=\"math/tex\">M</script> observable symbol  <script type=\"math/tex\">Y</script> per states (power consumption) observed in each state,  <script type=\"math/tex\">Y = \\{y_1, y_2....,y_M\\}</script>. The observable symbol  <script type=\"math/tex\">Y</script> can be discrete or a continuous set.</li>\n",
    "  <li>The transition matrix   <script type=\"math/tex\">\\mathbf{A}=\\{a_{ij},1\\leq i,j \\geq N\\}</script> represents the probability of moving from state  <script type=\"math/tex\">s_{t-1}=i</script> to  <script type=\"math/tex\">s_t =j</script> such that:  <script type=\"math/tex\">a_{ij} = P(s_{t} =j \\mid s_{t-1}=i)</script>, with  <script type=\"math/tex\">a_{ij} \\leq 0</script> and where  <script type=\"math/tex\">s_t</script> denotes the state occupied by the system at time  <script type=\"math/tex\">t</script>. The matrix  <script type=\"math/tex\">\\mathbf{A}</script> is  <script type=\"math/tex\">N x N</script>.</li>\n",
    "  <li>The emission matrix  <script type=\"math/tex\">\\mathbf{B} =\\{b_j(k)\\}</script> representing the probability of emission of symbol  <script type=\"math/tex\">k</script>  <script type=\"math/tex\">\\epsilon</script>  <script type=\"math/tex\">Y</script> when system state is  <script type=\"math/tex\">s_t=j</script> such that:  <script type=\"math/tex\">b_j(k) = p(y_t = k  \\mid  s_t=j)</script> The matrix  <script type=\"math/tex\">\\mathbf{B}</script> is an  <script type=\"math/tex\">N x M</script>. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission.</li>\n",
    "  <li>And the initial state probability distribution  <script type=\"math/tex\">\\mathbf{\\pi}  = \\{\\pi_i \\}</script> indicating the probability of each state of the hidden variable  at  <script type=\"math/tex\">t = 1</script> such that,  <script type=\"math/tex\">\\pi _i = P(q_1 = s_i), 1 \\leq i \\geq N</script>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The complete HMM specification requires;</p>\n",
    "\n",
    "<ol>\n",
    "  <li>Finite set of hidden states <script type=\"math/tex\">N</script> and observation symbols <script type=\"math/tex\">M</script>\n",
    "</li>\n",
    "  <li>Length of observation seqences <script type=\"math/tex\">T</script> and</li>\n",
    "  <li>Specification of three probability measures <script type=\"math/tex\">\\mathbf{A}, \\mathbf{B}</script> and <script type=\"math/tex\">\\mathbf{\\pi}</script>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>The set of all HMM model parameters is represented by <script type=\"math/tex\">\\mathbf{\\lambda} =(\\pi, A, B)</script>.</p>\n",
    "\n",
    "<p>Since <script type=\"math/tex\">S</script> is not observed, the likelihood function <script type=\"math/tex\">Y</script> is given by the joint distribution of <script type=\"math/tex\">Y</script> and <script type=\"math/tex\">S</script> over all possible state.</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(Y \\mid \\lambda) = \\sum P(Y, S \\mid  \\lambda)</script>\n",
    "\n",
    "<p>where</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(Y,S \\mid \\lambda) = P(Y \\mid S,\\lambda)P(S \\mid \\lambda)</script>\n",
    "\n",
    "<p>Note that <script type=\"math/tex\">y_t</script> is independent and identically distributed given state sequence <script type=\"math/tex\">S = \\{s_1, s_2....,s_N\\}</script>. Also each state at time <script type=\"math/tex\">t</script> depend on the state at its previous time <script type=\"math/tex\">t-1</script>. Then</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(Y \\mid S, \\lambda) = \\prod_{t=1}^T P(y_t \\mid s_t)</script>\n",
    "\n",
    "<p>Similary</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(S \\mid \\lambda) = \\pi _{s_1} \\prod _{t=2}^T a_{ij}</script>\n",
    "\n",
    "<p>The joint probability is therefore:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(Y \\mid \\lambda) = \\pi _{s_1}P(y_1 \\mid s_1) \\sum \\prod_{t=2}^T a_{ij} P(y_t \\mid s_t)</script>\n",
    "\n",
    "<h2 id=\"three-main-problems-in-hmms\">Three main problems in HMMs</h2>\n",
    "\n",
    "<p>When applying HMM to a real world problem, three important problem must be solved.</p>\n",
    "\n",
    "<ol>\n",
    "  <li>Evaluation Problem: Given HMM parameters <script type=\"math/tex\">\\lambda</script> and the observation seqence <script type=\"math/tex\">Y = \\{Y_1, Y_2....,Y_M\\}</script>, find <script type=\"math/tex\">P(Y \\mid \\lambda)</script> the likelihood of the observation sequence <script type=\"math/tex\">Y</script> given the model <script type=\"math/tex\">\\lambda</script>. This problem give a score on how well a given model matches a given observation and thus allows you to choose the model that best match the observation.</li>\n",
    "  <li>Decoding Problem: Given HMM parameters <script type=\"math/tex\">\\lambda</script> and the observation seqence <script type=\"math/tex\">Y = \\{Y_1, Y_2....,Y_M\\}</script>, find an optimal state sequense <script type=\"math/tex\">S = \\{S_1, S_2....,S_N\\}</script> which best explain the observation.This problem attempt to cover the hidden part of the model.</li>\n",
    "  <li>Learning Problem: Given the obseravtion seqence <script type=\"math/tex\">Y = \\{Y_1, Y_2....,Y_M\\}</script>, find the model parameters <script type=\"math/tex\">\\lambda</script> that maximize <script type=\"math/tex\">P(Y \\mid \\lambda)</script>.This problem attempt to optimize the model parameters so as to describe the model.</li>\n",
    "</ol>\n",
    "\n",
    "<p>The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post.</p>\n",
    "\n",
    "<h2 id=\"solution-to-problem-1\">Solution to Problem 1</h2>\n",
    "\n",
    "<p>A straight forward way to solve this problem is to find <script type=\"math/tex\">P(Y \\mid S, \\lambda)</script> for fixed state sequences <script type=\"math/tex\">S = \\{s_1,...s_T \\}</script> and then sum up over all possible states. This is generally infeasible since it requires about <script type=\"math/tex\">2TN^T</script> multiplications. However this problem can be efficiently solved by using the forward algorithm  as follows:</p>\n",
    "\n",
    "<h3 id=\"the-forward-backward-algorithm\">The forward-backward Algorithm</h3>\n",
    "\n",
    "<p>Let us define the <strong>forward variable</strong></p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">\\alpha _t(i)=P(y_1,\\ldots y_t, s_t=i \\mid \\lambda)</script>\n",
    "\n",
    "<p>the probability of the partial observation sequences <script type=\"math/tex\">y_1 \\ldots y_t</script>  up to time <script type=\"math/tex\">t</script> and the state <script type=\"math/tex\">s_t =i</script> at time <script type=\"math/tex\">t</script> given the model <script type=\"math/tex\">{\\lambda}</script>. We also define an emission probability given HMM state <script type=\"math/tex\">i</script> at time <script type=\"math/tex\">t</script> as <script type=\"math/tex\">b_i(y_t)</script>.</p>\n",
    "\n",
    "<h4 id=\"forward-algorithm\">Forward-Algorithm</h4>\n",
    "\n",
    "<p><strong>Initilization</strong></p>\n",
    "\n",
    "<p>Let</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\alpha _1(i)&=P(y_1, s_1=i \\mid \\lambda) \\\\\n",
    "    & = P(y_1 \\mid s_1=i,\\lambda)P(s_1=i \\mid \\lambda)\\\\\n",
    "    &= \\pi _i b_i(y_1) \\text{  for  } 1\\leq i \\geq N\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>Induction</strong></p>\n",
    "\n",
    "<p>For <script type=\"math/tex\">t=2,3...T</script> and <script type=\"math/tex\">1\\leq i \\geq N</script>, compute:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\alpha _{t}(i) & = P(y_1 \\ldots y_t, s_t=i \\mid \\lambda)\\\\\n",
    " &= \\displaystyle \\sum_{j=1}^{N} P(y_1 \\ldots y_{t}, s_{t-1}=j,s_t=i \\mid \\lambda) \\\\\n",
    " &= \\displaystyle \\sum_{j=1}^{N} P(y_t \\mid s_t=i, y_1,\\ldots y_{t-1}, s_{t-1}=j, \\lambda) \\\\\n",
    "   &  \\times P(s_t=i \\mid y_1 \\ldots y_{t-1} \\ldots , s_{t-1}=j, \\lambda) \\\\\n",
    "   & \\times P(y_1 \\ldots y_{t-1}, s_{t-1}=j,\\lambda) \\\\\n",
    " & = P(y_t \\mid s_t=i,\\lambda)\\displaystyle \\sum_{j=1}^{N} P(s_t=i \\mid s_{t-1}=j)\\cdot P(y_1, \\ldots y_{t-1}, s_{t-1}) \\\\\n",
    "& = b_i(y_{t})\\displaystyle \\sum_{j=1}^{N} \\alpha _{t-1}(i)a_{ij}  \n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>Termination</strong></p>\n",
    "\n",
    "<p>From <script type=\"math/tex\">\\alpha _t(i)=P(y_1,...y_t, s_t=i \\mid \\lambda)</script>, it cear that:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned} \n",
    "P(Y \\mid \\lambda) &= \\displaystyle \\sum_{i=1}^{N} P(y_1,\\ldots y_T, s_T = i \\mid \\lambda) \\\\\n",
    "&= \\displaystyle \\sum_{i=1}^{N}\\alpha _T(i)  \n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>The forward algorithm only requires about <script type=\"math/tex\">N^2T</script> multiplications and is it can be implemented in python as follows.</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">):</span>\n",
    "        <span class=\"n\">T</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
    "        <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">))</span>\n",
    "        <span class=\"n\">alpha</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">pi</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"p\">[:,</span><span class=\"n\">obs_seq</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n",
    "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">):</span>\n",
    "            <span class=\"n\">alpha</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">alpha</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[:,</span> <span class=\"n\">obs_seq</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]]</span>\n",
    "        <span class=\"k\">return</span> <span class=\"n\">alpha</span>\n",
    "\n",
    " <span class=\"k\">def</span> <span class=\"nf\">likelihood</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">):</span>\n",
    "        <span class=\"c1\"># returns log P(Y  \\mid  model)\n",
    "</span>        <span class=\"c1\"># using the forward part of the forward-backward algorithm\n",
    "</span>        <span class=\"k\">return</span>  <span class=\"n\">forward</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">()</span>      \n",
    "</code></pre></div></div>\n",
    "\n",
    "<h3 id=\"backward-algorithm\">Backward Algorithm</h3>\n",
    "\n",
    "<p>This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the <strong>backward variable</strong> <script type=\"math/tex\">\\beta_t(i)=P(y_{t+1},y_{t+2} \\ldots y_{T} \\mid s_t=i, {\\lambda})</script>: probability of the partial observed sequence from <script type=\"math/tex\">t+1</script> to the end at <script type=\"math/tex\">T</script> given state <script type=\"math/tex\">i</script> at time <script type=\"math/tex\">t</script> and the model <script type=\"math/tex\">\\lambda</script>.</p>\n",
    "\n",
    "<p>Then <script type=\"math/tex\">\\beta_t(i)</script> can be computed recursively as follows.</p>\n",
    "\n",
    "<p><strong>Initilization</strong></p>\n",
    "\n",
    "<p>Let <script type=\"math/tex\">\\beta_{T}(i)= 1</script>, for <script type=\"math/tex\">1 \\leq i\\geq N</script></p>\n",
    "\n",
    "<p><strong>Induction</strong></p>\n",
    "\n",
    "<p>For <script type=\"math/tex\">t =T-1, T-2,\\ldots1</script> for <script type=\"math/tex\">1 \\leq i\\geq N</script> and by using the sum and product rules, we can rewrite <script type=\"math/tex\">\\beta_t(j)</script> as:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\beta_t(i)&=P(y_{t+1},\\ldots y_{T} \\mid s_t=j, {\\lambda}) \\\\\n",
    " &= \\displaystyle \\sum_{i=1}^{N} P(y_{t+1} \\ldots y_T, s_{t+1}=i \\mid s_t=j, \\lambda) \\\\\n",
    " & = \\displaystyle \\sum_{i=1}^{N} P(y_{t+1} \\ldots y_T, s_{t+1}=i, s_t=j, \\lambda)\\cdot P(s_{t+1}=i \\mid s_t=j) \\\\\n",
    " &= \\displaystyle \\sum_{i=1}^{N} P(y_{t+2} \\ldots y_T, s_{t+1}=i, \\lambda)\\cdot P(y_{t+1} \\mid s_{t + 1}=i, \\lambda)\\cdot P(s_{t+1}=i \\mid s_t=j) \\\\\n",
    " & = \\displaystyle \\sum_{i=1}^{N} a_{ij}b_i(y_{t+1})\\beta _{t+1}(i)\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>Termination</strong></p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\beta_{0} & = P(Y \\mid \\lambda) \\\\\n",
    "& = \\displaystyle \\sum_{i=1}^{N} P(y_1,\\ldots y_T, s_1=i) \\\\\n",
    "&= \\displaystyle \\sum_{i=1}^{N} P(y_1,\\ldots y_T \\mid s_1=i)\\cdot P(s_1=i) \\\\\n",
    "& = \\displaystyle \\sum_{i=1}^{N} P(y_1 \\mid s_1=i)\\cdot P(y_2,\\ldots y_T \\mid s_1=i)\\cdot P(s_1=i) \\\\\n",
    "& = \\displaystyle \\sum_{i=1}^{N} \\pi _i b_i(y_1)\\beta _1(i)\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>Python implementation of forward algorithm is as shown below;</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">):</span>\n",
    "        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
    "        <span class=\"n\">T</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "\n",
    "        <span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"n\">T</span><span class=\"p\">))</span>\n",
    "        <span class=\"n\">beta</span><span class=\"p\">[:,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n",
    "\n",
    "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">reversed</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)):</span>\n",
    "            <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n",
    "                <span class=\"n\">beta</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">beta</span><span class=\"p\">[:,</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">,:]</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[:,</span> <span class=\"n\">obs_seq</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n",
    "\n",
    "        <span class=\"k\">return</span> <span class=\"n\">beta</span>\n",
    "</code></pre></div></div>\n",
    "\n",
    "<h4 id=\"posterior-probability\">Posterior Probability</h4>\n",
    "<p>The forward variable <script type=\"math/tex\">\\alpha _t(i)</script> and backward variable <script type=\"math/tex\">\\beta _t(i)</script> are used to calculate the posterior probability of a specific case. Now for <script type=\"math/tex\">t=1...T</script> and <script type=\"math/tex\">i=1..N</script>, let define posterior probability <script type=\"math/tex\">\\gamma_t(i)=P(s_t=i \\mid Y, \\lambda)</script> the probability of being in state <script type=\"math/tex\">s_t = i</script> at time <script type=\"math/tex\">t</script> given the observation <script type=\"math/tex\">Y</script> and the model <script type=\"math/tex\">\\lambda</script>.</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\gamma_t(i) & = \\frac{P(s_t=1, Y \\mid \\lambda)}{P(Y \\mid \\lambda)} \\\\\n",
    " &=\\frac{P(y_1,\\ldots y_t, s_t=1, \\mid \\lambda)}{P(Y \\mid \\lambda)}\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>Consider:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "P(y_1,\\ldots y_t, s_t=1, \\mid \\lambda) & = P(y_1,\\ldots y_t \\mid  s_t=1,\\lambda)\\cdot P(y_{t+1},\\ldots y_T \\mid  s_t=1,\\lambda)\\cdot P(s_t =i  \\mid \\lambda) \\\\\n",
    " & = \\alpha _t(i) \\cdot \\beta _t(i)\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>Thus</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">\\gamma_t(i) = \\frac{\\alpha _t(i) \\cdot \\beta _t(i)}{P(Y \\mid \\lambda)}</script>\n",
    "\n",
    "<p>where</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">P(Y \\mid {\\lambda}) =  \\displaystyle \\sum_{i=1}^{N}\\alpha _T(i)</script>\n",
    "\n",
    "<p>In python:</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">gamma</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">):</span>\n",
    "    <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">beta</span>  <span class=\"o\">=</span> <span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">obs_prob</span> <span class=\"o\">=</span> <span class=\"n\">likelihood</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">alpha</span><span class=\"p\">,</span><span class=\"n\">beta</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">obs_prob</span><span class=\"p\">)</span>\n",
    "</code></pre></div></div>\n",
    "\n",
    "<p>We can use <script type=\"math/tex\">\\gamma_t(i)</script> to find the most likely state at time <script type=\"math/tex\">t</script> which is the state <script type=\"math/tex\">s_t=i</script> for which <script type=\"math/tex\">\\gamma_t(i)</script> is maximum. This algorithm <a href=\"http://www.shokhirev.com/nikolai/abc/alg/hmm/hmm.html\">works fine in the case when HMM is ergodic</a> i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem <em>Viterbi algorithm</em> is the most common decoding algorithms used.</p>\n",
    "\n",
    "<h3 id=\"viterbi-algorithm\">Viterbi Algorithm</h3>\n",
    "\n",
    "<p>Viterbi is a kind of dynamic programming algorithm that make uses of a dynamic programming trellis.</p>\n",
    "\n",
    "<p>The virtebi algorithm offer an efficient way of finding  the single best state sequence.Let define the highest probability along a single path, at time <script type=\"math/tex\">t</script>, which accounts for the first <script type=\"math/tex\">t</script> observations and ends in state <script type=\"math/tex\">j</script> using a new notation:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\delta_t(i) & = \\max_{s_1,\\ldots s_{t-1}} P(s_1, \\ldots s_t =1, y_1,\\ldots y_t \\mid \\lambda)\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>By induction, a recursive formula of <script type=\"math/tex\">\\delta_{t+1}(i)</script> from <script type=\"math/tex\">\\delta_t(i)</script>  is derived to calculate this probability as follows:</p>\n",
    "\n",
    "<p>Consider the joint distribution appearing in <script type=\"math/tex\">\\delta_{t+1}(i)</script>, which can be rewritten when <script type=\"math/tex\">s_{t+1}=i</script> and <script type=\"math/tex\">s_t = j</script> as:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "P(s_1,\\ldots, s_t=j,s_{t+1}=i, y_1,\\ldots y_t, y_{t+1} \\mid \\lambda) & = P(s_1 \\ldots s_t=j, y_1,\\ldots y_t  \\mid \\lambda)\\\\& \\times P(s_{t+1}=i,y_{t+1} \\mid s_1, \\ldots s_t, y_1, \\ldots y_t, \\lambda) \\\\\n",
    " & = P(s_1 \\ldots s_t=j, y_1,\\ldots y_t  \\mid \\lambda)\\cdot P(s_{t+1} \\mid s_t, \\lambda)\\\\ & \\times P(y_{t+1} \\mid s_{t+1},\\lambda) \\\\\n",
    "  & = P(s_1 \\ldots s_t=j, y_1,\\ldots y_t  \\mid \\lambda)\\cdot a_{ij}b_i(y_{t+1})\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>Thus <script type=\"math/tex\">\\delta_{t+1}(i)</script>  is computed recursively from <script type=\"math/tex\">\\delta_{t+1}(j)</script> as:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\delta_{t+1}(i) &= \\max_{s_1,\\ldots s_{t}=j} P(s_1 \\ldots s_t=j, y_1,\\ldots y_t  \\mid \\lambda)\\cdot a_{ij}b_i(y_{t+1}) \\\\\n",
    " & = \\max_{j}\\Big[ \\delta_t(j) a_{ij}\\Big]\\cdot b_i(y_{t+1})\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p>We therefore need to keep track the state that maximize the above equation so as to backtrack to the single best state sequence in the following Viterbi algorithm:</p>\n",
    "\n",
    "<p><strong>Initilization</strong></p>\n",
    "\n",
    "<p>For <script type=\"math/tex\">1 \\leq i \\geq N</script>, let:</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\n",
    "\\delta _1(i)&= \\pi _{s_i}b_i(y_1)\\\\\n",
    "\\Theta _1(i)&=0\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>Recursion</strong></p>\n",
    "\n",
    "<p>Calculate  the ML (maximum likelihood) state sequences and their probabilities. For <script type=\"math/tex\">t=2,3,...T</script> and <script type=\"math/tex\">1\\leq i \\geq N</script></p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned}\\delta_t(i) & = \\displaystyle \\max_{j\\epsilon{1,..N}} \\Big[\\delta_{t-1}(j)a_{ij}\\Big]\\cdot b_i(y_t) \\\\\n",
    "\\Theta_t(i) & = \\arg\\max_j \\Big[\\delta_{t-1}(j)a_{ij} \\Big] \n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>Termination</strong>:</p>\n",
    "\n",
    "<p>Retrieve the most likely final state</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
    "\\begin{aligned} \\hat{P} &= \\displaystyle \\max_{j\\epsilon{1,..N}}[\\delta_T(j)]  \\\\\n",
    "\\hat{S}_T & = \\arg\\max_j [\\delta_T(j)]\n",
    "\\end{aligned} %]]></script>\n",
    "\n",
    "<p><strong>State sequence backtracking</strong>:</p>\n",
    "\n",
    "<p>Retrieve  the most likely state sequences (virtebi path)</p>\n",
    "\n",
    "<script type=\"math/tex; mode=display\">\\hat{S}_t = \\Theta_{t+1}(\\hat{S}_{t+1}) \\text{, where } t=T-1,T-2,\\ldots1</script>\n",
    "\n",
    "<p>Virtebi algorithm uses the same schema as the Forward algorithm except for two differences:</p>\n",
    "\n",
    "<ol>\n",
    "  <li>It uses maximization in place of summation at the recursion and termination steps.</li>\n",
    "  <li>It keeps track of the arguments that maximize <script type=\"math/tex\">\\delta_t(i)</script> for each <script type=\"math/tex\">t</script> and <script type=\"math/tex\">i</script>, storing them in the N by T matrix <script type=\"math/tex\">\\Theta</script>. This matrix is used to retrieve the optimal state sequence at the backtracking step.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Python implementation of virtebi algorithm</p>\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">viterbi</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">):</span>\n",
    "        <span class=\"c1\"># returns the most likely state sequence given observed sequence x\n",
    "</span>        <span class=\"c1\"># using the Viterbi algorithm\n",
    "</span>        <span class=\"n\">T</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">obs_seq</span><span class=\"p\">)</span>\n",
    "        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
    "        <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">))</span>\n",
    "        <span class=\"n\">psi</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">))</span>\n",
    "        <span class=\"n\">delta</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">pi</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"p\">[:,</span><span class=\"n\">obs_seq</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n",
    "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">):</span>\n",
    "            <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n",
    "                <span class=\"n\">delta</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">delta</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"p\">[:,</span><span class=\"n\">j</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">obs_seq</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]]</span>\n",
    "                <span class=\"n\">psi</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">delta</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"p\">[:,</span><span class=\"n\">j</span><span class=\"p\">])</span>\n",
    "\n",
    "        <span class=\"c1\"># backtrack\n",
    "</span>        <span class=\"n\">states</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n",
    "        <span class=\"n\">states</span><span class=\"p\">[</span><span class=\"n\">T</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">delta</span><span class=\"p\">[</span><span class=\"n\">T</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n",
    "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n",
    "            <span class=\"n\">states</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">psi</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">states</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]]</span>\n",
    "        <span class=\"k\">return</span> <span class=\"n\">states</span>\n",
    "</code></pre></div></div>\n",
    "\n",
    "<p>To summarize, we can compute the following from HMM:</p>\n",
    "\n",
    "<ol>\n",
    "  <li>The marginalized likelihood function <script type=\"math/tex\">P(Y \\mid \\lambda)</script> from the forward or backward algorithm.</li>\n",
    "  <li>The posterior probability <script type=\"math/tex\">\\gamma_t(i) = P(s_t=i  \\mid Y, \\lambda)</script> from the forward–backward algorithm.</li>\n",
    "  <li>The optimal state sequence <script type=\"math/tex\">\\hat{S} = \\max_{s} P(S \\mid Y, \\lambda) = \\max_{s} P(S, Y \\mid  \\lambda)</script>from the Viterbi algorithm.</li>\n",
    "  <li>The segmental joint likelihood function <script type=\"math/tex\">P(\\hat{S},Y \\mid \\lambda)</script> from the Viterbi algorithm.</li>\n",
    "</ol>\n",
    "\n",
    "<p>These values are used in the decoding step and the training step of estimating model parameters <script type=\"math/tex\">\\lambda</script>.</p>\n",
    "\n",
    "<p><strong>Example 1</strong></p>\n",
    "\n",
    "<p>Conside the Bob-Alice example as described <a href=\"https://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example\">here</a>. Two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.</p>\n",
    "\n",
    "<p>Alice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his activities, those are the observations.</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">states</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s\">'Rainy'</span><span class=\"p\">,</span> <span class=\"s\">'Sunny'</span><span class=\"p\">)</span>\n",
    "<span class=\"n\">observations</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s\">'walk'</span><span class=\"p\">,</span> <span class=\"s\">'shop'</span><span class=\"p\">,</span> <span class=\"s\">'clean'</span><span class=\"p\">)</span>\n",
    "<span class=\"n\">pi</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">])</span>  <span class=\"c1\">#initial probability \n",
    "</span><span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]])</span> <span class=\"c1\">#Transmission probability \n",
    "</span><span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],[</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]])</span> <span class=\"c1\">#Emission probability\n",
    "</span></code></pre></div></div>\n",
    "\n",
    "<p>Suppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears.</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">bob_says</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">])</span>\n",
    "<span class=\"n\">alice_hears</span><span class=\"o\">=</span><span class=\"n\">viterbi</span><span class=\"p\">(</span><span class=\"n\">bob_says</span><span class=\"p\">)</span>\n",
    "<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Bob says:\"</span><span class=\"p\">,</span> <span class=\"s\">\", \"</span><span class=\"p\">,</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">observ_bob</span><span class=\"p\">[</span><span class=\"n\">y</span><span class=\"p\">],</span> <span class=\"n\">bob_says</span><span class=\"p\">)))</span>\n",
    "<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Alice hears:\"</span><span class=\"p\">,</span> <span class=\"s\">\", \"</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"n\">states_bob</span><span class=\"p\">[</span><span class=\"n\">s</span><span class=\"p\">],</span> <span class=\"n\">alice_hears</span><span class=\"p\">)))</span>\n",
    "</code></pre></div></div>\n",
    "\n",
    "<blockquote>\n",
    "    ('Bob says:', 'walk, clean, shop, shop, clean, walk')\n",
    "    ('Alice hears:', 'Sunny, Rainy, Rainy, Rainy, Rainy, Sunny')\n",
    "</blockquote>\n",
    "\n",
    "<p>The notebook with codes for the above example can be found in <a href=\"https://github.com/sambaiga/HMM/blob/master/HMM%20Basics.ipynb\">here</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
