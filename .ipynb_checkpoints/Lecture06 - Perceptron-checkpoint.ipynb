{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Layer Perceptron\n",
    "Perhaps the simplest neural network we can define for binary classification is the single-layer perceptron. Given an input, the output neuron fires (produces an output of 1) only if the data point belongs to the target class. Otherwise, it does not fire (it produces an output of -1). The network looks something like this:\n",
    "\n",
    "<img src=\"images/sl_perceptron.png\" >\n",
    "\n",
    "Instead of using a linear activation function like in linear regression, we instead use a sign function. Recall the definition of the sign function:\n",
    "\n",
    "$$\n",
    "\\mbox{sign}(\\mathbf{w}^T\\mathbf{x}_i) = \n",
    "\\begin{cases}\n",
    "1 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i > 0 \\\\\n",
    "0 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i = 0 \\\\\n",
    "-1 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this, we are computing the dot product of an example with our weight vector. Points with positive projections will be given a label of 1 and points with negative projections will be given a label of -1. Consequently, our decision boundary will be perpendicular to our weight vector. Why? Consider a 2-dimensional decision problem. The decision boundary is the line where it is equally probable that a point on that line belongs to either class, i.e. $ h(\\mathbf{x}_i, \\mathbf{w}) = \\mbox{sign}(\\mathbf{w}^T\\mathbf{x}_i) = 0 $, or $ \\mathbf{w}^T\\mathbf{x}_i = 0 $. Then we have:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\mathbf{w}^T\\mathbf{x}_i =& 0\\\\\n",
    "w_1+w_2x_i^{(2)} + w_3x_i^{(3)} =& 0\\\\\n",
    "x_i^{(3)} =& -\\frac{w_2}{w_3}x_i^{(2)} -\\frac{w_1}{w_3}\n",
    "\\end{align} $$\n",
    "\n",
    "In this case, $ x_i^{(1)} $ is our bias value and is always equal to 1, $ x_i^{(2)}  $ is \"$ x $\" in the cartesian plane and $ x_i^{(3)} $ is \"$ y $.\" The slope of our weight vector in the cartesian plane is $ \\frac{w_3}{w_2} $ (they \"$ y $\" component of $ \\mathbf{w} $ is $ w_3 $, and the \"$ x $\" component is $ w_2 $), while the slope of the decision boundary is $ -\\frac{w_2}{w_3} $ (thus making them perpendicular). Graphically, this looks something like this:\n",
    "\n",
    "\n",
    "<img src=\"images/linear_db.png\">\n",
    "\n",
    "\n",
    "The problem we now face is that the step function is not continuously differentiable, and we cannot use standard gradient descent to learn the weights. Therefore, we will use the appropriately-named perceptron algorithm . This algorithm is an online method used to successively update the weights defining a linear boundary only if that boundary does not classify a training point correctly.  The algorithm is as follows: \n",
    "Initialize the weight vector $ \\mathbf{w} $ to all zeros.\n",
    "Repeat the following:\n",
    "      \n",
    "For each training example $ \\mathbf{x}_i $: \n",
    "        \t\n",
    " 1. If $ h(\\mathbf{x}_i, \\mathbf{w}) \\neq y_i $, then update the weights with $ \\mathbf{w}' = \\mathbf{w}+\\eta y_i\\mathbf{x}_i $ Here, $ \\eta $ is the step size.\n",
    "\n",
    "\n",
    " 2. If the stopping condition $ \\frac{1}{N} \\sum_{j=0}^M |w_j' - w_j| < \\delta $ is reached, then accept $  \\mathbf{w} $ as the final weight vector ($ M $ in this case is the number of features in the dataset).\n",
    "\n",
    "\n",
    "If our problem is linearly separable, the perceptron algorithm is guaranteed to converge. Therefore, at the algorithm's termination, we will end up with a linear decision boundary defined by $ \\mathbf{w} $. However, this decision boundary is not guaranteed to be a maximum margin hyperplane as in the case of SVMs.\n",
    " \n",
    "Finally, if we want to predict the label $ \\hat{y}_i $ of a test point $ \\mathbf{x}_i $, we use $ \\hat{y}_i = \\mbox{sign}(\\mathbf{w}^T\\mathbf{x}_i) $.While it is not strictly necessary to define a neural network to use the perceptron algorithm, this is a good first step towards single-layer classification.\n",
    "\n",
    "### Classification with a sigmoid (softmax) activation function\n",
    "Instead of an all-or-nothing classifier (like the sign function), it is helpful to come up with some way to measure the probability of assignment, that is $ P( Y = y_i\\ |\\ X = \\mathbf{x}_i, \\mathbf{w} ) $. If we can calculate this likelihood, we can use as a confidence measure of our predictions.\n",
    "\n",
    "Instead of using a sign activation function, we can instead use a sigmoid (usually called softmax in the neural net literature) to output a probability: \n",
    "\n",
    "$$ P(Y = y\\ |\\ X = \\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{1+\\exp\\left(-y\\mathbf{w}^T\\mathbf{x}_i\\right)}$$\n",
    "\n",
    "But how do we assign a class label when given only a probability? We can simply \"clamp\" the probability using a sign function, so that any $ P(Y = 1\\ |\\ \\mathbf{x}_i, \\mathbf{w}) \\geq 0.5$ is assigned a class label of 1, and any probability less than 0.5 is given a class label of -1. Our simple network now looks something like the following: \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/linear_sigmoid.png\">\n",
    "\n",
    "\n",
    "\n",
    "Luckily for us, this network function is identical to the likelihood used by logistic regression. Because the sigmoid is differentiable, we can use standard gradient acent to train the weights instead of the perceptron algorithm. For a derivation of the gradient for logistic regression, see the Appendix.ImplementationTo implement this theory, we'll be learning a set of weights that classify two groups of 2D data using both the perceptron algorithm and gradient descent. Let's start out by defining our 2D data (you can find this code in ann_linear_2D_classification_perceptron.py): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([[0, 0],[0, 1],[1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "lr = 1e-2\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x[0,0], x[0, 1], s=120, c='blue', marker='o', linewidths=2)\n",
    "plt.scatter(x[1,0], x[1, 1], s=120, c='blue', marker='o', linewidths=2)\n",
    "plt.scatter(x[2,0], x[2, 1], s=120, c='blue', marker='o', linewidths=2)\n",
    "plt.scatter(x[3,0], x[3, 1], s=120, c='red', marker='*', linewidths=2)\n",
    "plt.plot([0.6, 0],[0, 0.4)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.7585\n",
      "Epoch [200/1000], Loss: 0.5920\n",
      "Epoch [300/1000], Loss: 0.5026\n",
      "Epoch [400/1000], Loss: 0.4488\n",
      "Epoch [500/1000], Loss: 0.4133\n",
      "Epoch [600/1000], Loss: 0.3883\n",
      "Epoch [700/1000], Loss: 0.3699\n",
      "Epoch [800/1000], Loss: 0.3557\n",
      "Epoch [900/1000], Loss: 0.3445\n",
      "Epoch [1000/1000], Loss: 0.3355\n",
      "Final Weights: tensor([[0.9656, 0.4566]])\n",
      "Final Bias: tensor([1.8336])\n"
     ]
    }
   ],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.nueron = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.nueron(x)\n",
    "        x = F.logsigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Perceptron()\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "inputs = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(max_iter):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, max_iter, loss.item()))\n",
    "print ('Final Weights: {}'.format(model.nueron.weight.data))\n",
    "print ('Final Bias: {}'.format(model.nueron.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  2.  1.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Implements a perceptron network\"\"\"\n",
    "    def __init__(self, input_size, lr=1, epochs=100):\n",
    "        self.W = np.zeros(input_size+1)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "    \n",
    "    def activation_fn(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    " \n",
    "    def predict(self, x):\n",
    "        z = self.W.T.dot(x)\n",
    "        a = self.activation_fn(z)\n",
    "        return a\n",
    " \n",
    "    def fit(self, X, d):\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(d.shape[0]):\n",
    "                x = np.insert(X[i], 0, 1)\n",
    "                y = self.predict(x)\n",
    "                e = d[i] - y\n",
    "                self.W = self.W + self.lr * e * x\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    X = np.array([\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ])\n",
    "    d = np.array([0, 0, 0, 1])\n",
    " \n",
    "    perceptron = Perceptron(input_size=2)\n",
    "    perceptron.fit(X, d)\n",
    "    print(perceptron.W)\n",
    "    print(perceptron.predict(np.insert(X[3], 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "Multiclass classification using a linear neural network is a fairly simple extension of the binary classification setup. You may think that instead of outputting 0/1 from our second layer node, we could output 0, 1, ..., $K-1$. However, this is not the case . Our labels are not necessarily linear, and halfway between $\\hat{y}=0$ and $\\hat{y}=2$ is not necessarily $\\hat{y}=1$. They are in fact categorical, and we use $k \\in \\{0, 1, \\ldots, K\\}$ out of computational convenience.\n",
    "\n",
    "Consider instead representing a label using a binary vector of length $K$. Having a 1 in position $k$ corresponds to a label of $k$. Then, we can extend our linear network (with a sigmoid activation at the output) to learn how to output this vector. It would look something like the following:\n",
    "\n",
    "<img src=\"images/linear_multiclass.png\">\n",
    "\n",
    "\n",
    "Note that instead of $|\\mathbf{w}| = M$ (where $M$ is the number of features), we instead have $|\\mathbf{w}| = MK$. So in this figure, we have $K=3$ classes, and 3 features, giving us 9 weights in total. In fact, this is the neural network view of multinomial logistic regression. Recall the previous likelihood used in binary logistic regression: $$ P(Y = y\\ |\\ X = \\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{1+\\exp\\left(-y\\mathbf{w}^T\\mathbf{x}_i\\right)}$$ To extend this to $K$ classes, we use the following likelihood: $$ P(Y = k\\ |\\ X = \\mathbf{x}_i, \\mathbf{w}) = \\frac{\\exp(\\mathbf{w}_k^T\\mathbf{x}_i)}{\\sum_{k'}^K\\exp\\left(\\mathbf{w}_{k'}^T\\mathbf{x}_i\\right)}$$ The key difference is that there are now $K$ sets of $M$ weights, one for each label. These are specified when determining the likelihood for a particular $k$. By using this formalism, we ensure that the values produced by the output nodes forms a valid probability distribution, as we are normalizing the likelihood by summing over all values of $k$. \n",
    "\n",
    "Our training routine is exactly the same as in Part 2, except that the gradient of the multinomial logistic regression objective is slightly different: $$ \\nabla_{\\mathbf{w}_k}\\ell(\\mathbf{w}) = \\frac{1}{N} \\sum_i^N\\left(\\mathbf{x_i}(1-P(Y = y_i\\ |\\ X = \\mathbf{x}_i, \\mathbf{w})\\right) $$ See the Appendix for a full derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer1 = Linear(in_features, hidden_features)\n",
    "        self.layer2 = Linear(hidden_features, out_features)\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = t.sigmoid(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we haven't utilized any of the expressive non-linear power of neural networks - all of our simple one layer models corresponded to a linear model such as multinomial logistic regression. These one-layer models had a simple derivative. We only had one set of weights the fed directly to our output, and it was easy to compute the derivative with respect to these weights. However, what happens when we want to use a deeper model? What happens when we start stacking layers? \n",
    "\n",
    "<img src=\"images/back_prop.png\">\n",
    "\n",
    "No longer is there a linear relation in between a change in the weights and a change of the target. Any perturbation at a particular layer will be further transformed in successive layers. So, then, how do we compute the gradient for all weights in our network? This is  where we use the backpropagation algorithm.\n",
    "\n",
    "Backpropagation, at its core, simply consists of repeatedly applying the chain rule through all of the possible paths in our network. However, there are an exponential number of directed paths from the input to the output. Backpropagation's real power arises in the form of a dynamic programming algorithm, where we reuse intermediate results to calculate the gradient. We transmit intermediate errors backwards through a network, thus leading to the name backpropagation. In fact, backpropagation is closely related to forward propagation, but instead of propagating the inputs forward through the network, we propagate the error backwards.\n",
    "\n",
    "Most explanations of backpropagation start directly with a general theoretical derivation, but I’ve found that computing the gradients by hand naturally leads to the backpropagation algorithm itself, and that’s what I’ll be doing in this blog post. This is a lengthy section, but I feel that this is the best way to learn how backpropagation works.\n",
    "\n",
    "I’ll start with a simple one-path network, and then move on to a network with multiple units per layer. Finally, I’ll derive the general backpropagation algorithm. Code for the backpropagation algorithm will be included in my next installment, where I derive the matrix form of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the base rules of backpropagation\n",
    "Remember that our ultimate goal in training a neural network is to find the gradient of each weight with respect to the output: $$\\begin{align} \\frac{\\partial E}{\\partial w_{i\\rightarrow j}} \\end{align}$$ We do this so that we can update the weights incrementally using stochastic gradient descent: $$\\begin{align*} w_{i\\rightarrow j} =& w_{i\\rightarrow j} -\\eta \\frac{\\partial E}{\\partial w_{i\\rightarrow j}} \\end{align*}$$\n",
    "For a single unit in a general network, we can have several cases: the unit may have only one input and one output (case 1), the unit may have multiple inputs (case 2), or the unit may have multiple outputs (case 3). Technically there is a fourth case: a unit may have multiple inputs and outputs. But as we will see, the multiple input case and the multiple output case are independent, and we can simply combine the rules we learn for case 2 and case 3 for this case.\n",
    "\n",
    "I will go over each of this cases in turn with relatively simple multilayer networks, and along the way will derive some general rules for backpropagation.  At the end, we can combine all of these rules into a single grand unified backpropagation algorithm for arbitrary networks.\n",
    "\n",
    "##### Case 1: Single input and single output\n",
    "Suppose we have the following network:\n",
    "\n",
    "<img src=\"images/single_out.png\">\n",
    "\n",
    "\n",
    "We can explicitly write out the values of each of variable in this network: \n",
    "\n",
    "$$ \\begin{align} s_j =&\\ w_1\\cdot x_i\\\\ z_j =&\\ \\sigma(in_j) = \\sigma(w_1\\cdot x_i)\\\\ s_k =&\\ w_2\\cdot z_j\\\\ z_k =&\\ \\sigma(in_k) = \\sigma(w_2\\cdot\\sigma(w_1\\cdot x_i))\\\\ s_o =&\\ w_3\\cdot z_k\\\\ \\hat{y}_i =&\\ in_o = w_3\\cdot\\sigma(w_2\\cdot\\sigma(w_1\\cdot x_i))\\\\ E =&\\ \\frac{1}{2}(\\hat{y}_i - y_i)^2 = \\frac{1}{2}(w_3\\cdot\\sigma(w_2\\cdot\\sigma(w_1\\cdot x_i)) - y_i)^2 \\end{align} $$ \n",
    "\n",
    "For this simple example, it's easy to find all of the derivatives by hand. In fact, let's do that now. I am going to color code certain parts of the derivation, and see if you can deduce a pattern that we might exploit in an iterative algorithm. First, let's find the derivative for $w_{k\\rightarrow o}$ (remember that $\\hat{y} = w_{k\\rightarrow o}z_k$, as our output is a linear unit): \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial E}{\\partial w_{k\\rightarrow o}} =&\\ \\frac{\\partial}{\\partial w_{k\\rightarrow o}} \\frac{1}{2}(\\hat{y}_i - y_i)^2\\\\ =&\\ \\frac{\\partial}{\\partial w_{k\\rightarrow o}} \\frac{1}{2}(w_{k\\rightarrow o}\\cdot z_k - y_i)^2\\\\ =&\\ (w_{k\\rightarrow o}\\cdot z_k - y_i)\\frac{\\partial}{\\partial w_{k\\rightarrow o}}(w_{k\\rightarrow o}\\cdot z_k - y_i)\\\\ =&\\ \\color{blue}{(\\hat{y_i} - y_i)}(z_k) \\end{align} $$ \n",
    "\n",
    "Finding the weight update for $w_{i\\rightarrow k}$ is also relatively simple: \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial E}{\\partial w_{j\\rightarrow k}} =&\\ \\frac{\\partial}{\\partial w_{j\\rightarrow k}} \\frac{1}{2}(\\hat{y}_i - y_i)^2\\\\ =&\\ (\\hat{y}_i-y_i)\\left( \\frac{\\partial}{\\partial w_{j\\rightarrow k}} (w_{k\\rightarrow o}\\cdot\\sigma(w_{j\\rightarrow k}\\cdot z_j) - y_i) \\right)\\\\ =&\\ (\\hat{y}_i-y_i)(w_{k\\rightarrow o})\\left( \\frac{\\partial}{\\partial w_{j\\rightarrow k}} \\sigma(w_{j\\rightarrow k}\\cdot z_j) \\right)\\\\ =&\\ (\\hat{y}_i-y_i)(w_{k\\rightarrow o})\\left( \\sigma(s_k)(1-\\sigma(s_k)) \\frac{\\partial }{\\partial w_{j\\rightarrow k}}(w_{j\\rightarrow k}\\cdot z_j) \\right)\\\\ =&\\ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})\\left( \\sigma(s_k)(1-\\sigma(s_k)\\right)}(z_j) \\end{align} $$ \n",
    "\n",
    "Again, finding the weight update for $w_{i\\rightarrow j}$ consists of some straightforward calculus: \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial E}{\\partial w_{i\\rightarrow j}} =&\\ \\frac{\\partial}{\\partial w_{i\\rightarrow j}} \\frac{1}{2}(\\hat{y}_i-y_i)^2\\\\ =&\\ (\\hat{y}_i-y_i)\\left( \\frac{\\partial}{\\partial w_{i\\rightarrow j}} (\\hat{y}_i-y_i) \\right)\\\\ =&\\ (\\hat{y}_i-y_i)(w_{k\\rightarrow o})\\left( \\frac{\\partial}{\\partial w_{i\\rightarrow j}}\\cdot\\sigma(w_{j\\rightarrow k}\\cdot\\sigma(w_{i\\rightarrow j}\\cdot x_i))\\right)\\\\ =&\\ (\\hat{y}_i-y_i)(w_{k\\rightarrow o})(\\sigma(s_k)(1-\\sigma(s_k)))(w_{j\\rightarrow k})\\left( \\frac{\\partial}{\\partial w_{i\\rightarrow j}}\\sigma(w_{i\\rightarrow j}\\cdot x_i) \\right)\\\\ =&\\ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})(\\sigma(s_k)(1-\\sigma(s_k)))}\\color{OliveGreen}{(w_{j\\rightarrow k})(\\sigma(s_j)(1-\\sigma(s_j)))}(x_i) \\end{align} $$\n",
    "\n",
    "\n",
    "By now, you should be seeing a pattern emerging, a pattern that hopefully we could encode with backpropagation. We are reusing multiple values as we compute the updates for weights that appear earlier and earlier in the network. Specifically, we see the derivative of the network error, the weighted derivative of unit $k$'s output with respect to $s_k$, and the weighted derivative of unit $j$'s output with respect to $s_j$. \n",
    "So, in summary, for this simple network, we have: \n",
    "\n",
    "$$ \\begin{align} \\Delta w_{i\\rightarrow j} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})(\\sigma(s_k)(1-\\sigma(s_k)))}\\color{OliveGreen}{(w_{j\\rightarrow k})(\\sigma(s_j)(1-\\sigma(s_j)))}(x_i) \\right]\\\\ \\Delta w_{j\\rightarrow k} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})\\left( \\sigma(s_k)(1-\\sigma(s_k)\\right)}(z_j)\\right]\\\\ \\Delta w_{k\\rightarrow o} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y_i} - y_i)}(z_k)\\right] \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case 2: Handling multiple inputs\n",
    "\n",
    "Consider the more complicated network, where a unit may have more than one input:\n",
    "\n",
    "<img src=\"images/multi_out.png\">\n",
    "\n",
    "What happens to a weight when it leads to a unit that has multiple inputs? Is $w_{i\\rightarrow k}$'s update rule affected by $w_{j\\rightarrow k}$'s update rule? To see, let's derive the update for $w_{i\\rightarrow k}$ by hand: \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial E}{w_{i\\rightarrow k}} =& \\frac{\\partial}{w_{i\\rightarrow k}}\\frac{1}{2}(\\hat{y}_i - y_i)^2\\\\ =&\\ (\\hat{y}_i - y_i)\\left( \\frac{\\partial}{w_{i\\rightarrow k}}z_k w_{k\\rightarrow o} \\right)\\\\ =&\\ (\\hat{y}_i - y_i)(w_{k\\rightarrow o})\\left( \\frac{\\partial}{w_{i\\rightarrow k}}\\sigma\\left( s_k \\right) \\right)\\\\ =&\\ (\\hat{y}_i - y_i)(\\sigma(s_k)(1-\\sigma(s_k)) w_{k\\rightarrow o})\\left( \\frac{\\partial}{w_{i\\rightarrow k}}\\left( z_iw_{i\\rightarrow k} + z_jw_{j\\rightarrow k} \\right) \\right)\\\\ =&\\ (\\hat{y}_i - y_i)(\\sigma(s_k)(1-\\sigma(s_k)) w_{k\\rightarrow o})z_i \\end{align} $$ \n",
    "\n",
    "\n",
    "Here we see that the update for $w_{i\\rightarrow k}$ does not depend on $w_{j\\rightarrow k}$'s derivative, leading to our first rule: The derivative for a weight is not dependent on the derivatives of any of the other weights in the same layer. Thus we can update weights in the same layer in isolation. There is a natural ordering of the updates - they only depend on the values of other weights in the same layer, and (as we shall see), the derivatives of weights further in the network. This ordering is good news for the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case 3: Handling multiple outputs\n",
    "\n",
    "Now let's examine the case where a hidden unit has more than one output.\n",
    "\n",
    "<img src=\"images/multi_out2.png\">\n",
    "\n",
    "\n",
    "Based on the previous sections, the only \"new\" type of weight update is the derivative of $w_{in\\rightarrow j}$. The difference in the multiple output case is that unit $i$ has more than one immediate successor, so (spoiler!) we must sum the error accumulated along all paths that are rooted at unit $i$. Let's explicitly derive the weight update for $w_{in\\rightarrow i}$ (to keep track of what's going on, we define $\\sigma_i(\\cdot)$ as the activation function for unit $i$): \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial E}{w_{in\\rightarrow i}} =& \\frac{\\partial}{w_{in\\rightarrow i}}\\frac{1}{2}(\\hat{y}_i - y_i)^2\\\\ =&\\ (\\hat{y}_i - y_i)\\left( \\frac{\\partial}{w_{in\\rightarrow i}}(z_j w_{j\\rightarrow o} + z_k w_{k\\rightarrow o}) \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( \\frac{\\partial}{w_{in\\rightarrow i}}(\\sigma_j(s_j) w_{j\\rightarrow o} + \\sigma_k(s_k)w_{k\\rightarrow o}) \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( w_{j\\rightarrow o}\\sigma_j'(s_j) \\frac{\\partial}{w_{in\\rightarrow i}}s_j + w_{k\\rightarrow o}\\sigma_k'(s_k) \\frac{\\partial}{w_{in\\rightarrow i}}s_k \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( w_{j\\rightarrow o}\\sigma_j'(s_j) \\frac{\\partial}{w_{in\\rightarrow i}}z_iw_{i\\rightarrow j} + w_{k\\rightarrow o}\\sigma_k'(s_k) \\frac{\\partial}{w_{in\\rightarrow i}}z_iw_{i\\rightarrow k} \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( w_{j\\rightarrow o}\\sigma_j'(s_j) \\frac{\\partial}{w_{in\\rightarrow i}}\\sigma_i(s_i)w_{i\\rightarrow j} + w_{k\\rightarrow o}\\sigma_k'(s_k) \\frac{\\partial}{w_{in\\rightarrow i}}\\sigma_i(s_i)w_{i\\rightarrow k} \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( w_{j\\rightarrow o}\\sigma_j'(s_j) w_{i\\rightarrow j}\\sigma'_i(s_i)\\frac{\\partial}{w_{in\\rightarrow i}}s_i + w_{k\\rightarrow o}\\sigma_k'(s_k) w_{i\\rightarrow k}\\sigma'_i(s_i) \\frac{\\partial}{w_{in\\rightarrow i}}s_i \\right)\\\\ =&\\ (\\hat{y}_i - y_i)\\left( w_{j\\rightarrow o}\\sigma_j'(s_j) w_{i\\rightarrow j}\\sigma'_i(s_i) + w_{k\\rightarrow o}\\sigma_k'(s_k) w_{i\\rightarrow k}\\sigma'_i(s_i) \\right)x_i \\end{align} $$ \n",
    "\n",
    "\n",
    "There are two things to note here. The first, and most relevant, is our second derived rule: the weight update for a weight leading to a unit with multiple outputs is dependent on derivatives that reside on both paths. \n",
    "\n",
    "But more generally, and more importantly, we begin to see the relation between backpropagation and forward propagation. During backpropagation, we compute the error of the output. We then pass the error backward and weight it along each edge. When we come to a unit, we multiply the weighted backpropagated error by the unit's derivative. We then continue backpropagating this error in the same fashion, all the way to the input. Backpropagation, much like forward propagation, is a recursive algorithm. In the next section, I introduce the notion of an error signal, which allows us to rewrite our weight updates in a compact form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Signals\n",
    "Deriving all of the weight updates by hand is intractable, especially if we have hundreds of units and many layers. But we saw a pattern emerge in the last few sections - the error is propagated backwards through the network. In this section, we define the error signal, which is simply the accumulated error at each unit. For now, let's just consider the contribution of a single training instance (so we use $\\hat{y}$ instead of $\\hat{y}_i$). \n",
    "\n",
    "We define the recursive error signal at unit $j$ as: \n",
    "\n",
    "$$ \\begin{align} \\delta_j =&\\ \\frac{\\partial E}{\\partial s_j} \\end{align} $$ \n",
    "\n",
    "In layman's terms, it is a measure of how much the network error varies with the input to unit $j$. Using the error signal has some nice properties - namely, we can rewrite backpropagation in a more compact form. To see this, let's expand $\\delta_j$: \n",
    "\n",
    "$$ \\begin{align} \\delta_j =&\\ \\frac{\\partial E}{\\partial s_j}\\\\ =&\\ \\frac{\\partial}{\\partial s_j}\\frac{1}{2}(\\hat{y} - y)^2\\\\ =&\\ (\\hat{y} - y)\\frac{\\partial \\hat{y}}{\\partial s_j} \\end{align} $$ \n",
    "\n",
    "Consider the case where unit $j$ is an output node. This means that $\\hat{y} = f_j(s_j)$ (if unit $j$'s activation function is $f_j(\\cdot)$), so $\\frac{\\partial \\hat{y}}{\\partial s_j}$ is simply $f_j'(s_j)$, giving us $\\delta_j = (\\hat{y} - y)f'_j(s_j)$. \n",
    "\n",
    "Otherwise, unit $j$ is a hidden node that leads to another layer of nodes $k\\in \\text{outs}(j)$. We can expand $\\frac{\\partial \\hat{y}}{\\partial s_j}$ further, using the chain rule: \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial \\hat{y}}{\\partial s_j} =&\\ \\frac{\\partial \\hat{y}}{\\partial z_j}\\frac{\\partial z_j}{\\partial s_j}\\\\ =&\\ \\frac{\\partial \\hat{y}}{\\partial z_j}f_j'(s_j) \\end{align} $$ \n",
    "\n",
    "Take note of the term $\\frac{\\partial \\hat{y}}{\\partial z_j}$. Multiple units depend on $z_j$, specifically, all of the units $k\\in\\text{outs}(j)$. We saw in the section on multiple outputs that a weight that leads to a unit with multiple outputs does have an effect on those output units. But for each unit $k$, we have $s_k = z_jw_{j\\rightarrow k}$, with each $s_k$ not depending on any other $s_k$. Therefore, we can use the chain rule again and sum over the output nodes $k\\in\\text{outs}(j)$: \n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial \\hat{y}}{\\partial s_j} =&\\ f_j'(s_j)\\sum_{k\\in\\text{outs}(j)} \\frac{\\partial \\hat{y}}{\\partial s_k}\\frac{\\partial s_k}{\\partial z_j}\\\\ =&\\ f_j'(s_j)\\sum_{k\\in\\text{outs}(j)} \\frac{\\partial \\hat{y}}{\\partial s_k}w_{j\\rightarrow k} \\end{align} $$ \n",
    "\n",
    "Plugging this equation back into the function $\\delta_j = (\\hat{y} - y) \\frac{\\partial \\hat{y}}{\\partial s_j}$, we get: \n",
    "\n",
    "$$ \\begin{align} \\delta_j =& (\\hat{y} - y)f_j'(s_j)\\sum_{k\\in\\text{outs}(j)} \\frac{\\partial \\hat{y}}{\\partial s_k}w_{j\\rightarrow k} \\end{align} $$ \n",
    "\n",
    "Based on our definition of the error signal, we know that $\\delta_k = (\\hat{y} - y) \\frac{\\partial \\hat{y}}{\\partial s_k}$, so if we push $(\\hat{y} - y)$ into the summation, we get the following recursive relation: $$ \\begin{align} \\delta_j =& f_j'(s_j)\\sum_{k\\in\\text{outs}(j)} \\delta_k w_{j\\rightarrow k} \\end{align} $$ We now have a compact representation of the backpropagated error. The last thing to do is tie everything together with a general algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of backpropagation\n",
    "Recall the simple network from the first section:\n",
    "\n",
    "<img src=\"images/single_out.png\">\n",
    "\n",
    "We can use the definition of $\\delta_i$ to derive the values of all the error signals in the network: $$ \\begin{align} \\delta_o =&\\ (\\hat{y} - y) \\text{ (The derivative of a linear function is 1)}\\\\ \\delta_k =&\\ \\delta_o w_{k\\rightarrow o}\\sigma(s_k)(1 - \\sigma(s_k))\\\\ \\delta_j =&\\ \\delta_k w_{j\\rightarrow k}\\sigma(s_j)(1 - \\sigma(s_j)) \\end{align} $$ Also remember that the explicit weight updates for this network were of the form: $$ \\begin{align} \\Delta w_{i\\rightarrow j} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})(\\sigma(s_k)(1-\\sigma(s_k)))}\\color{OliveGreen}{(w_{j\\rightarrow k})(\\sigma(s_j)(1-\\sigma(s_j)))}(x_i) \\right]\\\\ \\Delta w_{j\\rightarrow k} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y}_i-y_i)}\\color{red}{(w_{k\\rightarrow o})\\left( \\sigma(s_k)(1-\\sigma(s_k)\\right)}(z_j)\\right]\\\\ \\Delta w_{k\\rightarrow o} =&\\ -\\eta\\left[ \\color{blue}{(\\hat{y_i} - y_i)}(z_k)\\right] \\end{align} $$ By substituting each of the error signals, we get: $$ \\begin{align} \\Delta w_{k\\rightarrow o} =&\\ -\\eta \\delta_o z_k\\\\ \\Delta w_{j\\rightarrow k} =&\\ -\\eta \\delta_kz_j\\\\ \\Delta w_{i\\rightarrow j} =&\\ -\\eta \\delta_jx_i \\end{align} $$ As another example, let's look at the more complicated network from the section on handling multiple outputs:\n",
    "\n",
    "<img src=\"images/multi_out.png\">\n",
    "\n",
    "\n",
    "We can again derive all of the error signals: $$ \\begin{align} \\delta_o =&\\ (\\hat{y} - y)\\\\ \\delta_k =&\\ \\delta_o w_{k\\rightarrow o}\\sigma(s_k)(1 - \\sigma(s_k))\\\\ \\delta_j =&\\ \\delta_o w_{j\\rightarrow o}\\sigma(s_j)(1 - \\sigma(s_j))\\\\ \\delta_i =&\\ \\sigma(s_i)(1 - \\sigma(s_i))\\sum_{k\\in\\text{outs}(i)}\\delta_k w_{i\\rightarrow k} \\end{align} $$ Although we did not derive all of these weight updates by hand, by using the error signals, the weight updates become (and you can check this by hand, if you'd like): $$ \\begin{align} \\Delta w_{k\\rightarrow o} =&\\ -\\eta \\delta_o z_k\\\\ \\Delta w_{j\\rightarrow o} =&\\ -\\eta \\delta_o z_j\\\\ \\Delta w_{i\\rightarrow k} =&\\ -\\eta \\delta_k z_i\\\\ \\Delta w_{i\\rightarrow j} =&\\ -\\eta \\delta_j z_i\\\\ \\Delta w_{in\\rightarrow i} =&\\ -\\eta \\delta_i x_i \\end{align} $$ It should be clear by now that we've derived a general form of the weight updates, which is simply $\\Delta w_{i\\rightarrow j} = -\\eta \\delta_j z_i$. \n",
    "\n",
    "The last thing to consider is the case where we use a minibatch of instances to compute the gradient. Because we treat each $y_i$ as independent, we sum over all training instances to compute the full update for a weight (we typically scale by the minibatch size $N$ so that steps are not sensitive to the magnitude of $N$). For each separate training instance $y_i$, we add a superscript $(y_i)$ to the values that change for each training example: $$ \\begin{align} \\Delta w_{i\\rightarrow j} =&\\ -\\frac{\\eta}{N} \\sum_{y_i} \\delta_j^{(y_i)}z_i^{(y_i)} \\end{align} $$ Thus, the general form of the backpropagation algorithm for updating the weights consists the following steps:\n",
    "\n",
    "1. Feed the training instances forward through the network, and record each $s_j^{(y_i)}$ and $z_{j}^{(y_i)}$.\n",
    "2. Calculate the error signal $\\delta_j^{(y_i)}$ for all units $j$ and each training example $y_{i}$. If $j$ is an output node, then $\\delta_j^{(y_i)} = f'_j(s_j^{(y_i)})(\\hat{y}_i - y_i)$. If $j$ is not an output node, then $\\delta_j^{(y_i)} = f'_j(s_j^{(y_i)})\\sum_{k\\in\\text{outs}(j)}\\delta_k^{(y_i)} w_{j\\rightarrow k}$.\n",
    "3. Update the weights with the rule $\\Delta w_{i\\rightarrow j} =-\\frac{\\eta}{N} \\sum_{y_i} \\delta_j^{(y_i)}z_i^{(y_i)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
