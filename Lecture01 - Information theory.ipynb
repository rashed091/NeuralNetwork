{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory quantifies the amount of information present. In information theory, the amount of information is characterized as:\n",
    "\n",
    "- Predictability:\n",
    "  - Guaranteed events have zero information.\n",
    "  - Likely events have little information. (Biased dice have little information.)\n",
    "  - Random events process more information. (Random dice have more information.)\n",
    "- Independent events add information. Rolling a dice twice with heads have twice the information of rolling the dice once with a head.\n",
    "\n",
    "In information theory, chaos processes more information.\n",
    "\n",
    "Information of an event is defined as:\n",
    "\n",
    "$$I(x) = - \\log(P(x))$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy \n",
    "\n",
    "In DL, we often use the cross entropy or the KL-Divergence as our cost function. Those terms occurs frequently in research papers. In this section, we will understand what are they? \n",
    "\n",
    "Entropy measures the amount of information. In data compression, it represents the minimum number of bits in representing data. By definition, entropy is defined as:\n",
    "$$\n",
    "H(y) = \\sum_{i} y_i \\log \\frac{1}{y_i} = -\\sum_{i } y_i \\log y_{i}\n",
    "$$\n",
    "Suppose we have strings only composed of \"a\", \"b\" or \"c\" with the chance of occurrence be 25%, 25% and \n",
    "50% respectively. The entropy is:\n",
    "$$\n",
    "\\begin{align}\n",
    "H & = 0.25 \\log(\\frac{1}{0.25}) + 0.25 \\log(\\frac{1}{0.25})  + 0.5 \\log(\\frac{1}{0.5}) \\\\\n",
    "H  &= 0.25 \\cdot 2 + 0.25 \\cdot 2  +  0.5 \\cdot 1 \\\\\n",
    "& = 1.5 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We will use bit 0 to represent 'c' and bits 10 for 'a' and bits 11 for 'b'. In average, we need 1.5 bits per character to represent a string.\n",
    "\n",
    "#### Cross entropy\n",
    "\n",
    "Cross entropy is defined as:\n",
    "$$\n",
    "H(y, \\hat{y}) = \\sum_i y_i \\log \\frac{1}{\\hat{y}_i} = -\\sum_i y_i \\log \\hat{y}_i\n",
    "$$\n",
    "If entropy measures the minimum of bits to encode information using the most optimized scheme. Cross entropy measures the minimum of bits to encode $$y$$ using the wrong optimized scheme from $$\\hat{y}$$. The cross entropy is always higher than entropy unless both distributions are the same: you need more bits to encode the information if you use a less optimized scheme. \n",
    "\n",
    "In our previous example, we classify a picture as either a bus, a truck or an airplane. The output probability has the format: (bus, truck, airplane). The true label probability distribution for a bus is (1, 0, 0) and our model prediction can be (0.88, 0.08, 0.04).\n",
    "\n",
    "The cross entropy of this example is:\n",
    "$$\n",
    "\\begin{align}\n",
    "H(y, \\hat{y}) &= -\\sum_i y_i \\log \\hat{y}_i \\\\\n",
    "&= - 1 \\log{0.88} - 0 \\cdot \\log{0.08} - 0 \\cdot \\log{0.04} =   - \\log{0.88} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Divergence\n",
    "\n",
    "KL Divergence is defined as:\n",
    "$$\n",
    "D_{KL}(y~||~\\hat{y}) = \\sum_i y_i \\log \\frac{y_i}{\\hat{y}_i}\n",
    "$$\n",
    "In machine learning, KL Divergence measures the difference between 2 probability distributions. \n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/kl.png\" style=\"border:none;width:80%\">\n",
    "</div>\n",
    "\n",
    "(Source Wikipedia)\n",
    "\n",
    "It becomes a very good cost function to penalize the difference between the true labels and the predictions made by the model. It is even better for stochastic processes when the true label $$y_i$$ is stochastic rather than deterministic (with probability either 0 or 1).\n",
    "\n",
    "#### Solving cross entropy = solving KL-divergence\n",
    "\n",
    "KL divergence is simply cross entropy $$H(y, \\hat{y})$$ minus entropy $$H(y) $$ (the extra bits needed to encode the data):\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(y~||~\\hat{y}) &= \\sum_i y_i \\log \\frac{y_i}{\\hat{y}_i} \\\\\n",
    "&= \\sum_i y_i \\log \\frac{1}{\\hat{y}_i} - \\sum_i y_i \\log \\frac{1}{y_i}  \\\\\n",
    "&= H(y, \\hat{y}) - H(y) \n",
    "\\end{align}\n",
    "$$\n",
    "The entropy of the true label is un-related with how we model it, i.e. $$\\frac{\\partial{H(y)}}{\\partial{w}} = 0 $$. Therefore, the optimal solution for the KL-divergence is the same as that of the cross entropy.\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(y~||~\\hat{y}) &= H(y, \\hat{y}) - H(y) \\\\\n",
    "\\\\\n",
    "\\frac{\\partial{D_{KL}(y~||~\\hat{y})}} {\\partial{w}} &= \\frac{\\partial{H(y, \\hat{y})}}{\\partial{w}} - \\frac{\\partial{H(y)}}{\\partial{w}} \\\\ \n",
    "&= \\frac{\\partial{H(y, \\hat{y})}}{\\partial{w}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "> Optimizing KL-divergence is the same as optimizing cross entropy.\n",
    "\n",
    "KL-Divergence is more intuitive in the cost function discussion. In some research, we add constraints to KL-divergence to optimize our model. Nevertheless, cross entropy requires less computation than KL-divergence and used frequently in deep learning.\n",
    "\n",
    "For a deterministic process, $$y_i$$ is always equal to 1 for the true label and 0 otherwise. Hence, the cross entropy can be further simplified as:\n",
    "$$\n",
    "\\begin{align}\n",
    "H(y, \\hat{y}) &= -\\sum_i y_i \\log \\hat{y}_i \\\\\n",
    "&= -\\sum_i \\log \\hat{y}_i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "### Choice of cost function\n",
    "\n",
    "**Deep learning is about knowing your costs. Good cost function builds good models.** San Francisco is about 400 miles from Los Angeles. It costs about $80 for the gas. When you order food from a restaurant, they do not deliver to homes more than a few miles away. From their perspective, the cost grows exponentially with distance. So in reality, we can modify our cost function to address special objectives. For example, some cost functions ignore outliers better than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "We want to build a model with $$\\hat\\theta$$ that maximizes the probability of the observed data (a model that fits the data the best: **Maximum Likelihood Estimation MLE**):\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\theta & = \\arg\\max_{\\theta} \\prod^N_{i=1} p(x_i \\vert \\theta ) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "However, multiplications overflow or underflow easily. Since $$\\log(x)$$ is monotonic, optimize $$log(f(x))$$ is the same as optimize $$f(x)$$. We add the negative sign because the log of a probability invert the direction of $$p(x)$$. So instead of the MLE, we take the $$\\log$$ and minimize the **negative log likelihood (NLL)**. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\theta & = \\arg\\min_{\\theta} - \\sum^N_{i=1} \\log p(x_i \\vert \\theta ) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "**NLL and minimizing cross entropy is equivalent**:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\theta & = \\arg\\min_{\\theta} - \\sum^N_{i=1} \\log q(x_i \\vert \\theta ) \\\\\n",
    "&  = \\arg\\min_{\\theta} - \\sum_{x \\in X} p(x) \\log q(x \\vert \\theta ) \\\\\n",
    "& = \\arg\\min_{\\theta} H(p, q) \\\\ \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "#### Putting it together\n",
    "\n",
    "We want to build a model that fits our data the best. We start with the maximum likelihood estimation (MLE) which later change to negative log likelihood to avoid overflow or underflow. Mathematically, the negative log likelihood and the cross entropy have the same equation. KL divergence provides another perspective in optimizing a model. However, even they uses different formula, they both end up with the same solution.\n",
    "\n",
    "> Cross entropy is one common objective function in deep learning.\n",
    "\n",
    "### Mean square error (MSE)\n",
    "\n",
    "In a regression problem, $$y = f(x; w)$$. In real life, we are dealing with un-certainty and in-complete information. So we may model the problem as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x; θ) \\\\\n",
    "y \\sim \\mathcal{N}(y;μ=\\hat{y}, σ^2) \\\\\n",
    "p(y | x; θ) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp({\\frac{-(y - \\hat{y})^{2} } {2\\sigma^{2}}}) \\\\\n",
    "$$\n",
    "\n",
    "with $$\\sigma$$ pre-defined by users:\n",
    "\n",
    "The log likelihood becomes optimizing the mean square error:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J &= \\sum^m_{i=1} \\log p(y | x; θ) \\\\\n",
    "& = \\sum^m_{i=1}  \\log \\frac{1}{\\sigma\\sqrt{2\\pi}}  \\exp({\\frac{-(y^{(i)} - \\hat{y^{(i)}})^{2} } {2\\sigma^{2}}}) \\\\\n",
    "& = \\sum^m_{i=1} - \\log(\\sigma\\sqrt{2\\pi}) - \\log \\exp({\\frac{(y^{(i)} - \\hat{y^{(i)}})^{2} } {2\\sigma^{2}}}) \\\\\n",
    "& = \\sum^m_{i=1} - \\log(\\sigma) - \\frac{1}{2} \\log( 2\\pi) - {\\frac{(y^{(i)} - \\hat{y^{(i)}})^{2} } {2\\sigma^{2}}} \\\\\n",
    "& =  - m\\log(\\sigma) - \\frac{m}{2} \\log( 2\\pi) - \\sum^m_{i=1} {\\frac{(y^{(i)} - \\hat{y^{(i)}})^{2} } {2\\sigma^{2}}} \\\\\n",
    "& =  - m\\log(\\sigma) - \\frac{m}{2} \\log( 2\\pi) - \\sum^m_{i=1} {\\frac{ \\| y^{(i)} - \\hat{y^{(i)}} \\|^{2} } {2\\sigma^{2}}} \\\\\n",
    "\\nabla_θ J & = - \\nabla_θ \\sum^m_{i=1} {\\frac{ \\| y^{(i)} - \\hat{y^{(i)}} \\|^{2} } {2\\sigma^{2}}} \\\\ \n",
    "\\end{split} \n",
    "$$\n",
    "\n",
    "> Many cost functions used in deep learning, including the MSE, can be derived from the MLE.\n",
    "\n",
    "### Maximum A Posteriori (MAP)\n",
    "\n",
    "MLE maximizes $$ p(y \\vert x; θ) $$. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "θ^* = \\arg\\max_θ (P(y \\vert x; θ)) & = \\arg\\max_w \\prod^n_{i=1} P( y \\vert x; θ)\\\\\n",
    "\\end{split} \n",
    "$$\n",
    "\n",
    "Alternative, we can find the most likely $$θ$$ given $$y$$:\n",
    "\n",
    "$$\n",
    "θ^{*}_{MAP} = \\arg \\max_θ p(θ \\vert y) = \\arg \\max_θ \\log p(y \\vert θ) + \\log p(θ) \\\\\n",
    "$$\n",
    "\n",
    "Apply Bayes' theorem:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "θ_{MAP} & = \\arg \\max_θ p(θ \\vert y) \\\\\n",
    "& = \\arg \\max_θ \\log p(y \\vert θ) + \\log p(θ) - \\log p(y)\\\\\n",
    "& = \\arg \\max_θ \\log p(y \\vert θ) + \\log p(θ) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "To demonstrate the idea, we use a Gaussian distribution of $$ \\mu=0, \\sigma^2 = \\frac{1}{\\lambda}$$ as the our prior:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(θ) & =  \\frac{1}{\\sqrt{2 \\pi \\frac{1}{\\lambda}}} e^{-\\frac{(θ - 0)^{2}}{2\\frac{1}{\\lambda}} } \\\\\n",
    "\\log p(θ) & = - \\log {\\sqrt{2 \\pi \\frac{1}{\\lambda}}} + \\log e^{- \\frac{\\lambda}{2}θ^2}  \\\\\n",
    "& = C^{'} - \\frac{\\lambda}{2}θ^2 \\\\\n",
    "- \\sum^N_{j=1} \\log p(θ) &= C + \\frac{\\lambda}{2} \\| θ \\|^2 \\quad \\text{ L-2 regularization}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Assume the likelihood is also gaussian distributed:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y^{(i)} \\vert θ) & \\propto e^{ - \\frac{(\\hat{y}^{(i)} - y^{(i)})^2}{2 \\sigma^2} } \\\\\n",
    "- \\sum^N_{i=1} \\log p(y^{(i)} \\vert θ) & \\propto \\frac{1}{2 \\sigma^2} \\| \\hat{y}^{(i)} - y^{(i)} \\|^2 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "So for a Gaussian distribution prior and likelihood, the cost function is\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(θ) & = \\sum^N_{i=1} - \\log p(y^{(i)} \\vert θ) - \\log p(θ) \\\\\n",
    "& = - \\sum^N_{i=1} \\log p(y^{(i)} | θ) - \\sum^d_{j=1} \\log p(θ) \\\\\n",
    "&=  \\frac{1}{2 \\sigma^2} \\| \\hat{y}^{(i)} - y^{(i)} \\|^2 + \\frac{\\lambda}{2} \\| θ \\|^2 + constant\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "which is the same as the MSE with L2-regularization.\n",
    "\n",
    "If the likeliness is computed from a logistic function, the corresponding cost function is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y_i \\vert x_i, w) & = \\frac{1}{ 1 + e^{- y_i w^T x_i} } \\\\\n",
    "J(w) & = - \\sum^N_{i=1} \\log p(y_i \\vert x_i, w) - \\sum^d_{j=1} \\log p(w_j) - C \\\\\n",
    "&= \\sum^N_{i=1} \\log(1 + e^{- y_i w^T x_i})  + \\frac{\\lambda}{2} \\| w \\|^2 + constant\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Like, MLE, MAP provides a mechanism to derive the cost function. However, MAP can also model the uncertainty into the cost function which turns out to be the regularization factor used in deep learning.\n",
    "\n",
    "### Nash Equilibrium\n",
    "\n",
    "In the game theory, the Nash Equilibrium is reached when no player will change its strategy after considering all possible strategy of opponents. i.e. in the Nash equilibrium, no one will change its decision even after we will all the player strategy to everyone. A game can have 0, 1 or multiple Nash Equilibria. \n",
    "\n",
    "#### The Prisoner's Dilemma\n",
    "\n",
    "In the prisoner's dilemma problem, police arrests 2 suspects but only have evidence to charge them for a lesser crime with 1 month jail time. But if one of them confess, the other party will receive a 12 months jail time and the one confess will be released. Yet, if both confess, both will receive a jail time of 6 months. The first value in each cell is what Mary will get in jail time for each decision combinations while the second value is what Peter will get.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/nash.png\" style=\"border:none;width:80%\">\n",
    "</div>\n",
    "\n",
    "For Mary, if she thinks Peter will keep quiet, her best strategy will be confess to receive no jail time instead of 1 month.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/nash2.png\" style=\"border:none;width:80%\">\n",
    "</div>\n",
    "\n",
    "On the other hand, if she thinks Peter will confess, her best strategy will be confess also to get 6 months jail time.\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/nash3.png\" style=\"border:none;width:80%\">\n",
    "</div>\n",
    "\n",
    "After knowing all possible actions, in either cases, Mary's best action is to confess. Similarly, Peter should confess also. Therefore (-6, -6) is the Nash Equilibrium even (-1, -1) is the least jail time combined. Why (-1, -1) is not a Nash Equilibrium? Because if Mary knows Peter will keep quiet, she can switch to confess and get a lesser sentence which Peter will response by confessing the crime also. (Providing that Peter and Mary cannot co-ordinate their strategy.)\n",
    "\n",
    "### Jensen-Shannon Divergence\n",
    "\n",
    "It measures how distinguishable two or more distributions are from each other.\n",
    "\n",
    "$$\n",
    "JSD(X || Y) = H(\\frac{X + Y}{2}) - \\frac{H(X) + H(Y)}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
