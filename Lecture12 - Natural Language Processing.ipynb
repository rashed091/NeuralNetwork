{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "To solve this problem we need several processing steps. First we need to convert the raw text-words into so-called tokens which are integer values. These tokens are really just indices into a list of the entire vocabulary. Then we convert these integer-tokens into so-called embeddings which are real-valued vectors, whose mapping will be trained along with the neural network, so as to map words with similar meanings to similar embedding-vectors. Then we input these embedding-vectors to a Recurrent Neural Network which can take sequences of arbitrary length as input and output a kind of summary of what it has seen in the input. This output is then squashed using a Sigmoid-function to give us a value between 0.0 and 1.0, where 0.0 is taken to mean a negative sentiment and 1.0 means a positive sentiment. This whole process allows us to classify input-text as either having a negative or positive sentiment.\n",
    "\n",
    "The flowchart of the algorithm is roughly:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/natural_language.png\" style=\"border:none;width:60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import re\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import six\n",
    "import requests\n",
    "import csv\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick fox jumped over a lazy dog.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The quick fox jumped over a lazy dog.'\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_CHARS = 20000\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "    sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "    \n",
    "    if (len(sentence) > MAX_CHARS):\n",
    "        sentence = sentence[:MAX_CHARS]\n",
    "\n",
    "    return [x.text for x in nlp.tokenizer(sentence) if x.text != \" \"]\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:19<00:00, 4.24MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12250\n",
      "Number of validation examples: 5250\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build a vocabulary. This is a effectively a look up table where every unique word in your data set has a corresponding index (an integer).\n",
    "\n",
    "<img src=\"images/one_hot.png\" width=\"350\">\n",
    "\n",
    "We do this as our machine learning model cannot operate on strings, only numbers. Each index is used to construct a one-hot vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
    "\n",
    "\n",
    "\n",
    "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one).\n",
    "\n",
    "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
    "\n",
    "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special unknown or <unk> token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I <unk> it\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I', 'really', 'hate', 'this', 'retarded', 'show', ',', 'it', 'SUCKS', '!', 'big', 'time', ',', 'and', 'personally', 'I', 'think', 'it', 'is', 'insulting', 'to', 'fairy', 'kind', '(', 'if', 'you', 'believe', 'in', 'fairies', 'that', 'is', ')', ';', 'I', 'mean', 'the', 'people', 'who', 'had', 'come', 'up', 'with', 'such', 'crap', \"'\", 'ought', 'to', 'have', 'their', 'heads', 'examine', 'huh', '?', 'and', 'also', 'there', 'is', 'a', 'LOT', 'of', 'craziness', '(', 'the', 'evil', 'school', 'teacher', ',', 'which', 'I', 'think', 'is', 'getting', 'really', 'old', ')', 'and', 'also', 'stupidity', '(', 'the', 'boy', \"'s\", 'parents', 'and', 'fairy', 'godfather', ')', 'in', 'this', 'show', '-', 'two', 'of', 'the', 'things', 'that', 'I', 'dispised', 'and', 'loathe', 'in', 'the', 'WHOLE', 'world', '(', 'especially', 'stupidity).<br', '/><br', '/>Overall', ',', 'I', 'say', 'that', 'this', 'show', 'is', 'so', 'f', '*', '*', '*', '*', '*', \"'\", 'annoying', 'and', 'should', 'not', 'be', 'seen', 'by', 'prying', 'eyes', 'at', 'ALL', '(', 'it', 'would', \"make'em\", 'bleed', 'to', 'death', ')', '!'], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 141858), (',', 134951), ('.', 115669), ('a', 76382), ('and', 76313), ('of', 70587), ('to', 65470), ('is', 53198), ('in', 43016), ('I', 37616), ('it', 37363), ('that', 34248), ('\"', 30880), (\"'s\", 30449), ('this', 29550), ('-', 25925), ('/><br', 25282), ('was', 24437), ('as', 20879), ('with', 20789)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x114eb2c80>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
    "\n",
    "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                                           batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_url(url, path):\n",
    "    \"\"\"Download file, with logic (from tensor2tensor) for Google Drive\"\"\"\n",
    "    def process_response(r):\n",
    "        chunk_size = 16 * 1024\n",
    "        total_size = int(r.headers.get('Content-length', 0))\n",
    "        with open(path, \"wb\") as file:\n",
    "            with tqdm(total=total_size, unit='B',\n",
    "                      unit_scale=1, desc=path.split('/')[-1]) as t:\n",
    "                for chunk in r.iter_content(chunk_size):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        t.update(len(chunk))\n",
    "\n",
    "    if 'drive.google.com' not in url:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True)\n",
    "        process_response(response)\n",
    "        return\n",
    "\n",
    "    print('downloading from Google Drive; may take a few minutes')\n",
    "    confirm_token = None\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, stream=True)\n",
    "    for k, v in response.cookies.items():\n",
    "        if k.startswith(\"download_warning\"):\n",
    "            confirm_token = v\n",
    "\n",
    "    if confirm_token:\n",
    "        url = url + \"&confirm=\" + confirm_token\n",
    "        response = session.get(url, stream=True)\n",
    "\n",
    "    process_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "news.csv: 6.13MB [00:00, 8.07MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "# Upload data from GitHub to notebook's local drive\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
    "file_name = 'news.csv'\n",
    "download_from_url(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "df = pd.read_csv(file_name, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABELS = data.Field()\n",
    "\n",
    "news_data = data.TabularDataset(\n",
    "    path=file_name, format='csv',skip_header=True,\n",
    "    fields=[('category', LABELS), ('title', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(news_data, max_size=25000)\n",
    "LABELS.build_vocab(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABELS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': ['Business'], 'title': ['Wall', 'St.', 'Bears', 'Claw', 'Back', 'Into', 'the', 'Black', '(', 'Reuters', ')']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(news_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 22793), ('(', 17132), (')', 17130), ('in', 16767), (',', 16321), ('-', 13503), ('#', 12950), ('for', 11660), (':', 9629), ('on', 8986), ('of', 8736), (';', 7778), ('AP', 7777), ('39;s', 6079), ('the', 4990), (\"'\", 4328), ('Reuters', 4261), ('US', 3956), (\"'s\", 3860), ('a', 3745)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x114eb2c80>, {'<unk>': 0, '<pad>': 1, 'Business': 2, 'Sci/Tech': 3, 'Sports': 4, 'World': 5})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
