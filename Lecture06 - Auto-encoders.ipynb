{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "**Autoencoder** is a neural network designed to learn an identity function in an unsupervised way  to reconstruct the original input while compressing the data in the process so as to discover a more efficient and compressed representation. The idea was originated in [the 1980s](https://en.wikipedia.org/wiki/Autoencoder), and later promoted by the seminal paper by [Hinton & Salakhutdinov, 2006](https://pdfs.semanticscholar.org/c50d/ca78e97e335d362d6b991ae0e1448914e9a3.pdf).\n",
    "\n",
    "It consists of two networks:\n",
    "- *Encoder* network: It translates the original high-dimension input into the latent low-dimensional code. The input size is larger than the output size.\n",
    "- *Decoder* network: The decoder network recovers the data from the code, likely with larger and larger output layers.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/autoencoder-architecture.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "The encoder network essentially accomplishes the [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), just like how we would use Principal Component Analysis (PCA) or Matrix Factorization (MF) for. In addition, the autoencoder is explicitly optimized for the data reconstruction from the code. A good intermediate representation not only can capture latent variables, but also benefits a full [decompression](https://ai.googleblog.com/2016/09/image-compression-with-neural-networks.html) process.\n",
    "\n",
    "The model contains an encoder function $g(.)$ parameterized by $\\phi$ and a decoder function $f(.)$ parameterized by $\\theta$. The low-dimensional code learned for input $\\mathbf{x}$ in the bottleneck layer is $\\mathbf{z} = $ and the reconstructed input is $\\mathbf{x}' = f_\\theta(g_\\phi(\\mathbf{x}))$.\n",
    "\n",
    "The parameters $(\\theta, \\phi)$ are learned together to output a reconstructed data sample same as the original input, $\\mathbf{x} \\approx f_\\theta(g_\\phi(\\mathbf{x}))$, or in other words, to learn an identity function. There are various metrics to quantify the difference between two vectors, such as cross entropy when the activation function is sigmoid, or as simple as MSE loss:\n",
    "\n",
    "$$\n",
    "L_\\text{AE}(\\theta, \\phi) = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\mathbf{x}^{(i)})))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimization\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "# 28x28\n",
    "train_dataset = data.MNIST(root='.data', train=True,\n",
    "                           transform=img_transform, download=True)\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_mnist_sample(tensor, sample_idx=None, size=10):\n",
    "    \"\"\"Plot MNIST samples from tensor.\n",
    "    \"\"\"\n",
    "    # sample from data if sample indices are not provided\n",
    "    if sample_idx is None:\n",
    "        # get sample indices\n",
    "        sample_idx = np.random.uniform(size=size) * tensor.shape[0]\n",
    "        sample_idx = sample_idx.astype(int)\n",
    "    # plot data\n",
    "    nrows = int(np.ceil(len(sample_idx)/10))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=10, figsize=(20,2))\n",
    "    for i, s in enumerate(sample_idx):\n",
    "        ax = axes.item(i)\n",
    "        ax.imshow(tensor[s].numpy())\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.set_title(\"Sample Index\\n{}\".format(s))\n",
    "        \n",
    "plot_mnist_sample(train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Conv2d(1, 32, kernel_size=5, stride=1), nn.ReLU(True),\n",
    "                                    nn.Conv2d(32, 32, kernel_size=5, stride=1), nn.ReLU(True),\n",
    "                                    nn.Conv2d(32, 32, kernel_size=4, stride=2), nn.ReLU(True),\n",
    "                                    nn.Conv2d(32, 32, kernel_size=3, stride=2), nn.ReLU(True),\n",
    "                                    nn.Conv2d(32, 8, kernel_size=4, stride=1))\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(8, 32, kernel_size=4, stride=1), nn.ReLU(True),\n",
    "                                    nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2), nn.ReLU(True),\n",
    "                                    nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2), nn.ReLU(True),\n",
    "                                    nn.ConvTranspose2d(32, 32, kernel_size=5, stride=1), nn.ReLU(True),\n",
    "                                    nn.ConvTranspose2d(32, 1, kernel_size=5, stride=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = AutoEncoder()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-712852708de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-v-rhk90k/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for images, _ in train_loader:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder\n",
    "\n",
    "Since the autoencoder learns the identity function, we are facing the risk of “overfitting” when there are more network parameters than the number of data points. \n",
    "\n",
    "To avoid overfitting and improve the robustness, **Denoising Autoencoder** (Vincent et al. 2008) proposed a modification to the basic autoencoder. The input is partially corrupted by adding noises to or masking some values of the input vector in a stochastic manner, $\\tilde{\\mathbf{x}} \\sim \\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}} \\vert \\mathbf{x})$. Then the model is trained to recover the original input (**Note: Not the corrupt one!**).\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mathbf{x}}^{(i)} &\\sim \\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}}^{(i)} \\vert \\mathbf{x}^{(i)})\\\\\n",
    "L_\\text{DAE}(\\theta, \\phi) &= \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\tilde{\\mathbf{x}}^{(i)})))^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mathcal{M}_\\mathcal{D}$ defines the mapping from the true data samples to the noisy or corrupted ones.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/denoising-autoencoder-architecture.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "This design is motivated by the fact that humans can easily recognize an object or a scene even the view is partially occluded or corrupted. To “repair” the partially destroyed input, the denoising autoencoder has to discover and capture relationship between dimensions of input in order to infer missing pieces. \n",
    "\n",
    "For high dimensional input with high redundancy, like images, the model is likely to depend on evidence gathered from a combination of many input dimensions to recover the denoised version (sounds like the [attention]({{ site.baseurl }}{% post_url 2018-06-24-attention-attention %}) mechanism, right?) rather than to overfit one dimension. This builds up a good foundation for learning *robust* latent representation.\n",
    "\n",
    "The noise is controlled by a stochastic mapping $$\\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}} \\vert \\mathbf{x})$$, and it is not specific to a particular type of corruption process (i.e. masking noise, Gaussian noise, salt-and-pepper noise, etc.). Naturally the corruption process can be equipped with prior knowledge\n",
    "\n",
    "In the experiment of the original DAE paper, the noise is applied in this way: a fixed proportion of input dimensions are selected at random and their values are forced to 0. Sounds a lot like dropout, right? Well, the denoising autoencoder was proposed in 2008, 4 years before the dropout paper ([Hinton, et al. 2012](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)) ;)\n",
    "\n",
    "**Stacked Denoising Autoencoder**: In the old days when it was still hard to train deep neural networks, stacking denoising autoencoders was a way to build deep models ([Vincent et al., 2010](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)). The denoising autoencoders are trained layer by layer. Once one layer has been trained, it is fed with clean, uncorrupted inputs to learn the encoding in the next layer.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/stacking-dae.png\" width=\"40%\">\n",
    "</center>\n",
    "    \n",
    "Image source: [Vincent et al., 2010](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder\n",
    "\n",
    "**Sparse Autoencoder** applies a \"sparse\" constraint on the hidden unit activation to avoid overfitting and improve robustness. It forces the model to only have a small number of hidden units being activated at the same time, or in other words, one hidden neuron should be inactivate most of time.\n",
    "\n",
    "Recall that common [activation functions](http://cs231n.github.io/neural-networks-1/#actfun) include sigmoid, tanh, relu, leaky relu, etc. A neuron is activated when the value is close to 1 and inactivate with a value close to 0.\n",
    "\n",
    "Let’s say there are $s_l$ neurons in the $l$-th hidden layer and the activation function for the $j$-th neuron in this layer is labelled as $a^{(l)}_j(.)$, $j=1, \\dots, s_l$. The fraction of activation of this neuron $\\hat{\\rho}_j$ is expected to be a small number $\\rho$, known as *sparsity parameter*; a common config is $\\rho = 0.05$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_j^{(l)} = \\frac{1}{n} \\sum_{i=1}^n [a_j^{(l)}(\\mathbf{x}^{(i)})] \\approx \\rho\n",
    "$$\n",
    "\n",
    "This constraint is achieved by adding a penalty term into the loss function. The KL-divergence $D_\\text{KL}$ measures the difference between two Bernoulli distributions, one with mean $$\\rho$$ and the other with mean $\\hat{\\rho}_j^{(l)}$. The hyperparameter $\\beta$ controls how strong the penalty we want to apply on the sparsity loss.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_\\text{SAE}(\\theta) \n",
    "&= L(\\theta) + \\beta \\sum_{l=1}^L \\sum_{j=1}^{s_l} D_\\text{KL}(\\rho \\| \\hat{\\rho}_j^{(l)}) \\\\\n",
    "&= L(\\theta) + \\beta \\sum_{l=1}^L \\sum_{j=1}^{s_l} \\rho\\log\\frac{\\rho}{\\hat{\\rho}_j^{(l)}} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j^{(l)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/kl-metric-sparse-autoencoder.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "\n",
    "*Fig. 4. The KL divergence between a Bernoulli distribution with mean $$\\rho=0.25$$ and a Bernoulli distribution with mean $$0 \\leq \\hat{\\rho} \\leq 1$$.*\n",
    "\n",
    "**$k$-Sparse Autoencoder**\n",
    "\n",
    "In $k$-Sparse Autoencoder ([Makhzani and Frey, 2013](https://arxiv.org/abs/1312.5663)), the sparsity is enforced by only keeping the top k highest activations in the bottleneck layer with linear activation function. \n",
    "First we run feedforward through the encoder network to get the compressed code: $$\\mathbf{z} = g(\\mathbf{x})$$.\n",
    "Sort the values  in the code vector $$\\mathbf{z}$$. Only the k largest values are kept while other neurons are set to 0. This can be done in a ReLU layer with an adjustable threshold too. Now we have a sparsified code: $$\\mathbf{z}’ = \\text{Sparsify}(\\mathbf{z})$$.\n",
    "Compute the output and the loss from the sparsified code, $$L = \\|\\mathbf{x} - f(\\mathbf{z}') \\|_2^2$$.\n",
    "And, the back-propagation only goes through the top k activated hidden units!\n",
    "\n",
    "<center>\n",
    "<img src=\"images/k-sparse-autoencoder.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "*Fig. 5. Filters of the k-sparse autoencoder for different sparsity levels k, learnt from MNIST with 1000 hidden units.. (Image source: [Makhzani and Frey, 2013](https://arxiv.org/abs/1312.5663))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractive Autoencoder\n",
    "\n",
    "Similar to sparse autoencoder, **Contractive Autoencoder** ([Rifai, et al, 2011](http://www.icml-2011.org/papers/455_icmlpaper.pdf)) encourages the learned representation to stay in a contractive space for better robustness. \n",
    "\n",
    "It adds a term in the loss function to penalize the representation being too sensitive to the input,  and thus improve the robustness to small perturbations around the training data points. The sensitivity is measured by the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input:\n",
    "\n",
    "\n",
    "$$\n",
    "\\|J_f(\\mathbf{x})\\|_F^2 = \\sum_{ij} \\Big( \\frac{\\partial h_j(\\mathbf{x})}{\\partial x_i} \\Big)^2\n",
    "$$\n",
    "\n",
    "where $$h_j$$ is one unit output in the compressed code $$\\mathbf{z} = f(x)$$. \n",
    "\n",
    "This penalty term is the sum of squares of all partial derivatives of the learned encoding with respect to input dimensions. The authors claimed that empirically this penalty was found to  carve a representation that corresponds to a lower-dimensional non-linear manifold, while staying more invariant to majority directions orthogonal to the manifold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto Encoders (VAE)\n",
    "\n",
    "In a VAE, there is a strong assumption for the distribution that is learned in the hidden representation. The hidden representation is constrained to be a multivariate guassian. The motivation behind this is that we assume the hidden representation learns high level features and these features follow a very simple form of distribiution. Thus, we assume that each feature is a guassian distribiution and their combination which creates the hidden representation is a multivariate guassian.From a probabilistic graphical models prespective, an auto encoder can be seen as a directed graphical model where the hidden units are latent variables ($z$) and the following rule applies:\n",
    "\n",
    "$$p_{\\theta}(x,\\ z) = p_{\\theta}(z)\\ p_{\\theta}(x|z),$$\n",
    "\n",
    "\n",
    "where $ \\theta$  indicates that $p$  is parametrized by $ \\theta$  . And according to the Bayes rule, the likelihood of the data ( $p_{ \\theta}(x)$  ) can be derived using the following:\n",
    "\n",
    "$$p_{ \\theta}(x) =  \\frac{p_{ \\theta}(x| \\ z) \\ p_{ \\theta}(z)}{p_{ \\theta}(z, \\ x)},$$ \n",
    " \n",
    "$p_{ \\theta}(x|z)$ is the distribiution that generates the data ( $x$  ) and is tractable using the dataset. In a VAE it is assumed the prior distribiution ( $p_{ \\theta}(z)$  ) is a multivariate normal distribiution (centered at zero with co-varience of $I$):\n",
    "\n",
    "$$p_{ \\theta}(z) =  \\prod_{k=1} ^ N  \\mathcal{N}(z_{k} \\ | \\ 0,1)$$  \n",
    "\n",
    "The posterior distribiution ( $p_{ \\theta}(z|x)$  ) is an intractable distribiution (never observed), but the encoder learns $q_{ \\varphi}(z|x)$  as its estimator. As mentioned above, we assume $q_{ \\varphi}(z|x)$  is a normal distribiution which is parameterized by ${ \\varphi}$  :\n",
    "\n",
    "$$q_{ \\varphi}(z|x) =  \\prod_{k=1} ^ N  \\mathcal{N}(z_{k} \\ | \\  \\mu_{k}(x), \\  \\sigma_{k}^2(x)).$$  \n",
    "\n",
    "Now the likelihood is parameterized by $ \\theta$  and $ \\varphi$  . The goal is to find a $ \\theta^{*}$  and a $ \\varphi^{*}$  such that $log \\ p_{ \\theta,  \\varphi}(x)$  is maximized. Or equivallently, we minimize the negative log-likelihood (nill):\n",
    "\n",
    "In this setting, the following is a lower-bound on the log-likelihood of $x$  . \n",
    "\n",
    "$$ \\mathcal{L}(x) = - D_{kl} \\ (q_{ \\varphi}(z|x) \\ || \\ p_{ \\theta}(z)) + E_{q_{ \\varphi} \\ (z|x)}[ \\ log p_{ \\theta}(x \\ | \\ z)],$$  \n",
    "\n",
    "The second term is a reconstruction error which is approximated by sampling from $q_{ \\varphi}(z|x)$  (the encoder) and then computing $p_{ \\theta}(x \\ | \\ z)$  (the decoder). The first term, $D_{kl}$  is the Kullback–Leibler divergence which measures the differnce between two probability distribiutions. The KL term encourges the model to learn a $q_{ \\varphi}(z|x)$  that is of the form of $p_{ \\theta}(z)$  which is a normal distribiution and acts as a regularizer. Considering that $p_{ \\theta}(z)$  and $q_{ \\varphi}(z|x)$  are normal distribiutions, the KL term can be simplified to the following form:\n",
    "\n",
    "$$ D_{kl}=  \\frac{1}{2} \\  \\sum_{k = 1}^{N} \\ 1 \\ + \\ log( \\sigma_{k}^2 (x))-  \\mu_{k}^{2} (x) -  \\sigma_{k}^2 (x)$$\n",
    "\n",
    "<center>\n",
    "<img src=\"images/vae.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Variational Auto Encoder: the hidden units parameterize the multivariate normal distribiution $q_{ \\varphi}(z|x)$ by finding a set of $( \\  \\mu, \\  \\sigma^2)$ . $  \\mathcal{N}$ is a normal distribiution from which we sample and is used in the reparametrization trick.\n",
    "\n",
    "### In short  \n",
    "A variational autoencoder has a very similar structure to an autoencoder except for several changes: Strong assumption that the hidden representation follows a guassian distribiution. The loss function has a new regularizer term (KL term) which forces the hidden representation to be close to a normal distribiution. The model can be used for generation. Since the KL term makes sure that $q_{ \\varphi}(z|x)$ and $p_{ \\theta}(z)$ are close, one can sample from $q_{ \\varphi}(z|x)$ to generate new datapoints which will look very much like training samples. \n",
    "\n",
    "\n",
    "### The Reparametrization \n",
    "Trick  The problem that might come to ones mind is that how the gradient flows through a VAE where it involves sampling from $q_{ \\varphi}(z|x)$  which is a non-deterministic procedure. To tackle this problem, the reparametrization trick is used. In order to have a sample from the distribiution $  \\mathcal{N}( \\  \\mu, \\  \\sigma^2)$  , one can first sample from a normal distribiution $  \\mathcal{N}( \\ 0, \\ 1)$  and then calculate: \n",
    "\n",
    "$$  \\mathcal{N}( \\  \\mu, \\  \\sigma^2) =  \\mathcal{N}( \\ 0, \\ 1) *  \\sigma^2 +  \\mu$$ \n",
    "\n",
    "<center>\n",
    "<img src=\"images/reparam.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "The reparametrization Trick: sampling from $  \\mathcal{N}( \\  \\mu, \\  \\sigma^2)$ is non-deteministic and thus the gradient can not be passed through it. If one samples from the noraml distribiution $  \\mathcal{N}( \\ 0, \\ 1)$ and just scale and move it using the parameters $ \\sigma^2, \\  \\mu$ , the backward pass will be deterministic. \n",
    "\n",
    "PyTorch implementation of a VAE. The code also generates new samples. It also does a generation with interpolation which means that it starts from one sample and moves towards another sample in the latent space and generates new samples by interpolating between the mean and the variance of the first and second sample. Fig7. shows the generated samples with interpolation. Fig7. From left to right: generated samples with interpolation at 1st, 40th, 80th, 150th and 200th epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE cost function in detail\n",
    "\n",
    "In VAE, we want to model the data distribution $p(x)$ with an encoder $ q_ϕ(z \\vert x)$ , a decoder $p_θ(x  \\vert z) $ and a latent variable model $p(z)$ through the VAE objective function:\n",
    "\n",
    "$$\n",
    "\\log p(x) \\approx \\mathbb{E}_q [   \\log p_θ (x \\vert z)] - D_{KL} [q_ϕ (z \\vert x) \\Vert p(z)]   \\\\\n",
    "$$\n",
    "\n",
    "To draw this conclusion, we start with the KL divergence which measures the difference of 2 distributions. By definition, KL divergence is defined as: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}\\left(q \\Vert p\\right) & = \\sum_{x} q(x) \\log (\\frac{q(x)}{p(x)}) \\\\ \n",
    "& = \\mathbb{E}_q[log (q(x))−log (p(x))] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Apply it with:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}[q(z \\vert x) \\Vert p(z \\vert x)] &= \\mathbb{E}[\\log q(z \\vert x) - \\log p(z \\vert x)] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Let $ q_\\lambda (z \\vert x) $ be the distribution of $ z $ predicted by our encoder deep network. We want it to match the true distribution $ p(z \\vert x) $. We want the distribution approximated by the deep network has little divergence from the true distribution. i.e. we want to optimize $\\lambda $ with the smallest KL divergence.\n",
    "\n",
    "$$\n",
    "D_{KL} [ q_λ (z \\vert x) \\Vert p(z \\vert x) ] = \\mathbb{E}_q [ \\log q_λ (z \\vert x)  -   \\log p (z \\vert x) ]\n",
    "$$\n",
    "\n",
    "Apply:\n",
    "\n",
    "$$\n",
    "p(z \\vert x) = \\frac{p(x \\vert z) p(z)}{p(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ] & = \\mathbb{E}_q [ \\log q_λ (z \\vert x) - \\log \\frac{ p (x \\vert z) p(z)}{p(x)}  ] \\\\\n",
    "& = \\mathbb{E}_q [ \\log q_λ (z \\vert x)  - \\log p (x \\vert z) - \\log p(z)  + \\log p(x)]   \\\\\n",
    "& = \\mathbb{E}_q [ \\log q_λ (z \\vert x)  - \\log p (x \\vert z) - \\log p(z) ] + \\log p(x) \\\\\n",
    " D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ]  - \\log p(x) & = \\mathbb{E}_q [ \\log q_λ (z \\vert x)  - \\log p (x \\vert z) - \\log p(z) ] \\\\\n",
    " \\log p(x) - D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ]  & = \\mathbb{E}_q [   \\log p (x \\vert z) - ( \\log q_λ (z \\vert x) - \\log p(z)) ] \\\\\n",
    "&=  \\mathbb{E}_q [   \\log p (x \\vert z)] - \\mathbb{E}_q [ \\log q_λ (z \\vert x) - \\log p(z)) ] \\\\\n",
    "&=  \\mathbb{E}_q [   \\log p (x \\vert z)] - D_{KL} [q_λ (z \\vert x) \\Vert p(z)] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Define the term ELBO (Evidence lower bound) as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "ELBO(λ) & =  \\mathbb{E}_q [   \\log p (x \\vert z)] - D_{KL} [q_λ (z \\vert x) \\Vert p(z)] \\\\\n",
    "\\log p(x) - D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ] & = ELBO(λ)  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We call ELBO the evidence lower bound because:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x) - D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ] & = ELBO(λ) \\\\\n",
    "\\log p(x) & \\geqslant ELBO(λ) \\quad \\text{since KL is always positive} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, we define our VAE objective function\n",
    "\n",
    "> $$ \\log p(x) - D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ] = \\mathbb{E}_q [   \\log p (x \\vert z)] - D_{KL} [q_λ (z \\vert x) \\Vert p(z)]  $$\n",
    "\n",
    "\n",
    "Instead of the distribution $p(x)$, we can model the data $x$ with $ \\log p(x) $. With the error term, $D_{KL} [ q_\\lambda (z \\vert x) \\Vert p(z \\vert x)  ]$, we can establish a lower bound $ELBO$ for $ \\log p(x) $ which in practice is good enough in modeling the data distribution. In the VAE objective function, maximize our model probability $ \\log p(x) $ is the same as maximize $ \\log p (x \\vert z)]$ while minimize the divergence of $D_{KL} [q_λ (z \\vert x) \\Vert p(z)] $. \n",
    "\n",
    "Maximizing $\\log p (x \\vert z)$ can be done by building a decoder network and maximize its likelihood. So with an encoder $ q_ϕ(z \\vert x)$ , a decoder $p_θ(x  \\vert z) $, our objective become optimizing:\n",
    "\n",
    "$$\n",
    "ELBO(\\theta, \\phi) = E_{q_\\theta(z \\vert x) }  [  \\log (p_{\\theta}(x_{i}|z))  ] - D_{KL} [ q_\\phi (z \\vert x) \\Vert p(z) ]\n",
    "$$\n",
    "\n",
    "We can apply a constrain to $ p(z) $ such that we can evaluate $D_{KL} [ q_\\phi (z \\vert x) \\Vert p(z) ]$ easily. In AVE, we use  $ p(z) = \\mathcal{N} (0, 1) $. For optimal solution, we want $ q_\\phi (z \\vert x) $ to be as close as $\\mathcal{N} (0, 1) $.\n",
    "\n",
    "In VAE, we model $ q_\\phi (z \\vert x) $ as $ \\mathcal{N} (\\mu, \\Sigma)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL} [ q_\\phi (z \\vert x) \\Vert p(z) ] &= D_{KL}[N(\\mu, \\Sigma) \\Vert N(0, 1)] \\\\\n",
    "& = \\frac{1}{2} \\, ( \\textrm{tr}(\\Sigma) + \\mu^T\\mu - k - \\log \\, \\det(\\Sigma) ) \\\\\n",
    "& = \\frac{1}{2} \\, ( \\sum_k \\Sigma + \\sum_k \\mu^2 - \\sum_k 1 - \\log \\, \\prod_k \\Sigma ) \\\\\n",
    "& = \\frac{1}{2} \\, ( \\sum_k \\Sigma(X) + \\sum_k \\mu^2(X) - \\sum_k 1 - \\sum_k \\log \\Sigma(X) ) \\\\\n",
    "& = \\frac{1}{2} \\, \\sum_k ( \\Sigma + \\mu^2 - 1 - \\log \\Sigma )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### KL-divergence of 2 Gaussian distributions\n",
    "\n",
    "Here is an exercise in computing the KL divergence of 2 simple gaussian distributions:\n",
    "\n",
    "$$\n",
    "p(x) = N(\\mu_1, \\sigma_1) \\\\\n",
    "q(x) = N(\\mu_2, \\sigma_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(p, q) &= \\int \\left[\\log( p(x)) - log( q(x)) \\right] p(x) dx \\\\\n",
    "& = E_1 \\left[ -\\frac{1}{2} \\log(2\\pi) - \\log(\\sigma_1) - \\frac{1}{2} \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 + \\frac{1}{2}\\log(2\\pi) + \\log(\\sigma_2) + \\frac{1}{2} \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2  \\right] \\\\\n",
    "&=E_{1} \\left\\{\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2} \\left[ \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2 - \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\right]\\right\\} \\\\\n",
    "& =\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2\\sigma_1^2} E_1 \\left\\{(X-\\mu_1)^2\\right\\} \\\\\n",
    "& =\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2} \\quad \\text{ because } E_1 \\left\\{(X-\\mu_1)^2\\right\\} = \\sigma_1^2\\\\\n",
    "Note: & (X - \\mu_2)^2 = (X-\\mu_1+\\mu_1-\\mu_2)^2 = (X-\\mu_1)^2 + 2(X-\\mu_1)(\\mu_1-\\mu_2) + (\\mu_1-\\mu_2)^2 \\\\\n",
    "KL(p, q) & = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2}\n",
    "\\left[E_1\\left\\{(X-\\mu_1)^2\\right\\} + 2(\\mu_1-\\mu_2)E_1\\left\\{X-\\mu_1\\right\\} + (\\mu_1-\\mu_2)^2\\right] - \\frac{1}{2} \\\\\n",
    "& = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
