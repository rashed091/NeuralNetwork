{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Gradient Vector\n",
    "\n",
    "First of all, I would like to make sure we can distinguish the following terms. They are very similar, closely related, but not exactly the same. The rate of change of a function $f(x,y,z,...)$ at a point $(x_0,y_0,z_0,...)$, which is the slope of the tangent line at the point. The instantaneous rate of change of $f(x,y,z, ...)$ in the direction of an unit vector $\\vec{u}$. It points in the direction of the greatest rate of increase of the function, containing all the partial derivative information of a multivariable function.\n",
    "\n",
    "In the image processing, we want to know the direction of colors changing from one extreme to the other (i.e. black to white on a grayscale image). Therefore, we want to measure \"gradient\" on pixels of colors. The gradient on an image is discrete because each pixel is independent and cannot be further split.\n",
    "\n",
    "The [image gradient vector](https://en.wikipedia.org/wiki/Image_gradient) is defined as a metric for every individual pixel, containing the pixel color changes in both x-axis and y-axis. The definition is aligned with the gradient of a continuous multi-variable function, which is a vector of partial derivatives of all the variables. Suppose f(x, y) records the color of the pixel at location (x, y), the gradient vector of the pixel (x, y) is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f(x, y)\n",
    "= \\begin{bmatrix}\n",
    "  g_x \\\\\n",
    "  g_y\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "  \\frac{\\partial f}{\\partial x} \\\\[6pt]\n",
    "  \\frac{\\partial f}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "  f(x+1, y) - f(x-1, y)\\\\\n",
    "  f(x, y+1) - f(x, y-1)\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $\\frac{\\partial f}{\\partial x}$ term is the partial derivative on the x-direction, which is computed as the color difference between the adjacent pixels on the left and right of the target, f(x+1, y) - f(x-1, y). Similarly, the $\\frac{\\partial f}{\\partial y}$ term is the partial derivative on the y-direction, measured as f(x, y+1) - f(x, y-1), the color difference between the adjacent pixels above and below the target.\n",
    "\n",
    "\n",
    "There are two important attributes of an image gradient:\n",
    "- **Magnitude** is the L2-norm of the vector, $g = \\sqrt{ g_x^2 + g_y^2 }$.\n",
    "- **Direction** is the arctangent of the ratio between the partial derivatives on two directions, $\\theta = \\arctan{(g_y / g_x)}$.\n",
    "\n",
    "![Pixels for Gradient Vector]({{ '/assets/images/image-gradient-vector-pixel-location.png' | relative_url }})\n",
    "{: style=\"width: 70%;\" class=\"center\"}\n",
    "*Fig. 1. To compute the gradient vector of a target pixel at location (x, y), we need to know the colors of its four neighbors (or eight surrounding pixels depending on the kernel).*\n",
    "\n",
    "\n",
    "The gradient vector of the example in Fig. 1. is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f \n",
    "= \\begin{bmatrix}\n",
    "  f(x+1, y) - f(x-1, y)\\\\\n",
    "  f(x, y+1) - f(x, y-1)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "  55-105\\\\\n",
    "  90-40\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "  -50\\\\\n",
    "  50\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "- the magnitude is $\\sqrt{50^2 + (-50)^2} = 70.7107$, and\n",
    "- the direction is $\\arctan{(-50/50)} = -45^{\\circ}$.\n",
    "\n",
    "Repeating the gradient computation process for every pixel iteratively is too slow. Instead, it can be well translated into applying a convolution operator on the entire image matrix, labeled as $\\mathbf{A}$ using one of the specially designed convolutional kernels.\n",
    "\n",
    "Let's start with the x-direction of the example in Fig 1. using the kernel $[-1,0,1]$ sliding over the x-axis; $\\ast$ is the convolution operator:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{G}_x &= \n",
    "[-1, 0, 1] \\ast [105, 255, 55] = -105 + 0 + 55 = -50\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, on the y-direction, we adopt the kernel $[+1, 0, -1]^\\top$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{G}_y &= \n",
    "[+1, 0, -1]^\\top \\ast\n",
    "\\begin{bmatrix}\n",
    "  90\\\\\n",
    "  255\\\\\n",
    "  40\n",
    "\\end{bmatrix} \n",
    "= 90 + 0 - 40 = 50\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Try this in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "data = np.array([[0, 105, 0], [40, 255, 90], [0, 55, 0]])\n",
    "G_x = sig.convolve2d(data, np.array([[-1, 0, 1]]), mode='valid') \n",
    "G_y = sig.convolve2d(data, np.array([[-1], [0], [1]]), mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions return `array([[0], [-50], [0]])` and `array([[0, 50, 0]])` respectively. (Note that in the numpy array representation, 40 is shown in front of 90, so -1 is listed before 1 in the kernel correspondingly.)\n",
    "\n",
    "### Common Image Processing Kernels\n",
    "\n",
    "[Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator): Rather than only relying on four directly adjacent neighbors, the Prewitt operator utilizes eight surrounding pixels for smoother results.\n",
    "\n",
    "$$\n",
    "\\mathbf{G}_x = \\begin{bmatrix}\n",
    "-1 & 0 & +1 \\\\\n",
    "-1 & 0 & +1 \\\\\n",
    "-1 & 0 & +1\n",
    "\\end{bmatrix} \\ast \\mathbf{A} \\text{ and }\n",
    "\\mathbf{G}_y = \\begin{bmatrix}\n",
    "+1 & +1 & +1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{bmatrix} \\ast \\mathbf{A}\n",
    "$$\n",
    "\n",
    "[Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator): To emphasize the impact of directly adjacent pixels more, they get assigned with higher weights. \n",
    "\n",
    "$$\n",
    "\\mathbf{G}_x = \\begin{bmatrix}\n",
    "-1 & 0 & +1 \\\\\n",
    "-2 & 0 & +2 \\\\\n",
    "-1 & 0 & +1\n",
    "\\end{bmatrix} \\ast \\mathbf{A} \\text{ and }\n",
    "\\mathbf{G}_y = \\begin{bmatrix}\n",
    "+1 & +2 & +1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -2 & -1\n",
    "\\end{bmatrix} \\ast \\mathbf{A}\n",
    "$$\n",
    "\n",
    "Different kernels are created for different goals, such as edge detection, blurring, sharpening and many more. Check [this wiki page](https://en.wikipedia.org/wiki/Kernel_(image_processing)) for more examples and references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Manu in 2004\n",
    "\n",
    "Let's run a simple experiment on the photo of Manu Ginobili in 2004 [[Download Image]({{ '/assets/data/manu-2004.jpg' | relative_url }}){:target=\"_blank\"}] when he still had a lot of hair. For simplicity, the photo is converted to grayscale first. For colored images, we just need to repeat the same process in each color channel respectively.\n",
    "\n",
    "<img src=\"images/manu-2004.png\" width=\"60%\">\n",
    "\n",
    "Manu Ginobili in 2004 with hair. (Image source: [Manu Ginobili's bald spot through the years](http://ftw.usatoday.com/2013/05/manu-ginobilis-bald-spot-through-the-years))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newscred/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'manu-2004.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d40bed2562fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# With mode=\"L\", we force the image to be parsed in the grayscale, so it is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# actually unnecessary to convert the photo color beforehand.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"manu-2004.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define the Sobel operator kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'manu-2004.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal as sig\n",
    "# With mode=\"L\", we force the image to be parsed in the grayscale, so it is\n",
    "# actually unnecessary to convert the photo color beforehand.\n",
    "img = scipy.misc.imread(\"manu-2004.jpg\", mode=\"L\")\n",
    "\n",
    "# Define the Sobel operator kernels.\n",
    "kernel_x = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\n",
    "kernel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "\n",
    "G_x = sig.convolve2d(img, kernel_x, mode='same') \n",
    "G_y = sig.convolve2d(img, kernel_y, mode='same') \n",
    "\n",
    "# Plot them!\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "# Actually plt.imshow() can handle the value scale well even if I don't do \n",
    "# the transformation (G_x + 255) / 2.\n",
    "ax1.imshow((G_x + 255) / 2, cmap='gray'); ax1.set_xlabel(\"Gx\")\n",
    "ax2.imshow((G_y + 255) / 2, cmap='gray'); ax2.set_xlabel(\"Gy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/manu-2004-sobel-operator.png\" width=\"60%\">\n",
    "\n",
    "Apply Sobel operator kernel on the example image.*\n",
    "\n",
    "\n",
    "You might notice that most area is in gray. Because the difference between two pixel is between -255 and 255 and we need to convert them back to [0, 255] for the display purpose. A simple linear transformation ($\\mathbf{G}$ + 255)/2 would interpret all the zeros (i.e., constant colored background shows no change in gradient) as 125 (shown as gray)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "The Histogram of Oriented Gradients (HOG) is an efficient way to extract features out of the pixel colors for building an object recognition classifier. With the knowledge of image gradient vectors, it is not hard to understand how HOG works. Let's start!\n",
    "\n",
    "\n",
    "### How HOG works\n",
    "\n",
    "1) Preprocess the image, including resizing and color normalization.\n",
    "\n",
    "2) Compute the gradient vector of every pixel, as well as its magnitude and direction.\n",
    "\n",
    "3) Divide the image into many 8x8 pixel cells. In each cell, the magnitude values of these 64 cells are binned and cumulatively added into 9 buckets of unsigned direction (no sign, so 0-180 degree rather than 0-360 degree; this is a practical choice based on empirical experiments). \n",
    "<br/><br/>\n",
    "For better robustness, if the direction of the gradient vector of a pixel lays between two buckets, its magnitude does not all go into the closer one but proportionally split between two. For example, if a pixel's gradient vector has magnitude 8 and degree 15, it is between two buckets for degree 0 and 20 and we would assign 2 to bucket 0 and 6 to bucket 20. \n",
    "<br/><br/>\n",
    "This interesting configuration makes the histogram much more stable when small distortion is applied to the image.\n",
    "\n",
    "<img src=\"images/HOG-histogram-creation.png\" width=\"60%\">\n",
    "\n",
    "How to split one gradient vector's magnitude if its degress is between two degree bins. (Image source: https://www.learnopencv.com/histogram-of-oriented-gradients/)*\n",
    "\n",
    "4) Then we slide a 2x2 cells (thus 16x16 pixels) block across the image. In each block region, 4 histograms of 4 cells are concatenated into one-dimensional vector of 36 values and then normalized to have an unit weight.\n",
    "The final HOG feature vector is the concatenation of all the block vectors. It can be fed into a classifier like SVM for learning object recognition tasks.\n",
    "\n",
    "### Example: Manu in 2004\n",
    "\n",
    "Let's reuse the same example image in the previous section. Remember that we have computed $\\mathbf{G}_x$ and $\\mathbf{G}_y$ for the whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0445682cd938>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0445682cd938>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    lambda (m, d): assign_bucket_vals(m, d, bucket_vals),\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "N_BUCKETS = 9\n",
    "CELL_SIZE = 8  # Each cell is 8x8 pixels\n",
    "BLOCK_SIZE = 2  # Each block is 2x2 cells\n",
    "\n",
    "def assign_bucket_vals(m, d, bucket_vals):\n",
    "    left_bin = int(d / 20.)\n",
    "    # Handle the case when the direction is between [160, 180)\n",
    "    right_bin = (int(d / 20.) + 1) % N_BUCKETS\n",
    "    assert 0 <= left_bin < right_bin < N_BUCKETS\n",
    "\n",
    "    left_val= m * (right_bin * 20 - d) / 20\n",
    "    right_val = m * (d - left_bin * 20) / 20\n",
    "    bucket_vals[left_bin] += left_val\n",
    "    bucket_vals[right_bin] += right_val\n",
    "\n",
    "def get_magnitude_hist_cell(loc_x, loc_y):\n",
    "    # (loc_x, loc_y) defines the top left corner of the target cell.\n",
    "    cell_x = G_x[loc_x:loc_x + CELL_SIZE, loc_y:loc_y + CELL_SIZE]\n",
    "    cell_y = G_y[loc_x:loc_x + CELL_SIZE, loc_y:loc_y + CELL_SIZE]\n",
    "    magnitudes = np.sqrt(cell_x * cell_x + cell_y * cell_y)\n",
    "    directions = np.abs(np.arctan(cell_y / cell_x) * 180 / np.pi)\n",
    "\n",
    "    buckets = np.linspace(0, 180, N_BUCKETS + 1)\n",
    "    bucket_vals = np.zeros(N_BUCKETS)\n",
    "    map(\n",
    "        lambda (m, d): assign_bucket_vals(m, d, bucket_vals), \n",
    "        zip(magnitudes.flatten(), directions.flatten())\n",
    "    )\n",
    "    return bucket_vals\n",
    "\n",
    "def get_magnitude_hist_block(loc_x, loc_y):\n",
    "    # (loc_x, loc_y) defines the top left corner of the target block.\n",
    "    return reduce(\n",
    "        lambda arr1, arr2: np.concatenate((arr1, arr2)),\n",
    "        [get_magnitude_hist_cell(x, y) for x, y in zip(\n",
    "            [loc_x, loc_x + CELL_SIZE, loc_x, loc_x + CELL_SIZE],\n",
    "            [loc_y, loc_y, loc_y + CELL_SIZE, loc_y + CELL_SIZE],\n",
    "        )]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code simply calls the functions to construct a histogram and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_magnitude_hist_block' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58396a322889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_magnitude_hist_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mydata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_magnitude_hist_block' is not defined"
     ]
    }
   ],
   "source": [
    "# Random location [200, 200] as an example.\n",
    "loc_x = loc_y = 200\n",
    "\n",
    "ydata = get_magnitude_hist_block(loc_x, loc_y)\n",
    "ydata = ydata / np.linalg.norm(ydata)\n",
    "\n",
    "xdata = range(len(ydata))\n",
    "bucket_names = np.tile(np.arange(N_BUCKETS), BLOCK_SIZE * BLOCK_SIZE)\n",
    "\n",
    "assert len(ydata) == N_BUCKETS * (BLOCK_SIZE * BLOCK_SIZE)\n",
    "assert len(bucket_names) == len(ydata)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(xdata, ydata, align='center', alpha=0.8, width=0.9)\n",
    "plt.xticks(xdata, bucket_names * 20, rotation=90)\n",
    "plt.xlabel('Direction buckets')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.grid(ls='--', color='k', alpha=0.1)\n",
    "plt.title(\"HOG of block at [%d, %d]\" % (loc_x, loc_y))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, I use the block with top left corner located at [200, 200] as an example and here is the final normalized histogram of this block. You can play with the code to change the block location to be identified by a sliding window.\n",
    "\n",
    "<img src=\"images/block_histogram.png\" width=\"60%\">\n",
    "\n",
    "Demonstration of a HOG histogram for one block.*\n",
    "\n",
    "\n",
    "The code is mostly for demonstrating the computation process. There are many off-the-shelf libraries with HOG algorithm implemented, such as [OpenCV](https://github.com/opencv/opencv), [SimpleCV](http://simplecv.org/) and [scikit-image](http://scikit-image.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation (Felzenszwalb's Algorithm)\n",
    "\n",
    "When there exist multiple objects in one image (true for almost every real-world photos), we need to identify a region that potentially contains a target object so that the classification can be executed more efficiently. \n",
    "\n",
    "Felzenszwalb and Huttenlocher ([2004](http://cvcl.mit.edu/SUNSeminar/Felzenszwalb_IJCV04.pdf)) proposed an algorithm for segmenting an image into similar regions using a graph-based approach. It is also the initialization method for Selective Search (a popular region proposal algorithm) that we are gonna discuss later.\n",
    "\n",
    "Say, we use a undirected graph $G=(V, E)$ to represent an input image. One vertex $v_i \\in V$ represents one pixel. One edge $e = (v_i, v_j) \\in E$ connects two vertices $v_i$ and $v_j$. Its associated weight $w(v_i, v_j)$ measures the dissimilarity between $v_i$ and $v_j$. The dissimilarity can be quantified in dimensions like color, location, intensity, etc. The higher the weight, the less similar two pixels are. A segmentation solution $S$ is a partition of $V$ into multiple connected components, $\\{C\\}$. Intuitively similar pixels should belong to the same components while dissimilar ones are assigned to different components.\n",
    "\n",
    "\n",
    "### Graph Construction\n",
    "\n",
    "There are two approaches to constructing a graph out of an image.\n",
    "- **Grid Graph**: Each pixel is only connected with surrounding neighbours (8 other cells in total). The edge weight is the absolute difference between the intensity values of the pixels.\n",
    "- **Nearest Neighbor Graph**: Each pixel is a point in the feature space (x, y, r, g, b), in which (x, y) is the pixel location and (r, g, b) is the color values in RGB. The weight is the Euclidean distance between two pixels' feature vectors.\n",
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "Before we lay down the criteria for a good graph partition (aka image segmentation), let us define a couple of key concepts:\n",
    "- **Internal difference**: $Int(C) = \\max_{e\\in MST(C, E)} w(e)$, where $MST$ is the minimum spanning tree of the components. A component $C$ can still remain connected even when we have removed all the edges with weights < $Int(C)$.\n",
    "- **Difference between two components**: $Dif(C_1, C_2) = \\min_{v_i \\in C_1, v_j \\in C_2, (v_i, v_j) \\in E} w(v_i, v_j)$. $Dif(C_1, C_2) = \\infty$ if there is no edge in-between.\n",
    "- **Minimum internal difference**: $MInt(C_1, C_2) = min(Int(C_1) + \\tau(C_1), Int(C_2) + \\tau(C_2))$, where $\\tau(C) = k / \\vert C \\vert$ helps make sure we have a meaningful threshold for the difference between components. With a higher $k$, it is more likely to result in larger components. \n",
    "\n",
    "The quality of a segmentation is assessed by a pairwise region comparison predicate defined for given two regions $C_1$ and $C_2$:\n",
    "\n",
    "$$\n",
    "D(C_1, C_2) = \n",
    "\\begin{cases}\n",
    "  \\text{True} & \\text{ if } Dif(C_1, C_2) > MInt(C_1, C_2) \\\\\n",
    "  \\text{False} & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Only when the predicate holds True, we consider them as two independent components; otherwise the segmentation is too fine and they probably should be merged.\n",
    "\n",
    "\n",
    "### How Image Segmentation Works\n",
    "\n",
    "The algorithm follows a bottom-up procedure. Given $G=(V, E)$ and $|V|=n, |E|=m$:\n",
    "1. Edges are sorted by weight in ascending order, labeled as $e_1, e_2, \\dots, e_m$.\n",
    "2. Initially, each pixel stays in its own component, so we start with $n$ components.\n",
    "3. Repeat for $k=1, \\dots, m$:\n",
    "    * The segmentation snapshot at the step $k$ is denoted as $S^k$.\n",
    "    * We take  the k-th edge in the order, $e_k = (v_i, v_j)$. \n",
    "    * If $v_i$ and $v_j$ belong to the same component, do nothing and thus $S^k = S^{k-1}$.\n",
    "    * If $v_i$ and $v_j$ belong to two different components $C_i^{k-1}$ and $C_j^{k-1}$ as in the segmentation $S^{k-1}$, we want to merge them into one if $w(v_i, v_j) \\leq MInt(C_i^{k-1}, C_j^{k-1})$; otherwise do nothing.\n",
    "\n",
    "If you are interested in the proof of the segmentation properties and why it always exists, please refer to the [paper](http://fcv2011.ulsan.ac.kr/files/announcement/413/IJCV(2004)%20Efficient%20Graph-Based%20Image%20Segmentation.pdf).\n",
    "\n",
    "<img src=\"images/image-segmentation-indoor.png\" width=\"60%\">\n",
    "\n",
    "An indoor scene with segmentation detected by the grid graph construction in Felzenszwalb's graph-based segmentation algorithm (k=300).*\n",
    "\n",
    "\n",
    "### Example: Manu in 2013\n",
    "\n",
    "This time I would use the photo of old Manu Ginobili in 2013, as the example image when his bald spot has grown up strong. Still for simplicity, we use the picture in grayscale.\n",
    "\n",
    "<img src=\"images/manu-2013.png\" width=\"60%\">\n",
    "\n",
    "Manu Ginobili in 2013 with bald spot. (Image source: [Manu Ginobili's bald spot through the years](http://ftw.usatoday.com/2013/05/manu-ginobilis-bald-spot-through-the-years))*\n",
    "\n",
    "\n",
    "Rather than coding from scratch, let us apply [skimage.segmentation.felzenszwalb](http://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb) to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newscred/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'manu-2013.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-593454f04132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"manu-2013.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msegment_mask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfelzenszwalb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msegment_mask2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfelzenszwalb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'manu-2013.jpg'"
     ]
    }
   ],
   "source": [
    "import skimage.segmentation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img2 = scipy.misc.imread(\"manu-2013.jpg\", mode=\"L\")\n",
    "segment_mask1 = skimage.segmentation.felzenszwalb(img2, scale=100)\n",
    "segment_mask2 = skimage.segmentation.felzenszwalb(img2, scale=1000)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.imshow(segment_mask1); ax1.set_xlabel(\"k=100\")\n",
    "ax2.imshow(segment_mask2); ax2.set_xlabel(\"k=1000\")\n",
    "fig.suptitle(\"Felsenszwalb's efficient graph based image segmentation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code ran two versions of Felzenszwalb's algorithms as shown in Fig. 8. The left k=100 generates a finer-grained segmentation with small regions where Manu's bald spot is identified. The right one k=1000 outputs a coarser-grained segmentation where regions tend to be larger.\n",
    "\n",
    "<img src=\"images/manu-2013-segmentation.png\" width=\"60%\">\n",
    "\n",
    "Felsenszwalb's efficient graph-based image segmentation is applied on the photo of Manu in 2013.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Search\n",
    "\n",
    "Selective search is a common algorithm to provide region proposals that potentially contain objects. It is built on top of the image segmentation output and use region-based characteristics (NOTE: not just attributes of a single pixel) to do a bottom-up hierarchical grouping.\n",
    "\n",
    "\n",
    "### How Selective Search Works\n",
    "\n",
    "1. At the initialization stage, apply Felzenszwalb and Huttenlocher's graph-based image segmentation algorithm to create regions to start with. \n",
    "2. Use a greedy algorithm to iteratively group regions together: \n",
    "    * First the similarities between all neighbouring regions are calculated.\n",
    "    * The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours. \n",
    "3. The process of grouping the most similar regions (Step 2) is repeated until the whole image becomes a single region. \n",
    "\n",
    "<img src=\"images/selective-search-algorithm.png\" width=\"60%\">\n",
    "\n",
    "The detailed algorithm of Selective Search.*\n",
    "\n",
    "\n",
    "### Configuration Variations\n",
    "\n",
    "Given two regions $(r_i, r_j)$, selective search proposed four complementary similarity measures:\n",
    "- **Color** similarity\n",
    "- **Texture**: Use algorithm that works well for material recognition such as [SIFT](http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf).\n",
    "- **Size**: Small regions are encouraged to merge early.\n",
    "- **Shape**: Ideally one region can fill the gap of the other. \n",
    "\n",
    "By (i) tuning the threshold $k$ in Felzenszwalb and Huttenlocher's algorithm, (ii) changing the color space and (iii) picking different combinations of similarity metrics, we can produce a diverse set of Selective Search strategies. The version that produces the region proposals with best quality is configured with (i) a mixture of various initial segmentation proposals, (ii) a blend of multiple color spaces and (iii) a combination of all similarity measures. Unsurprisingly we need to balance between the quality (the model complexity) and the speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics: mAP\n",
    "\n",
    "A common evaluation metric used in many object recognition and detection tasks is \"**mAP**\", short for \"**mean average precision**\". It is a number from 0 to 100; higher value is better.\n",
    "- Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The \"average precision\" (AP) is the area under the PR curve.\n",
    "- Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.\n",
    "- A detection is a true positive if it has **\"intersection over union\" (IoU)** with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is \"mAP@0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformable Parts Model\n",
    "\n",
    "The Deformable Parts Model (DPM) ([Felzenszwalb et al., 2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)) recognizes objects with a mixture graphical model (Markov random fields) of deformable parts. The model consists of three major components:\n",
    "1. A coarse ___root filter___ defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.\n",
    "2. Multiple ___part filters___ that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.\n",
    "3. A ___spatial model___ for scoring the locations of part filters relative to the root.\n",
    "\n",
    "<img src=\"images/DPM.png\" width=\"60%\">\n",
    "\n",
    "The DPM model contains (a) a root filter, (b) multiple part filters at twice the resolution, and (c) a model for scoring the location and deformation of parts.*\n",
    "\n",
    "\n",
    "The quality of detecting an object is measured by the score of filters minus the deformation costs. The matching score $f$, in laymen's terms, is:\n",
    "\n",
    "$$\n",
    "f(\\text{model}, x) = f(\\beta_\\text{root}, x) + \\sum_{\\beta_\\text{part} \\in \\text{part filters}} \\max_y [f(\\beta_\\text{part}, y) - \\text{cost}(\\beta_\\text{part}, x, y)]\n",
    "$$\n",
    "\n",
    "in which,\n",
    "- $x$ is an image with a specified position and scale;\n",
    "- $y$ is a sub region of $x$.\n",
    "- $\\beta_\\text{root}$ is the root filter.\n",
    "- $\\beta_\\text{part}$ is one part filter.\n",
    "- cost() measures the penalty of the part deviating from its ideal location relative to the root.\n",
    "\n",
    "\n",
    "The basic score model is the dot product between the filter $\\beta$ and the region feature vector $\\Phi(x)$: $f(\\beta, x) = \\beta \\cdot \\Phi(x)$. The feature set $\\Phi(x)$ can be defined by HOG or other similar algorithms.\n",
    "\n",
    "\n",
    "A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.\n",
    "\n",
    "<img src=\"images/DPM-matching.png\" width=\"60%\">\n",
    "\n",
    "The matching process by DPM. (Image source: [Felzenszwalb et al., 2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf))*\n",
    "\n",
    "\n",
    "The author later claimed that DPM and CNN models are not two distinct approaches to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. (Check the details in [Girshick et al., 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfeat\n",
    "\n",
    "Overfeat [[paper](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)][[code](https://github.com/sermanet/OverFeat)] is a pioneer model of integrating the object detection, localization and classification tasks all into one convolutional neural network. The main idea is to (i) do image classification at different locations on regions of multiple scales of the image in a sliding window fashion, and (ii) predict the bounding box locations with a regressor trained on top of the same convolution layers.\n",
    "\n",
    "The Overfeat model architecture is very similar to [AlexNet](#alexnet-krizhevsky-et-al-2012). It is trained as follows:\n",
    "\n",
    "<img src=\"images/overfeat-training.png\" width=\"60%\">\n",
    "\n",
    "The training stages of the Overfeat model. (Image source: [link](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf))*\n",
    "\n",
    "\n",
    "1. Train a CNN model (similar to AlexNet) on the image classification task.\n",
    "2. Then, we replace the top classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. The regressor is class-specific, each generated for one image class.\n",
    "\t- Input: Images with classification and bounding box.\n",
    "\t- Output: $(x_\\text{left}, x_\\text{right}, y_\\text{top}, y_\\text{bottom})$, 4 values in total, representing the coordinates of the bounding box edges.\n",
    "\t- Loss: The regressor is trained to minimize $l2$ norm between generated bounding box and the ground truth for each training example.\n",
    "\n",
    "\n",
    "At the detection time,\n",
    "1. Perform classification at each location using the pretrained CNN model.\n",
    "2. Predict object bounding boxes on all classified regions generated by the classifier.\n",
    "3. Merge bounding boxes with sufficient overlap from localization and sufficient confidence of being the same object from the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Dalal, Navneet, and Bill Triggs. [\"Histograms of oriented gradients for human detection.\"](https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf) Computer Vision and Pattern Recognition (CVPR), 2005.\n",
    "\n",
    "[2] Pedro F. Felzenszwalb, and Daniel P. Huttenlocher. [\"Efficient graph-based image segmentation.\"](http://cvcl.mit.edu/SUNSeminar/Felzenszwalb_IJCV04.pdf) Intl. journal of computer vision 59.2 (2004): 167-181.\n",
    "\n",
    "[3] [Histogram of Oriented Gradients by Satya Mallick](https://www.learnopencv.com/histogram-of-oriented-gradients/)\n",
    "\n",
    "[4] [Gradient Vectors by Chris McCormick](http://mccormickml.com/2013/05/07/gradient-vectors/)\n",
    "\n",
    "[5] [HOG Person Detector Tutorial by Chris McCormick](http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/)\n",
    "\n",
    "[1] Vincent Dumoulin and Francesco Visin. [\"A guide to convolution arithmetic for deep learning.\"](https://arxiv.org/pdf/1603.07285.pdf) arXiv preprint arXiv:1603.07285 (2016).\n",
    "\n",
    "[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. [\"On the Origin of Deep Learning.\"](https://arxiv.org/pdf/1702.07800.pdf) arXiv preprint arXiv:1702.07800 (2017).\n",
    "\n",
    "[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. [\"Object detection with discriminatively trained part-based models.\"](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf) IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010): 1627-1645.\n",
    "\n",
    "[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik. [\"Deformable part models are convolutional neural networks.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf\n",
    ") In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446. 2015.\n",
    "\n",
    "[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. [\"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\"](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf) arXiv preprint arXiv:1312.6229 (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN\n",
    "\n",
    "R-CNN ([Girshick et al., 2014](https://arxiv.org/abs/1311.2524)) is short for \"Region-based Convolutional Neural Networks\". The main idea is composed of two steps. First, using [selective search]({{ site.baseurl }}{% post_url 2017-10-29-object-recognition-for-dummies-part-1 %}#selective-search), it identifies a manageable number of bounding-box object region candidates (\"region of interest\" or \"RoI\"). And then it extracts CNN features from each region independently for classification.\n",
    "\n",
    "<img src=\"images/RCNN.png\" width=\"60%\">\n",
    "\n",
    "The architecture of R-CNN. (Image source: [Girshick et al., 2014](https://arxiv.org/abs/1311.2524))*\n",
    "\n",
    "\n",
    "### Model Workflow\n",
    "\n",
    "How R-CNN works can be summarized as follows:\n",
    "\n",
    "1. **Pre-train** a CNN network on image classification tasks; for example, VGG or ResNet trained on [ImageNet](http://image-net.org/index) dataset. The classification task involves N classes. \n",
    "<br />\n",
    "> NOTE: You can find a pre-trained [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet) in Caffe Model [Zoo](https://github.com/caffe2/caffe2/wiki/Model-Zoo). I don’t think you can [find it](https://github.com/tensorflow/models/issues/1394) in Tensorflow, but Tensorflow-slim model [library](https://github.com/tensorflow/models/tree/master/research/slim) provides pre-trained ResNet, VGG, and others.\n",
    "2. Propose category-independent regions of interest by selective search (~2k candidates per image). Those regions may contain target objects and they are of different sizes.\n",
    "3. Region candidates are **warped** to have a fixed size as required by CNN.\n",
    "4. Continue fine-tuning the CNN on warped proposal regions for K + 1 classes; The additional one class refers to the background (no object of interest). In the fine-tuning stage, we should use a much smaller learning rate and the mini-batch oversamples the positive cases because most proposed regions are just background.\n",
    "5. Given every image region, one forward propagation through the CNN generates a feature vector. This feature vector is then consumed by a **binary SVM** trained for **each class** independently. \n",
    "<br />\n",
    "The positive samples are proposed regions with IoU (intersection over union) overlap threshold >= 0.3, and negative samples are irrelevant others.\n",
    "6. To reduce the localization errors, a regression model is trained to correct the predicted detection window on bounding box correction offset using CNN features.\n",
    "\n",
    "\n",
    "### Bounding Box Regression\n",
    "\n",
    "Given a predicted bounding box coordinate $$\\mathbf{p} = (p_x, p_y, p_w, p_h)$$ (center coordinate, width, height) and its corresponding ground truth box coordinates $$\\mathbf{g} = (g_x, g_y, g_w, g_h)$$ , the regressor is configured to learn scale-invariant transformation between two centers and log-scale transformation between widths and heights. All the transformation functions take $$\\mathbf{p}$$ as input.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{g}_x &= p_w d_x(\\mathbf{p}) + p_x \\\\\n",
    "\\hat{g}_y &= p_h d_y(\\mathbf{p}) + p_y \\\\\n",
    "\\hat{g}_w &= p_w \\exp({d_w(\\mathbf{p})}) \\\\\n",
    "\\hat{g}_h &= p_h \\exp({d_h(\\mathbf{p})})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"images/RCNN-bbox-regression.png\" width=\"60%\">\n",
    "\n",
    "Illustration of transformation between predicted and ground truth bounding boxes.*\n",
    "\n",
    "An obvious benefit of applying such transformation is that all the bounding box correction functions, $$d_i(\\mathbf{p})$$ where $$i \\in \\{ x, y, w, h \\}$$, can take any value between [-∞, +∞]. The targets for them to learn are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t_x &= (g_x - p_x) / p_w \\\\\n",
    "t_y &= (g_y - p_y) / p_h \\\\\n",
    "t_w &= \\log(g_w/p_w) \\\\\n",
    "t_h &= \\log(g_h/p_h)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "A standard regression model can solve the problem by minimizing the SSE loss with regularization: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{reg} = \\sum_{i \\in \\{x, y, w, h\\}} (t_i - d_i(\\mathbf{p}))^2 + \\lambda \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "The regularization term is critical here and RCNN paper picked the best λ by cross validation. It is also noteworthy that not all the predicted bounding boxes have corresponding ground truth boxes. For example, if there is no overlap, it does not make sense to run bbox regression. Here, only a predicted box with a nearby ground truth box with at least 0.6 IoU is kept for training the bbox regression model.\n",
    "\n",
    "### Common Tricks\n",
    "\n",
    "Several tricks are commonly used in RCNN and other detection models.\n",
    "\n",
    "**Non-Maximum Suppression**\n",
    "\n",
    "Likely the model is able to find multiple bounding boxes for the same object. Non-max suppression helps avoid repeated detection of the same instance. After we get a set of matched bounding boxes for the same object category:\n",
    "Sort all the bounding boxes by confidence score.\n",
    "Discard boxes with low confidence scores.\n",
    "*While* there is any remaining bounding box, repeat the following:\n",
    "Greedily select the one with the highest score.\n",
    "Skip the remaining boxes with high IoU (i.e. > 0.5) with previously selected one.\n",
    "\n",
    "<img src=\"images/non-max-suppression.png\" width=\"60%\">\n",
    "\n",
    "Multiple bounding boxes detect the car in the image. After non-maximum suppression, only the best remains and the rest are ignored as they have large overlaps with the selected one. (Image source: [DPM paper](http://lear.inrialpes.fr/~oneata/reading_group/dpm.pdf))*\n",
    "\n",
    "\n",
    "**Hard Negative Mining**\n",
    "\n",
    "We consider bounding boxes without objects as negative examples. Not all the negative examples are equally hard to be identified. For example, if it holds pure empty background, it is likely an “*easy negative*”; but if the box contains weird noisy texture or partial object, it could be hard to be recognized and these are “*hard negative*”. \n",
    "\n",
    "The hard negative examples are easily misclassified. We can explicitly find those false positive samples during the training loops and include them in the training data so as to improve the classifier.\n",
    "\n",
    "\n",
    "### Speed Bottleneck\n",
    "\n",
    "Looking through the R-CNN learning steps, you could easily find out that training an R-CNN model is expensive and slow, as the following steps involve a lot of work:\n",
    "- Running selective search to propose 2000 region candidates for every image;\n",
    "- Generating the CNN feature vector for every image region (N images * 2000).\n",
    "- The whole process involves three models separately without much shared computation: the convolutional neural network for image classification and feature extraction; the top SVM classifier for identifying target objects; and the regression model for tightening region bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast R-CNN\n",
    "\n",
    "To make R-CNN faster, Girshick ([2015](https://arxiv.org/pdf/1504.08083.pdf)) improved the training procedure by unifying three independent models into one jointly trained framework and increasing shared computation results, named **Fast R-CNN**. Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier and the bounding-box regressor. In conclusion, computation sharing speeds up R-CNN.\n",
    "\n",
    "<img src=\"images/fast-RCNN.png\" width=\"60%\">\n",
    "\n",
    "The architecture of Fast R-CNN. (Image source: [Girshick, 2015](https://arxiv.org/pdf/1504.08083.pdf))*\n",
    "\n",
    "\n",
    "### RoI Pooling\n",
    "\n",
    "It is a type of max pooling to convert features in the projected region of the image of any size, h x w, into a small fixed window, H x W. The input region is divided into H x W grids, approximately every subwindow of size h/H x w/W. Then apply max-pooling in each grid.\n",
    "\n",
    "<img src=\"images/roi-pooling.png\" width=\"60%\">\n",
    "\n",
    "RoI pooling (Image source: [Stanford CS231n slides](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf).)*\n",
    "\n",
    "\n",
    "### Model Workflow\n",
    "\n",
    "How Fast R-CNN works is summarized as follows; many steps are same as in R-CNN: \n",
    "1. First, pre-train a convolutional neural network on image classification tasks.\n",
    "2. Propose regions by selective search (~2k candidates per image).\n",
    "3. Alter the pre-trained CNN:\n",
    "\t- Replace the last max pooling layer of the pre-trained CNN with a [RoI pooling](#roi-pooling) layer. The RoI pooling layer outputs fixed-length feature vectors of region proposals. Sharing the CNN computation makes a lot of sense, as many region proposals of the same images are highly overlapped.\n",
    "\t- Replace the last fully connected layer and the last softmax layer (K classes) with a fully connected layer and softmax over K + 1 classes.\n",
    "4. Finally the model branches into two output layers:\n",
    "\t- A softmax estimator of K + 1 classes (same as in R-CNN, +1 is the \"background\" class), outputting a discrete probability distribution per RoI.\n",
    "\t- A bounding-box regression model which predicts offsets relative to the original RoI for each of K classes.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The model is optimized for a loss combining two tasks (classification + localization):\n",
    "\n",
    "| **Symbol** | **Explanation** |\n",
    "| $$u$$ | True class label, $$ u \\in 0, 1, \\dots, K$$; by convention, the catch-all background class has $$u = 0$$. |\n",
    "| $$p$$ | Discrete probability distribution (per RoI) over K + 1 classes: $$p = (p_0, \\dots, p_K)$$, computed by a softmax over the K + 1 outputs of a fully connected layer. |\n",
    "| $$v$$ | True bounding box $$ v = (v_x, v_y, v_w, v_h) $$. |\n",
    "| $$t^u$$ | Predicted bounding box correction, $$t^u = (t^u_x, t^u_y, t^u_w, t^u_h)$$. See [above](#bounding-box-regression). |\n",
    "{:.info}\n",
    "\n",
    "\n",
    "The loss function sums up the cost of classification and bounding box prediction: $$\\mathcal{L} = \\mathcal{L}_\\text{cls} + \\mathcal{L}_\\text{box}$$. For \"background\" RoI, $$\\mathcal{L}_\\text{box}$$ is ignored by the indicator function $$\\mathbb{1} [u \\geq 1]$$, defined as:\n",
    "\n",
    "$$\n",
    "\\mathbb{1} [u >= 1] = \\begin{cases}\n",
    "    1  & \\text{if } u \\geq 1\\\\\n",
    "    0  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The overall loss function is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(p, u, t^u, v) &= \\mathcal{L}_\\text{cls} (p, u) + \\mathbb{1} [u \\geq 1] \\mathcal{L}_\\text{box}(t^u, v) \\\\\n",
    "\\mathcal{L}_\\text{cls}(p, u) &= -\\log p_u \\\\\n",
    "\\mathcal{L}_\\text{box}(t^u, v) &= \\sum_{i \\in \\{x, y, w, h\\}} L_1^\\text{smooth} (t^u_i - v_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The bounding box loss $$\\mathcal{L}_{box}$$ should measure the difference between $$t^u_i$$ and $$v_i$$ using a **robust** loss function. The [smooth L1 loss](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf) is adopted here and it is claimed to be less sensitive to outliers.\n",
    "\n",
    "$$\n",
    "L_1^\\text{smooth}(x) = \\begin{cases}\n",
    "    0.5 x^2             & \\text{if } \\vert x \\vert < 1\\\\\n",
    "    \\vert x \\vert - 0.5 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img src=\"images/l1-smooth.png\" width=\"60%\">\n",
    "\n",
    "The plot of smooth L1 loss, $$y = L_1^\\text{smooth}(x)$$. (Image source: [link](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf))*\n",
    "\n",
    "\n",
    "\n",
    "### Speed Bottleneck\n",
    "\n",
    "Fast R-CNN is much faster in both training and testing time. However, the improvement is not dramatic because the region proposals are generated separately by another model and that is very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN\n",
    "\n",
    "An intuitive speedup solution is to integrate the region proposal algorithm into the CNN model. **Faster R-CNN** ([Ren et al., 2016](https://arxiv.org/pdf/1506.01497.pdf)) is doing exactly this: construct a single, unified model composed of RPN (region proposal network) and fast R-CNN with shared convolutional feature layers.\n",
    "\n",
    "\n",
    "<img src=\"images/faster-RCNN.png\" width=\"60%\">\n",
    "\n",
    "An illustration of Faster R-CNN model. (Image source: [Ren et al., 2016](https://arxiv.org/pdf/1506.01497.pdf))*\n",
    "\n",
    "\n",
    "### Model Workflow\n",
    "\n",
    "1. Pre-train a CNN network on image classification tasks.\n",
    "2. Fine-tune the RPN (region proposal network) end-to-end for the region proposal task, which is initialized by the pre-train image classifier. Positive samples have IoU (intersection-over-union) > 0.7, while negative samples have IoU < 0.3.\n",
    "\t- Slide a small n x n spatial window over the conv feature map of the entire image.\n",
    "\t- At the center of each sliding window, we predict multiple regions of various scales and ratios simultaneously. An anchor is a combination of (sliding window center, scale, ratio). For example, 3 scales + 3 ratios => k=9 anchors at each sliding position.\n",
    "3. Train a Fast R-CNN object detection model using the proposals generated by the current RPN\n",
    "4. Then use the Fast R-CNN network to initialize RPN training. While keeping the shared convolutional layers, only fine-tune the RPN-specific layers. At this stage, RPN and the detection network have shared convolutional layers!\n",
    "5. Finally fine-tune the unique layers of Fast R-CNN\n",
    "6. Step 4-5 can be repeated to train RPN and Fast R-CNN alternatively if needed.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Faster R-CNN is optimized for a multi-task loss function, similar to fast R-CNN.\n",
    "\n",
    "| **Symbol**  | **Explanation** |\n",
    "| $$p_i$$     | Predicted probability of anchor i being an object. |\n",
    "| $$p^*_i$$   | Ground truth label (binary) of whether anchor i is an object. |\n",
    "| $$t_i$$     | Predicted four parameterized coordinates. |\n",
    "| $$t^*_i$$   | Ground truth coordinates. |\n",
    "| $$N_\\text{cls}$$ | Normalization term, set to be mini-batch size (~256) in the paper. |\n",
    "| $$N_\\text{box}$$ | Normalization term, set to the number of anchor locations (~2400) in the paper. |\n",
    "| $$\\lambda$$ | A balancing parameter, set to be ~10 in the paper (so that both $$\\mathcal{L}_\\text{cls}$$ and $$\\mathcal{L}_\\text{box}$$ terms are roughly equally weighted). |\n",
    "{:.info}\n",
    "\n",
    "The multi-task loss function combines the losses of classification and bounding box regression:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\mathcal{L}_\\text{cls} + \\mathcal{L}_\\text{box} \\\\\n",
    "\\mathcal{L}(\\{p_i\\}, \\{t_i\\}) &= \\frac{1}{N_\\text{cls}} \\sum_i \\mathcal{L}_\\text{cls} (p_i, p^*_i) + \\frac{\\lambda}{N_\\text{box}} \\sum_i p^*_i \\cdot L_1^\\text{smooth}(t_i - t^*_i) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $$\\mathcal{L}_\\text{cls}$$ is the log loss function over two classes, as we can easily translate a multi-class classification into a binary classification by predicting a sample being a target object versus not. $$L_1^\\text{smooth}$$ is the smooth L1 loss.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{cls} (p_i, p^*_i) = - p^*_i \\log p_i - (1 - p^*_i) \\log (1 - p_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN\n",
    "\n",
    "Mask R-CNN ([He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf)) extends Faster R-CNN to pixel-level [image segmentation]({{ site.baseurl }}{% post_url 2017-10-29-object-recognition-for-dummies-part-1 %}#image-segmentation-felzenszwalbs-algorithm). The key point is to decouple the classification and the pixel-level mask prediction tasks. Based on the framework of [Faster R-CNN](#faster-r-cnn), it added a third branch for predicting an object mask in parallel with the existing branches for classification and localization. The mask branch is a small fully-connected network applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner.\n",
    "\n",
    "\n",
    "<img src=\"images/mask-rcnn.png\" width=\"60%\">\n",
    "\n",
    "Mask R-CNN is Faster R-CNN model with image segmentation. (Image source: [He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf))*\n",
    "\n",
    "Because pixel-level segmentation requires much more fine-grained alignment than bounding boxes, mask R-CNN improves the RoI pooling layer (named \"RoIAlign layer\") so that RoI can be better and more precisely mapped to the regions of the original image.\n",
    "\n",
    "\n",
    "<img src=\"images/mask-rcnn-examples.png\" width=\"60%\">\n",
    "\n",
    "Predictions by Mask R-CNN on COCO test set. (Image source: [He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf))*\n",
    "\n",
    "\n",
    "### RoIAlign\n",
    "\n",
    "The RoIAlign layer is designed to fix the location misalignment caused by quantization in the RoI pooling. RoIAlign removes the hash quantization, for example, by using x/16 instead of [x/16], so that the extracted features can be properly aligned with the input pixels. [Bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) is used for computing the floating-point location values in the input.\n",
    "\n",
    "\n",
    "<img src=\"images/roi-align.png\" width=\"60%\">\n",
    "\n",
    "A region of interest is mapped **accurately** from the original image onto the feature map without rounding up to integers. (Image source: [link](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4))*\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The multi-task loss function of Mask R-CNN combines the loss of classification, localization and segmentation mask: $$ \\mathcal{L} = \\mathcal{L}_\\text{cls} + \\mathcal{L}_\\text{box} + \\mathcal{L}_\\text{mask}$$, where $$\\mathcal{L}_\\text{cls}$$ and $$\\mathcal{L}_\\text{box}$$ are same as in Faster R-CNN.\n",
    "\n",
    "\n",
    "The mask branch generates a mask of dimension m x m for each RoI and each class; K classes in total. Thus, the total output is of size $$K \\cdot m^2$$. Because the model is trying to learn a mask for each class, there is no competition among classes for generating masks.\n",
    "\n",
    "$$\\mathcal{L}_\\text{mask}$$ is defined as the average binary cross-entropy loss, only including k-th mask if the region is associated with the ground truth class k.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{mask} = - \\frac{1}{m^2} \\sum_{1 \\leq i, j \\leq m} \\big[ y_{ij} \\log \\hat{y}^k_{ij} + (1-y_{ij}) \\log (1- \\hat{y}^k_{ij}) \\big]\n",
    "$$\n",
    "\n",
    "where $$y_{ij}$$ is the label of a cell (i, j) in the true mask for the region of size m x m; $$\\hat{y}_{ij}^k$$ is the predicted value of the same cell in the mask learned for the ground-truth class k.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Models in the R-CNN family\n",
    "\n",
    "Here I illustrate model designs of R-CNN, Fast R-CNN, Faster R-CNN and Mask R-CNN. You can track how one model evolves to the next version by comparing the small differences.\n",
    "\n",
    "<img src=\"images/rcnn-family-summary.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
