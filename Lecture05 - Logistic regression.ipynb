{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now talk about the classification problem. This is just like the regression\n",
    "problem, except that the values $y$ we now want to predict take on only\n",
    "a small number of discrete values. For now, we will focus on the binary\n",
    "classification problem in which y can take on only two values, $0$ and $1$.\n",
    "(Most of what we say here will also generalize to the multiple-class case.)\n",
    "For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, and y may be $1$ if it is a piece\n",
    "of spam mail, and 0 otherwise. $0$ is also called the negative class, and $1$ the positive class, and they are sometimes also denoted by the symbols “$-$” and “$+$.” Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example.\n",
    "\n",
    "\n",
    "We could approach the classification problem ignoring the fact that $y$ is\n",
    "discrete-valued, and use our old linear regression algorithm to try to predict\n",
    "$y$ given $x$. However, it is easy to construct examples where this method\n",
    "performs very poorly. Intuitively, it also doesn’t make sense for $h_{\\theta}(x)$ to take\n",
    "values larger than $1$ or smaller than $0$ when we know that $y\\in{0, 1}$.\n",
    "To fix this, let’s change the form for our hypotheses $h_{\\theta}(x)$. We will choose\n",
    "\n",
    "$$h_{\\theta} = g(\\theta^Tx) = \\frac{1}{1+\\exp{(-\\theta^Tx)}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$g(z) = \\frac{1}{1+\\exp{(-z)}}$$\n",
    "\n",
    "and $z = \\theta^Tx$. $g$ is called the logistic function or the sigmoid function. Here is a plot\n",
    "showing $g(z)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "y_sigmoid = sigmoid(x)\n",
    "\n",
    "plt.plot(x.numpy(), y_sigmoid.numpy(), c='red', label='sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $g(z)$ tends towards $1$ as $z\\rightarrow \\infty$, and $g(z)$ tends towards $0$ as\n",
    "$z\\rightarrow -\\infty$. Moreover, $g(z)$, and hence also $h(x)$, is always bounded between\n",
    "$0$ and $1$. As before, we are keeping the convention of letting $x_0 = 1$, so that we can write $\\theta_0 + \\sum_{i=1}^{m}\\theta_ix_i = \\theta^Tx$.  Before moving on, here’s a useful property of the derivative of the sigmoid function, which we write as $g'$:\n",
    "\n",
    "\\begin{align*}\n",
    "g'(z) &= \\frac{d}{dz}\\frac{1}{1+\\exp{(-z)}}\\\\\n",
    "&= \\frac{1}{(1+\\exp{(-z))^2}}\\exp(-z)\\\\\n",
    "&= \\frac{1}{1+\\exp(-z)} . \\big(1-\\frac{1}{1+\\exp(-z)}\\big)\\\\\n",
    "&= g(z).(1-g(z))\n",
    "\\end{align*}\n",
    "\n",
    "So, given the logistic regression model, how do we fit $\\theta$ for it?  Following\n",
    "how we saw least squares regression could be derived as the maximum likelihood\n",
    "estimator under a set of assumptions, let’s endow our classification\n",
    "model with a set of probabilistic assumptions, and then fit the parameters\n",
    "via maximum likelihood. Let us assume\n",
    "\n",
    "\\begin{align*}\n",
    "P(y=1|x;\\theta) &= h_{\\theta}(x)\\\\\n",
    "P(y=0|x;\\theta) &= 1 - h_{\\theta}(x)\n",
    "\\end{align*}\n",
    "\n",
    "Note that this can be written more compactly as\n",
    "\n",
    "$$P(y|x;\\theta) = (h_{\\theta}(x))^y \\;(1 - h_{\\theta}(x))^{1-y}$$\n",
    "\n",
    "Assuming that the m training examples were generated independently, we\n",
    "can then write down the likelihood of the parameters as\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta) &= p(\\hat{y}|X;\\theta)\\\\\n",
    "&= \\prod_{i=1}^m p(y^{(i)}|x^{(i)};\\theta)\\\\\n",
    "&= \\prod_{i=1}^m (h_{\\theta}(x^{(i)}))^{y^{(i)}} \\;(1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}}\n",
    "\\end{align*}\n",
    "\n",
    "As before, it will be easier to maximize the log likelihood:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) &= log\\;L(\\theta)\\\\\n",
    "&= \\sum_{i=1}^m {y^{(i)}}log\\;(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\;log\\;(1 - h_{\\theta}(x^{(i)}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we maximize the likelihood? Similar to our derivation in the case\n",
    "of linear regression, we can use gradient ascent. Written in vectorial notation,\n",
    "our updates will therefore be given by $\\theta = \\theta + \\alpha\\nabla_{\\theta}\\ell(\\theta)$. (Note the positive\n",
    "rather than negative sign in the update formula, since we’re maximizing,\n",
    "rather than minimizing, a function now.) Let’s start by working with just\n",
    "one training example $(x, y)$, and take derivatives to derive the stochastic\n",
    "gradient ascent rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\theta_j}\\ell(\\theta) &= \\frac{\\partial}{\\partial\\theta} \\big[{y}\\;log\\;(h_{\\theta}(x)) + (1-y)\\;log\\;(1 - h_{\\theta}(x))\\big]\\\\\n",
    "&= \\big[ {y}\\frac{1}{g(\\theta^Tx)} - (1-y)\\frac{1}{(1 - g(\\theta^Tx))} \\big]\\;\\frac{\\partial}{\\partial\\theta} g(\\theta^Tx)\\\\\n",
    "&= \\big[ {y}\\frac{1}{g(\\theta^Tx)} - (1-y)\\frac{1}{(1 - g(\\theta^Tx))} \\big]\\; (g(\\theta^Tx).(1-g(\\theta^Tx)))\\frac{\\partial}{\\partial\\theta}\\theta^T x \\\\\n",
    "&= \\big[ {y}(1 - g(\\theta^Tx)) - (1-y)g(\\theta^Tx) \\big]\\;x_j\\\\\n",
    "&= \\big[y - yg(\\theta^Tx) - g(\\theta^Tx) + yg(\\theta^Tx) \\big]\\;x_j\\\\\n",
    "&= \\big[y - h_{\\theta}(x)\\big]\\;x_j\n",
    "\\end{align*}\n",
    "\n",
    "Using the above equation, we derive the gradient descent algorithm for logistic regression:\n",
    "\n",
    "\\begin{align}\n",
    "repeat \\{\\\\\n",
    "&\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - h_{\\theta}(x^{(i)})).x^{(i)}_j\\\\\n",
    "\\}\n",
    "\\end{align}\n",
    "\n",
    "#### Regularized logistic regression\n",
    "\n",
    "\\begin{align*}\n",
    "min_{\\theta}\\;J(\\theta)= \\big[-\\frac{1}{m} \\sum_{i=1}^m {y^{(i)}}log\\;(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\;log\\;(1 - h_{\\theta}(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^m \\theta_j^2\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align}\n",
    "repeat \\{\\\\\n",
    "&\\theta_0 = \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - h_{\\theta}(x^{(i)})).x^{(i)}_0\\\\\n",
    "&\\theta_j = \\theta_j - \\alpha\\big[\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - h_{\\theta}(x^{(i)})).x^{(i)}_j-\\frac{\\lambda}{m}\\theta_j\\big]\\\\\n",
    "\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a posteriori (MAP)\n",
    "\n",
    "We use Maximum likelihood estimation as our cost function to find the optimized $w*$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "w^* = \\arg\\max_w (P(Y \\vert X, w)) & = \\arg\\max_w \\prod^n_{i=1} P( y_i \\vert x_i, w)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Alternatively, we can use Maximum a posteriori (MAP) to find the optimized $w*$. \n",
    "\n",
    "$$\n",
    "P(w \\vert x, y)\n",
    "$$\n",
    "\n",
    "In this section, we go through the process of how some cost functions are defined using MAP.\n",
    "\n",
    "Using Baye's theorem:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(w \\vert x, y) & = \\frac{p(y \\vert x, w) p(w \\vert x)}{p(y \\vert x)} \\\\\n",
    "& = \\frac{p(y \\vert x, w) p(w)}{p(y \\vert x)} \\quad & \\text{ w is not depend on x}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Take the negative log\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "-\\log p(w \\vert x, y) & = - \\log p(y \\vert x, w) - \\log p(w \\vert x) - \\log p(y \\vert x) \\\\\n",
    "& = -\\log p(y \\vert x, w) - \\log p(w \\vert x) - C^{'} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The total cost for all training data is\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(w) & = - \\sum^N_{i=1} \\log p(y_i \\vert x_i, w) - \\sum^d_{j=1} \\log p(w_j) - C \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Compute the prior with the assumption that it has a Gaussian distribution of $ \\mu=0, \\sigma^2 = \\frac{1}{\\lambda}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(w_j) & =  \\frac{1}{\\sqrt{2 \\pi \\frac{1}{\\lambda}}} e^{-\\frac{(w_j - 0)^{2}}{2\\frac{1}{\\lambda}} } \\\\\n",
    "\\log p(w_j) & = - \\log {\\sqrt{2 \\pi \\frac{1}{\\lambda}}} + \\log e^{- \\frac{\\lambda}{2}w_j^2}  \\\\\n",
    "& = C^{'} - \\frac{\\lambda}{2}w_j^2 \\\\\n",
    "- \\sum^d_{j=1} \\log p(w_j) &= C + \\frac{\\lambda}{2} \\| w \\|^2 \\quad \\text{ L-2 regularization}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "If the likelihood is also gaussian distributed:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y_i \\vert x_i, w) & \\propto e^{ - \\frac{(w^T x_i - y_i)^2}{2 \\sigma^2} } \\\\\n",
    "- \\sum^N_{i=1} \\log p(y_i \\vert x_i, w) & = \\frac{1}{2 \\sigma^2} \\| w^T x_i - y_i \\|^2 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "So for a Gaussian distribution prior and likelihood, the cost function is\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(w) & = - \\sum^N_{i=1} \\log p(y_i \\vert x_i, w) - \\sum^d_{j=1} \\log p(w_j) - C \\\\\n",
    "&=  \\frac{1}{2 \\sigma^2} \\| w^T x_i - y_i \\|^2 + \\frac{\\lambda}{2} \\| w \\|^2 + constant\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "which is the same as the MSE with L2-regularization.\n",
    "\n",
    "If the likeliness is computed from a logistic function, the corresponding cost function is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y_i \\vert x_i, w) & = \\frac{1}{ 1 + e^{- y_i w^T x_i} } \\\\\n",
    "J(w) & = - \\sum^N_{i=1} \\log p(y_i \\vert x_i, w) - \\sum^d_{j=1} \\log p(w_j) - C \\\\\n",
    "&= \\sum^N_{i=1} \\log(1 + e^{- y_i w^T x_i})  + \\frac{\\lambda}{2} \\| w \\|^2 + constant\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt('data/ex2data1.txt', delimiter=',')\n",
    "X = torch.from_numpy(data[:, :2]).float()\n",
    "y = torch.from_numpy(data[:, 2]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1105.2411\n",
      "Epoch [200/1000], Loss: 1105.2411\n",
      "Epoch [300/1000], Loss: 1105.2411\n",
      "Epoch [400/1000], Loss: 1105.2411\n",
      "Epoch [500/1000], Loss: 1105.2411\n",
      "Epoch [600/1000], Loss: 1105.2411\n",
      "Epoch [700/1000], Loss: 1105.2411\n",
      "Epoch [800/1000], Loss: 1105.2411\n",
      "Epoch [900/1000], Loss: 1105.2411\n",
      "Epoch [1000/1000], Loss: 1105.2411\n",
      "Final Weights: tensor([[0.2199, 0.2499]])\n",
      "Final Bias: tensor([-0.2935])\n",
      "tensor(60)\n"
     ]
    }
   ],
   "source": [
    "class LogisticReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticReg, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "# Model\n",
    "model = LogisticReg()\n",
    "\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred[:,0], y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 1000, loss.item()))\n",
    "print ('Final Weights: {}'.format(model.linear.weight.data))\n",
    "print ('Final Bias: {}'.format(model.linear.bias.data))\n",
    "\n",
    "\n",
    "out = model(X)\n",
    "\n",
    "print((out[:,0] == y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classifier\n",
    "\n",
    "For many classification problems, we categorize an input to one of the many classes. For example, we can classify an image to one of the 100 possible object classes. We use a softmax classifier to compute K probabilities, one per class for an input image and the combined probabilities remains 1.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src=\"images/deep_learner2.jpg\" style=\"border:none;width:70%;\">\n",
    "</div>\n",
    "\n",
    "The network computes K scores per image. The probability that an image belongs to the class $$ i $$ will be.\n",
    "$$\n",
    "p_i =  \\frac{e^{score_i}}{\\sum_{c \\in y} e^{score_c}} \n",
    "$$\n",
    "For example, the school bus above may have a score of (3.2, 0.8, 0) for the class school bus, truck and airplane respectively. The probability for the corresponding class is\n",
    "$$\n",
    "p_{\\text{bus}} =  \\frac{e^{3.2}}{ e^{3.2} + e^{0.8} + e^0} = 0.88\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{\\text{truck}} =  \\frac{e^{0.2}}{ e^{3.2} + e^{0.8} + e^0} = 0.08\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{\\text{airplane}} =  \\frac{e^0}{ e^{3.2} + e^{0.8} + e^0} = 0.04\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88379809 0.08017635 0.03602556]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "a = np.array([3.2, 0.8, 0])\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88379809 0.08017635 0.03602556]\n"
     ]
    }
   ],
   "source": [
    "smax = nn.Softmax(dim=0)\n",
    "x = torch.from_numpy(a)\n",
    "z = smax(x)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the numerical stability problem caused by adding large exponential values, we subtract the inputs by its maximum. Adding or subtract a number from the input does not change the probability value in softmax.\n",
    "$$\n",
    "softmax(z) = \\frac{e^{z_i -max}}{\\sum e^{z_c  - max}} =  \\frac{e^{-max} e^{z_i}}{e^{-max} \\sum e^{z_c}} = \\frac{e^{z_i}}{\\sum e^{z_c}}\n",
    "$$\n",
    "```python\n",
    "z -= np.max(z)\n",
    "```\n",
    "\n",
    "**logits** is defined as a mean to measure odd.\n",
    "$$\n",
    "logits = \\log(\\frac{p}{1-p})\n",
    "$$\n",
    "If we combine the softmax equation with the logits equation, it is easy to see that the score is the logit. \n",
    "$$\n",
    "p = softmax(score) = softmax(z_i) = \\frac{e^{z_i}}{\\sum e^{z_c}} \\\\\n",
    "logits = z_i = score\n",
    "$$\n",
    "That is why in many literatures and APIs, logit and score are interchangeable when a softmax classifier is used. However, other function's output can be a logit. Sigmoid function output is also a logit.\n",
    "\n",
    "> Softmax is the most common classifier among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.2917\n",
      "Epoch [1/5], Step [200/600], Loss: 2.2788\n",
      "Epoch [1/5], Step [300/600], Loss: 2.2822\n",
      "Epoch [1/5], Step [400/600], Loss: 2.2676\n",
      "Epoch [1/5], Step [500/600], Loss: 2.2678\n",
      "Epoch [1/5], Step [600/600], Loss: 2.2465\n",
      "Epoch [2/5], Step [100/600], Loss: 2.2409\n",
      "Epoch [2/5], Step [200/600], Loss: 2.2415\n",
      "Epoch [2/5], Step [300/600], Loss: 2.2334\n",
      "Epoch [2/5], Step [400/600], Loss: 2.2400\n",
      "Epoch [2/5], Step [500/600], Loss: 2.2196\n",
      "Epoch [2/5], Step [600/600], Loss: 2.2248\n",
      "Epoch [3/5], Step [100/600], Loss: 2.2165\n",
      "Epoch [3/5], Step [200/600], Loss: 2.2081\n",
      "Epoch [3/5], Step [300/600], Loss: 2.1977\n",
      "Epoch [3/5], Step [400/600], Loss: 2.1987\n",
      "Epoch [3/5], Step [500/600], Loss: 2.1888\n",
      "Epoch [3/5], Step [600/600], Loss: 2.1876\n",
      "Epoch [4/5], Step [100/600], Loss: 2.1793\n",
      "Epoch [4/5], Step [200/600], Loss: 2.1699\n",
      "Epoch [4/5], Step [300/600], Loss: 2.1919\n",
      "Epoch [4/5], Step [400/600], Loss: 2.1692\n",
      "Epoch [4/5], Step [500/600], Loss: 2.1681\n",
      "Epoch [4/5], Step [600/600], Loss: 2.1487\n",
      "Epoch [5/5], Step [100/600], Loss: 2.1583\n",
      "Epoch [5/5], Step [200/600], Loss: 2.1363\n",
      "Epoch [5/5], Step [300/600], Loss: 2.1547\n",
      "Epoch [5/5], Step [400/600], Loss: 2.1523\n",
      "Epoch [5/5], Step [500/600], Loss: 2.1409\n",
      "Epoch [5/5], Step [600/600], Loss: 2.1232\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 72 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
