{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-5, 5, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"sigmoid---nnsigmoid\">Sigmoid - <code>nn.Sigmoid()</code></h3>\n",
    "\n",
    "$${Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "\n",
    "<p>If we stack sigmoids in many layers, it may be inefficient for the system to learn and requires careful initialization. This is because if the input is very large or small, the gradient of the sigmoid function is close to 0. In this case, there is no gradient flowing back to update the parameters, known as saturating gradient problem. Therefore, for deep neural networks, a single kink function (such as ReLU) is preferred.</p>\n",
    "\n",
    "This is so called Sigmoid function and it is shown in the image below.  Sigmoid function takes a real-valued number and “squashes” it into range between 0 and 1, i.e., $(\\sigma (x)\\in (0,1))$. In particular, large negative numbers become 0 and large positive numbers become 1. Moreover, the sigmoid function has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). But it has two major drawbacks: 1. Sigmoids saturate and kill gradients: The Sigmoid neuron has a property that when the neuron’s activation saturates at either tail of 0 or 1, the gradient (where $(\\sigma'(x)=\\sigma (x)\\centerdot (1-\\sigma (x)))$, see the red dotted line above) at these regions is almost zero. During backpropagation, this gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. So, it is critically important to initialize the weights of sigmoid neurons to prevent saturation. For instance, if the initial weights are too large then most neurons would become saturated and the network will barely learn$(.^{[2]})$ 2. Sigmoid outputs are not zero-centered: It is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $(x>0)$ elementwise in $(f(x)=w^{T}x+b))$, then the gradient on the weights $(w)$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $(f)$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem$(.^{[2]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.Sigmoid()\n",
    "out = activation(x)\n",
    "\n",
    "plt.plot(x.numpy(), out.numpy())\n",
    "plt.title('Sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"tanh---nntanh\">Tanh - <code>nn.Tanh()</code></h3>\n",
    "\n",
    "$${Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}$$\n",
    "\n",
    "<p>Tanh is basically identical to Sigmoid except it is centred, ranging from -1 to 1. The output of the function will have roughly zero mean. Therefore, the model will converge faster.  Note that convergence is usually faster if the average of each input variable is close to zero. One example is Batch Normalization.</p>\n",
    "\n",
    "See the following image, tanh function is similar to Sigmoid function (Mathematically, $(tanh(x)=2\\sigma (2x)-1)$), which is also sigmoidal (“S”-shaped). It squashes real-valued number to the range between -1 and 1, i.e., $(tanh(x)\\in (-1, 1))$.  Like the Sigmoid units, its activations saturate, but its output is zero-centered (means tanh solves the second drawback of Sigmoid). Therefore, in practice the tanh units is always preferred to the sigmoid units. The derivative of tanh function is defined as $tanh'(x)=1-tanh^{2}(x)$ See the red dotted line in the above image, it interprets that tanh also saturate and kill gradient, since tanh’s derivative has similar shape as compare to Sigmoid’s derivative. What’s more, tanh has stronger gradients, since data is centered around 0, the derivatives are higher, and tanh avoids bias in the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.Tanh()\n",
    "out = activation(x)\n",
    "\n",
    "plt.plot(x.numpy(), out.numpy())\n",
    "plt.title('Tanh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"relu---nnrelu\">Rectified Linear Units - <code>nn.ReLU()</code></h3>\n",
    "\n",
    "$${ReLU}(x) = (x)^{+} = \\max(0,x)$$\n",
    "\n",
    "Where $x$ is the input to a neuron. In other words, the activation is simply thresholded at zero. The range of ReLU is betweem 0 to $(\\infty)$. See the image below (red dotted line is the derivative)  The ReLU function is more effectively than the widely used logistic sigmoid and its more practical counterpart, the hyperbolic tangent, since it efficaciously reduce the computation cost as well as some other merits: 1. It was found to greatly accelerate (Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form $(.^{[2]})$ 2. Compared to tanh/sigmoid neurons that involve expensive oeprations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero $(.^{[2]})$\n",
    "\n",
    "Unfortunately, ReLU also suffers several drawbacks, for instance, - ReLU units can be fragile during training and can “die”.\n",
    "\n",
    "For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. You may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue $(.^{[2]})$ Plus, here is a smooth approximation to the rectifier, which is called the softplus function (see the green line in the above image). It is defined as\n",
    "$$\n",
    "f(x)=\\ln (1+e^{x})\n",
    "$$\n",
    "And its derivative is\n",
    "$$\n",
    "f'(x)=\\frac{e^{x}}{1+e^{x}}=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "Interestingly, the derivative of Softplus is the logistic function. We can see that both the ReLU and Softplus are largely similar, except near 0 where the softplus is enticingly smooth and differentiable. But it is much easier and efficient to compute ReLU and its derivative than for the softplus function which has $(log(\\centerdot))$ and $(exp(\\centerdot))$ in its formulation. In deep learning, computing the activation function and its derivative is as frequent as addition and subtraction in arithmetic. By using ReLU, the forward and backward passes are much faster while retaining the non-linear nature of the activation function required for deep neural networks to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.ReLU()\n",
    "out = activation(x)\n",
    "\n",
    "plt.plot(x.numpy(), out.numpy())\n",
    "plt.title('ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"leakyrelu---nnleakyrelu\">LeakyReLU - <code>nn.LeakyReLU()</code></h3>\n",
    "\n",
    "$${LeakyReLU}(x) = \\begin{cases}\n",
    "      x, & \\text{if} x \\geq 0\\\\\n",
    "      a_\\text{negative slope}x, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "<p>Here $a$ is a fixed parameter. The bottom part of the equation prevents the problem of dying ReLU which refers to the problem when ReLU neurons become inactive and only output 0 for any input. Therefore, its gradient is 0. By using a negative slope, it allows the network to propagate back and learn something useful.</p>\n",
    "\n",
    "<p>LeakyReLU is necessary for skinny network, which is almost impossible to get gradients flowing back with vanilla ReLU. With LeakyReLU, the network can still have gradients even we are in the region where everything is zero out.</p>\n",
    "\n",
    "\n",
    "<h3 id=\"rrelu---nnrrelu\">RReLU - <code>nn.RReLU()</code></h3>\n",
    "\n",
    "<p>There are variations in ReLU. The Random ReLU (RReLU) is defined as follows.</p>\n",
    "\n",
    "$${RReLU}(x) = \\begin{cases}\n",
    "      x, & \\text{if} x \\geq 0\\\\\n",
    "      ax, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "Randomized ReLU (RReLU) is a randomized version of Leaky ReLU, where the $(\\alpha)$ is a random number. In RReLU, the slopes of negative parts are randomized in a given range in the training, and then fixed in the testing. It is reported that RReLU could reduce overfitting due to its randomized nature in the Kaggle National Data Science Bowl (NDSB) competition. \n",
    "\n",
    "Here gives the comparing graph of different ReLUs  For Parametric ReLU, $(\\alpha_{i})$ is learned and for Leaky ReLU $(\\alpha_{i})$ is fixed. For RReLU, $(\\alpha_{ji})$ is a random variable keeps sampling in a given range, and remains fixed in testing. Generally, we summarize the advantages and potential problems of ReLUs: - (+) Biological plausibility: One-sided, compared to the antisymmetry of tanh. - (+) Sparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (having a non-zero output). - (+) Efficient gradient propagation: No vanishing or exploding gradient problems. - (+) Efficient computation: Only comparison, addition and multiplication. - (+) Scale-invariant: $(max(0,\\alpha x)=\\alpha\\centerdot max(0,x))$. - (-) Non-differentiable at zero: however it is differentiable anywhere else, including points arbitrarily close to (but not equal to) zero. - (-) Non-zero centered. - (-) Unbounded: Could potentially blow up. - (-) Dying Relu problem: Relu neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state and “dies.” In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high.\n",
    "\n",
    "<h3 id=\"prelu---nnprelu\">PReLU - <code>nn.PReLU()</code></h3>\n",
    "\n",
    "$${PReLU}(x) = \\begin{cases}\n",
    "      x, & \\text{if} x \\geq 0\\\\\n",
    "      ax, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "<p>Here $a$ is a learnable parameter.</p>### Tanh Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.LeakyReLU()\n",
    "out = activation(x)\n",
    "\n",
    "plt.plot(x.numpy(), out.numpy())\n",
    "plt.title('Leaky ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxout\n",
    "Some other types of units that do not have the functional form $(f(w^{T}x+b))$ where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron that generalizes the ReLU and its leaky version. The Maxout neuron computes the function\n",
    "\n",
    "$$\n",
    "max(w^{T}_{1}+b_{1},w^{T}_{2}+b_{2})\n",
    "$$\n",
    "\n",
    "Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $(w_{1},b_{1}=0)$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.\n",
    "\n",
    "### Softmax\n",
    "The Softmax function (Used for multi-classification neural network output), or normalized exponential function, in mathematics, is a generalization of the logistic function that “squashes” a $(K)$-dimensional vector $(\\mathbf{z})$ from arbitrary real values to a $(K)$-dimensional vector $(\\sigma (\\mathbf{z}))$ of real values in the range $([0,1])$ that add up to 1. The function is given by\n",
    "$$\n",
    "\\sigma (\\mathbf{z})_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}}, j=1, 2, \\dots, K\n",
    "$$\n",
    "In probability theory, the output of the Softmax function can be used to represent a categorical distribution, that is, a probability distribution over $(K)$ different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution. Here is an example of Softmax application \n",
    "<center>\n",
    "<img src=\"images/softmax_ac.png\"/>\n",
    "</center>\n",
    "The softmax function is used in various multiclass classification methods, such as multinomial logistic regression, multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of K distinct linear functions, and the predicted probability for the \\(j\\)th class given a sample vector $(\\mathbf{x})$ and a weighting vector $(\\mathbf{w})$ is\n",
    "\n",
    "$$\n",
    "P(y=j|\\mathbf{x})=\\frac{e^{x^{T}w_{j}}}{\\sum_{k=1}^{K}e^{x^{T}w_{k}}}\n",
    "$$\n",
    "\n",
    "This can be seen as the composition of $(K)$ linear functions $(\\mathbf{x}\\mapsto x^{T}w_{1},\\dots ,\\mathbf{x}\\mapsto x^{T}w_{K})$ and the softmax function (where $(x^{T}w)$ denotes the inner product of $(\\mathbf{x})$ and $(\\mathbf{w}))$. The operation is equivalent to applying a linear operator defined by $(\\mathbf{w})$ to vectors $(\\mathbf{x})$, thus transforming the original, probably highly-dimensional, input to vectors in a $(K)$-dimensional space $(R^{K})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.Softmax(dim=0)\n",
    "out = activation(x)\n",
    "\n",
    "plt.plot(x.numpy(), out.numpy())\n",
    "plt.title('Softmax')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"softmin---nnsoftmin\">Softmin - <code>nn.Softmin()</code></h3>\n",
    "\n",
    "$${Softmin}(x_i) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}$$\n",
    "\n",
    "<p>It turns numbers into a probability distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above activation functions (<em>i.e.</em> ReLU, LeakyReLU, PReLU) are scale-invariant.</p>\n",
    "\n",
    "<h3 id=\"softplus---softplus\">Softplus - <code>Softplus()</code></h3>\n",
    "\n",
    "$${Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))$$\n",
    "\n",
    "\n",
    "<p>Softplus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.</p>\n",
    "\n",
    "<p>The function will become more like ReLU, if the $\\beta$ gets larger and larger.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"logsoftargmax---nnlogsoftmax\">LogSoft(arg)max - <code>nn.LogSoftmax()</code></h3>\n",
    "\n",
    "$${LogSoftmax}(x_i) = \\log\\left(\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"elu---nnelu\">ELU - <code>nn.ELU()</code></h3>\n",
    "\n",
    "$${ELU}(x) = \\max(0, x) + \\min(0, \\alpha * (\\exp(x) - 1)$$\n",
    "\n",
    "<p>Unlike ReLU, it can go below 0 which allows the system to have average output to be zero. Therefore, the model may converge faster. And its variations (CELU, SELU) are just different parametrizations.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"celu---nncelu\">CELU - <code>nn.CELU()</code></h3>\n",
    "\n",
    "$${CELU}(x) = \\max(0, x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"selu---nnselu\">SELU - <code>nn.SELU()</code></h3>\n",
    "\n",
    "$${SELU}(x) = \\text{scale} * (\\max(0, x) + \\min(0, \\alpha * (\\exp(x) - 1))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"gelu---nngelu\">GELU - <code>nn.GELU()</code></h3>\n",
    "\n",
    "$${GELU(x)} = x * \\Phi(x)$$\n",
    "\n",
    "<p>where $\\Phi(x)$ is the Cumulative Distribution Function for Gaussian Distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"relu6---nnrelu6\">ReLU6 - <code>nn.ReLU6()</code></h3>\n",
    "\n",
    "$${ReLU6}(x) = \\min(\\max(0,x),6)$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/ReLU6.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 10</b>: ReLU6\n",
    "</center>\n",
    "\n",
    "<p>This is ReLU saturating at 6. But there is no particular reason why picking 6 as saturation, so we can do better by using Sigmoid function below.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"softsign---nnsoftsign\">Softsign - <code>nn.Softsign()</code></h3>\n",
    "\n",
    "$${SoftSign}(x) = \\frac{x}{1 + |x|}$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/Softsign.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 13</b>: Softsign\n",
    "</center>\n",
    "\n",
    "<p>It is similar to the Sigmoid function but gets to the asymptote slowly and alleviate the gradient vanishing problem (to some extent).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"hardtanh---nnhardtanh\">Hardtanh - <code>nn.Hardtanh()</code></h3>\n",
    "\n",
    "$${HardTanh}(x) = \\begin{cases}\n",
    "      1, & \\text{if} x &gt; 1\\\\\n",
    "      -1, & \\text{if} x &lt; -1\\\\\n",
    "      x, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "<p>The range of the linear region [-1, 1] can be adjusted using <code>min_val</code> and <code>max_val</code>.</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/Hardtanh.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 14</b>: Hardtanh\n",
    "</center>\n",
    "\n",
    "<p>It works surprisingly well especially when weights are kept within the small value range.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"threshold---nnthreshold\">Threshold - <code>nn.Threshold()</code></h3>\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "      x, & \\text{if} x &gt; \\text{threshold}\\\\\n",
    "      v, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "<p>It is rarely used because we cannot propagate the gradient back. And it is also the reason preventing people from using back-propagation in 60s and 70s when they were using binary neurons.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"tanhshrink---nntanhshrink\">Tanhshrink - <code>nn.Tanhshrink()</code></h3>\n",
    "\n",
    "$${Tanhshrink}(x) = x - \\tanh(x)$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/Tanhshrink.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 15</b>: Tanhshrink\n",
    "</center>\n",
    "\n",
    "<p>It is rarely used except for sparse coding to compute the value of the latent variable.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"softshrink---nnsoftshrink\">Softshrink - <code>nn.Softshrink()</code></h3>\n",
    "\n",
    "$${SoftShrinkage}(x) = \\begin{cases}\n",
    "      x - \\lambda, & \\text{if} x &gt; \\lambda\\\\\n",
    "      x + \\lambda, & \\text{if} x &lt; -\\lambda\\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/Softshrink.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 16</b>: Softshrink\n",
    "</center>\n",
    "\n",
    "<p>This basically shrinks the variable by a constant towards 0, and forces to 0 if the variable is close to 0. You can think of it as a step of gradient for the $\\ell_1$ criteria. It is also one of the step of the Iterative Shrinkage-Thresholding Algorithm (ISTA). But it is not commonly used in standard neural network as activations.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"hardshrink---nnhardshrink\">Hardshrink - <code>nn.Hardshrink()</code></h3>\n",
    "\n",
    "$${HardShrinkage}(x) = \\begin{cases}\n",
    "      x, & \\text{if} x &gt; \\lambda\\\\\n",
    "      x, & \\text{if} x &lt; -\\lambda\\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/Hardshrink.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 17</b>: Hardshrink\n",
    "</center>\n",
    "\n",
    "<p>It is rarely used except for sparse coding.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"logsigmoid---nnlogsigmoid\">LogSigmoid - <code>nn.LogSigmoid()</code></h3>\n",
    "\n",
    "$${LogSigmoid}(x) = \\log\\left(\\frac{1}{1 + \\exp(-x)}\\right)$$\n",
    "\n",
    "<center>\n",
    "<img src=\"/pytorch-Deep-Learning/images/week11/11-1/LogSigmoid.png\" height=\"400px\" /><br />\n",
    "<b>Fig. 18</b>: LogSigmoid\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "This python script illustrates the different loss functions for regression and classification. We start by loading the ncessary libraries and resetting the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various Predicted X-values\n",
    "X = torch.linspace(-1, 1, 500)\n",
    "\n",
    "# Create our target of zero\n",
    "y = torch.zeros(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is an important part in artificial neural networks, which is used to measure the inconsistency between predicted value $((\\hat{y}))$ and actual label $((y))$. It is a non-negative value, where the robustness of model increases along with the decrease of the value of loss function. Loss function is the hard core of empirical risk function as well as a significant component of structural risk function. Generally, the structural risk function of a model is consist of empirical risk term and regularization term, which can be represented as\n",
    "$$\\boldsymbol{\\theta}^{*}=\\arg\\min_{\\boldsymbol{\\theta}}\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\theta})+\\lambda\\centerdot\\Phi(\\boldsymbol{\\theta})=\\arg\\min_{\\boldsymbol{\\theta}}\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},\\hat{y}^{(i)}\\big)+\\lambda\\centerdot\\Phi(\\boldsymbol{\\theta})\\\\=\\arg\\min_{\\boldsymbol{\\theta}}\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta})\\big)+\\lambda\\centerdot\\Phi(\\boldsymbol{\\theta})$$\n",
    "where $(\\Phi(\\boldsymbol{\\theta}))$ is the regularization term or penalty term, $(\\boldsymbol{\\theta})$ is the parameters of model to be learned, $(f(\\centerdot))$ represents the activation function and $(\\mathbf{x}^{(i)}=\\{x_{1}^{(i)},x_{2}^{(i)},\\dots ,x_{m}^{(i)}\\}\\in\\mathbb{R}^{m})$ denotes the a training sample.\n",
    "Here we only concentrate on the empirical risk term (loss function)$\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta})\\big)$and introduce the mathematical expressions of several commonly-used loss functions as well as the corresponding expression in DeepLearning4J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"nnmseloss\"><code>nn.MSELoss()</code></h3>\n",
    "\n",
    "<p>This function gives the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$. It is also called L2 loss.</p>\n",
    "\n",
    "<p>If we are using a minibatch of $n$ samples, then there are $n$ losses, one for each sample in the batch. We can tell the loss function to keep that loss as a vector or to reduce it.</p>\n",
    "\n",
    "<p>If unreduced (<em>i.e.</em> set <code>reduction='none'</code>), the loss is</p>\n",
    "\n",
    "$$l(x,y) = L = \\{l_1, \\dots, l_N\\}^\\top, l_n = (x_n - y_n)^2$$\n",
    "\n",
    "<p>where $N$ is the batch size, $x$ and $y$ are tensors of arbitrary shapes with a total of n elements each.</p>\n",
    "\n",
    "<p>The reduction options are below (note that the default value is <code>reduction='mean'</code>).</p>\n",
    "\n",
    "$$l(x,y) = \\begin{cases}\\text{mean}(L), \\quad &\\text{if reduction='mean'}\\\\\n",
    "\\text{sum}(L), \\quad &\\text{if reduction='sum'}\n",
    "\\end{cases}$$\n",
    "\n",
    "<p>The sum operation still operates over all the elements, and divides by $n$.</p>\n",
    "\n",
    "<p>The division by $n$ can be avoided if one sets <code>reduction = 'sum'</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3347)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.MSELoss(reduction='mean')\n",
    "out = loss(X, y)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Logarithmic Error\n",
    "Mean Squared Logarithmic Error (MSLE) loss function is a variant of MSE, which is defined as\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=\\frac{1}{n}\\sum_{i=1}^{n}\\big(\\log(y^{(i)}+1)-\\log(\\hat{y}^{(i)}+1)\\big)^{2}\n",
    "$$\n",
    "MSLE is also used to measure the different between actual and predicted. By taking the log of the predictions and actual values, what changes is the variance that you are measuring. It is usually used when you do not want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers. Another thing is that MSLE penalizes under-estimates more than over-estimates. 1. If both predicted and actual values are small: MSE and MSLE is same. 2. If either predicted or the actual value is big: $(MSE > MSLE)$. 3. If both predicted and actual values are big: $(MSE > MSLE)$ (MSLE becomes almost negligible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\n",
    "L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division by $(n)$, it is computed by\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=\\sum_{i=1}^{n}(y^{(i)}-\\hat{y}^{(i)})^{2}\n",
    "$$\n",
    "For more details, typically in mathematic, please read the paper: On Loss Functions for Deep Neural Networks in Classification, which gives comprehensive explanation about several commomly-used loss functions, including L2, L1 loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = nn.MSELoss(reduction='none')\n",
    "l2_out = l2_loss(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by $$\n",
    "\\boldsymbol{\\mathcal{L}}=\\frac{1}{n}\\sum_{i=1}^{n}\\big\\lvert y^{(i)}-\\hat{y}^{(i)}\\big\\rvert\n",
    "$$\n",
    "where $(\\lvert\\centerdot\\rvert)$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5010)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.L1Loss(reduction='mean')\n",
    "out = loss(X, y)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error\n",
    "Mean Absolute Percentage Error (MAPE) is a variant of MAE, it is computed by\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=\\frac{1}{n}\\sum_{i=1}^{n}\\bigg\\lvert\\frac{y^{(i)}-\\hat{y}^{(i)}}{y^{(i)}}\\bigg\\rvert\\centerdot100\n",
    "$$\n",
    "Although the concept of MAPE sounds very simple and convincing, it has major drawbacks in practical application: 1. It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division by zero. 2. For forecasts which are too low the percentage error cannot exceed $(100\\%)$, but for forecasts which are too high there is no upper limit to the percentage error. 3. When MAPE is used to compare the accuracy of prediction methods it is biased in that it will systematically select a method whose forecasts are too low. This little-known but serious issue can be overcome by using an accuracy measure based on the ratio of the predicted to actual value (called the Accuracy Ratio), this approach leads to superior statistical properties and leads to predictions which can be interpreted in terms of the geometric mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\n",
    "<h3 id=\"nnl1loss\"><code>nn.L1Loss()</code></h3>\n",
    "\n",
    "<p>This measures the mean absolute error (MAE) between each element in the input $x$ and target $y$ (or the actual output and desired output).</p>\n",
    "\n",
    "<p>If unreduced (<em>i.e.</em> set <code>reduction='none'</code>), the loss is</p>\n",
    "\n",
    "$$l(x,y) = L = \\{l_1, \\dots, l_N\\}^\\top, l_n = \\vert x_n - y_n\\vert$$\n",
    "\n",
    "<p>, where $N$ is the batch size, $x$ and $y$ are tensors of arbitrary shapes with a total of n elements each.</p>\n",
    "\n",
    "<p>It also has <code>reduction</code> option of <code>'mean'</code> and <code>'sum'</code> similar to what <code>nn.MSELoss()</code> have.</p>\n",
    "\n",
    "<p><strong>Use Case:</strong> L1 loss is more robust against outliers and noise compared to L2 loss. In L2, the errors of those outlier/noisy points are squared, so the cost function gets very sensitive to outliers.</p>\n",
    "\n",
    "<p><strong>Problem:</strong> The L1 loss is not differentiable at the bottom (0). We need to be careful when handling its gradients (namely Softshrink). This motivates the following SmoothL1Loss.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss(reduction='none')\n",
    "l1_out = l1_loss(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Huber Loss\n",
    "The psuedo-huber loss function is a smooth approximation to the L1 loss as the (predicted - target) values get larger. When the predicted values are close to the target, the pseudo-huber loss behaves similar to the L2 loss.\n",
    "\n",
    "$$\\text{loss}(x, y) = \\frac{1}{n} \\sum_{i} z_{i}\\$$\n",
    "\n",
    "<h3 id=\"nnsmoothl1loss\"><code>nn.SmoothL1Loss()</code></h3>\n",
    "\n",
    "<p>This function uses L2 loss if the absolute element-wise error falls below 1 and L1 loss otherwise.</p>\n",
    "\n",
    "<p>\\(\\text{loss}(x, y) = \\frac{1}{n} \\sum_i z_i\\)\n",
    ", where $z_i$ is given by</p>\n",
    "\n",
    "$$z_i = \\begin{cases}0.5(x_i-y_i)^2, \\quad &\\text{if } |x_i - y_i| &lt; 1\\\\\n",
    "|x_i - y_i| - 0.5, \\quad &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "<p>It also has <code>reduction</code> options.</p>\n",
    "\n",
    "<p>This is advertised by Ross Girshick (<a href=\"https://arxiv.org/abs/1504.08083\">Fast R-CNN</a>). The Smooth L1 Loss is also known as the Huber Loss or  the Elastic Network when used as an objective function,.</p>\n",
    "\n",
    "<p><strong>Use Case:</strong> It is less sensitive to outliers than the <code>MSELoss</code> and is smooth at the bottom. This function is often used in computer vision for protecting against outliers.</p>\n",
    "\n",
    "<p><strong>Problem:</strong> This function has a scale ($0.5$ in the function above).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_huber = nn.SmoothL1Loss(reduction='none')\n",
    "p_out = p_huber(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Regression Losses\n",
    "Here we use Matplotlib to plot the L1, L2, and Pseudo-Huber Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X.numpy(), l2_out.numpy(), 'b-', label='L2 Loss')\n",
    "plt.plot(X.numpy(), l1_out.numpy(), 'r--', label='L1 Loss')\n",
    "plt.plot(X.numpy(), p_out.numpy(), 'k-.', label='P-Huber Loss (0.5)')\n",
    "plt.ylim(-0.2, 0.4)\n",
    "plt.legend(loc='lower right', prop={'size': 11})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"l1-vs-l2-for-computer-vision\">L1 <em>vs.</em> L2 for Computer Vision</h3>\n",
    "\n",
    "<p>In making predictions when we have a lot of different $y$’s:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>If we use MSE (L2 Loss), it results in an average of all $y$, which in CV it means we will have a blurry image.</li>\n",
    "  <li>If we use L1 loss, the value $y$ that minimize the L1 distance is the medium, which is not blurry, but note that medium is difficult to define in multiple dimensions.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Using L1 results in sharper image for prediction.</p>\n",
    "\n",
    "<h3 id=\"nnnllloss\"><code>nn.NLLLoss()</code></h3>\n",
    "\n",
    "<p>It is the negative log likelihood loss used when training a classification problem with C classes.</p>\n",
    "\n",
    "<p>Note that, mathematically, the input of <code>NLLLoss</code> should be (log) likelihoods, but PyTorch doesn’t enforce that. So the effect is to make the desired component as large as possible.</p>\n",
    "\n",
    "<p>The unreduced (<em>i.e.</em> with :attr:<code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>\n",
    "\n",
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$$\n",
    "\n",
    "<p>,where $N$ is the batch size.</p>\n",
    "\n",
    "<p>If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then</p>\n",
    "\n",
    "$$\\ell(x, y) = \\begin{cases}\n",
    "            \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n",
    "            \\text{if reduction} = \\text{'mean';}\\\\\n",
    "            \\sum_{n=1}^N l_n,  &\n",
    "            \\text{if reduction} = \\text{'sum'.}\n",
    "        \\end{cases}$$\n",
    "\n",
    "<p>This loss function has an optional argument <code>weight</code> that can be passed in using a 1D Tensor assigning weight to each of the classes. This is useful when dealing with imbalanced training set.</p>\n",
    "\n",
    "<h4 id=\"weights--imbalanced-classes\">Weights & Imbalanced Classes:</h4>\n",
    "\n",
    "<p>Weight vector is useful if the frequency is different for each category/class. For example, the frequency of the common flu is much higher than the lung cancer. We can simply increase the weight for categories that has small number of samples.</p>\n",
    "\n",
    "<p>However, instead of setting the weight, it’s better to equalize the frequency in training so that we can exploits stochastic gradients better.</p>\n",
    "\n",
    "<p>To equalize the classes in training, we put samples of each class in a different buffer. Then generate each minibatch by picking the same number samples from each buffer. When the smaller buffer runs out of samples to use, we iterate through the smaller buffer from the beginning again until every sample of the larger class is used. This way gives us equal frequency for all categories by going through those circular buffers. We should never go the easy way to equalize frequency by <strong>not</strong> using all samples in the majority class. Don’t leave data on the floor!</p>\n",
    "\n",
    "<p>An obvious problem of the above method is that our NN model wouldn’t know the relative frequency of the actual samples. To solve that, we fine-tune the system by running a few epochs at the end with the actual class frequency, so that the system adapts to the biases at the output layer to favour things that are more frequent.</p>\n",
    "\n",
    "<p>To get an intuition of this scheme, let’s go back to the medical school example: students spend just as much time on rare disease as they do on frequent diseases (or maybe even more time, since the rare diseases are often the more complex ones). They learn to adapt to the features of all of them, then correct it to know which are rare.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Predictions\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "We now consider categorical loss functions.  Here, the predictions will be around the target of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various predicted X values\n",
    "x_vals = torch.empty(500, dtype=torch.long).random_(5)\n",
    "\n",
    "# Target of 1.0\n",
    "y_val = torch.empty(500, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback Leibler (KL) Divergence\n",
    "KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{D}_{KL}(y^{(i)}||\\hat{y}^{(i)})=\\frac{1}{n}\\sum_{i=1}^{n}\\big[y^{(i)}\\centerdot\\log\\big(\\frac{y^{(i)}}{\\hat{y}^{(i)}}\\big)\\big]\\\\=\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\big(y^{(i)}\\centerdot\\log(y^{(i)})\\big)}_{\\boldsymbol{entropy}}\\underbrace{-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y^{(i)}\\centerdot\\log(\\hat{y}^{(i)})\\big)}_{\\boldsymbol{cross-entropy}}\n",
    "$$\n",
    "where the first term is entropy and another is cross entropy (another kind of loss function which will be introduced later). KL divergence is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a KL divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a KL divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. For more details, please visit the wikipedia: [link].\n",
    "\n",
    "<code>nn.KLDivLoss()</code>\n",
    "\n",
    "$$\\ell(x,y) = L = \\{l_1,...,l_N\\}^T, \\qquad l_n = y_n(\\log y_n-x_n)$$\n",
    "\n",
    "<p>This is simple loss function for when your target is a one-hot distribution (<em>i.e.</em> $y$ is a category). Again it assumes $x$ and $y$ are probabilities. It has the disadvantage that it is not merged with a softmax or log-softmax so it may have numerical stability issues.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "Cross Entropy is commonly-used in binary classification (labels are assumed to take values 0 or 1) as a loss function (For multi-classification, use Multi-class Cross Entropy), which is computed by\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=-\\frac{1}{n}\\sum_{i=1}^{n}\\big[y^{(i)}\\log(\\hat{y}^{(i)})+(1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\big]\n",
    "$$\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. As we have mentioned in MSE that it suffers slow divergence when using Sigmoid as activation function, here the cross entropy does not have such problem. Samely, $(\\hat{y}^{(i)}=\\sigma(\\mathbf{z}^{(i)})=\\sigma(\\boldsymbol{\\theta}^{T}\\mathbf{x}^{(i)}))$, and we only consider one training sample, by using Sigmoid, we have $(\\boldsymbol{\\mathcal{L}}=y\\log(\\sigma(\\mathbf{z}))+(1-y)\\log(1-\\sigma(\\mathbf{z})))$, and compute it derivative as\n",
    "$$\n",
    "\\frac{\\partial\\boldsymbol{\\mathcal{L}}}{\\partial\\boldsymbol{\\theta}}=(y-\\sigma(\\mathbf{z}))\\centerdot\\mathbf{x}\n",
    "$$\n",
    "compare to the derivative in MSE, it eliminates the term $(\\sigma'(\\mathbf{z}))$, where the learning speed is only controlled by $((y-\\sigma(\\mathbf{z})))$. In this case, when the difference between predicted value and actual value is large, the learning speed, i.e., convergence speed, is fast, otherwise, the difference is small, the learning speed is small, this is our expectation. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization (like the momentum, it increases the update step). For the mathematical details, see wikipedia: [link].\n",
    "\n",
    "<code>nn.CrossEntropyLoss()</code>\n",
    "\n",
    "<p>This function combines <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code> in one single class. The combination of the two makes the score of the correct class as large as possible.</p>\n",
    "\n",
    "<p>The reason why the two functions are merged here is for numerical stability of gradient computation. When the value after softmax is close to 1 or 0, the log of that can get close to 0 or $-\\infty$. Slope of log close to 0 is close to $\\infty$, causing the intermediate step in backpropagation to have numerical issues. When the two functions are combined, the gradients is saturated so we get a reasonable number at the end.</p>\n",
    "\n",
    "<p>The input is expected to be unnormalised score for each class.</p>\n",
    "\n",
    "<p>The loss can be described as:</p>\n",
    "\n",
    "$${loss}(x, c) = -\\log\\left(\\frac{\\exp(x[c])}{\\sum_j \\exp(x[j])}\\right)\n",
    "= -x[c] + \\log\\left(\\sum_j \\exp(x[j])\\right)$$\n",
    "\n",
    "<p>or in the case of the <code>weight</code> argument being specified:</p>\n",
    "\n",
    "$${loss}(x, c) = w[c] \\left(-x[c] + \\log\\left(\\sum_j\\exp(x[j])\\right)\\right)$$\n",
    "\n",
    "<p>The losses are averaged across observations for each minibatch.</p>\n",
    "\n",
    "<p>A physical interpretation of the Cross Entropy Loss is related to the Kullback–Leibler divergence (KL divergence), where we are measuring the divergence between two distributions. Here, the (quasi) distributions are represented by the x vector (predictions) and the target distribution (a one-hot vector with 0 on the wrong classes and 1 on the right class).</p>\n",
    "\n",
    "<p>Mathematically,</p>\n",
    "\n",
    "$$H(p,q) = H(p) + \\mathcal{D}_{KL} (p \\mid\\mid q)$$\n",
    "\n",
    "<p>where \\(H(p,q) = - \\sum_i p(x_i) \\log (q(x_i))\\) is the cross-entropy (between two distributions), \\(H(p) = - \\sum_i p(x_i) \\log (p(x_i))\\) is the entropy, and \\(\\mathcal{D}_{KL} (p \\mid\\mid q) = \\sum_i p(x_i) \\log \\frac{p(x_i)}{q(x_i)}\\) is the KL divergence.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"nnadaptivelogsoftmaxwithloss\"><code>nn.AdaptiveLogSoftmaxWithLoss()</code></h3>\n",
    "\n",
    "<p>This is an efficient softmax approximation of softmax for large number of classes (for example, millions of classes). It implements tricks to improve the speed of the computation.</p>\n",
    "\n",
    "<p>Details of the method is described in <a href=\"https://arxiv.org/abs/1609.04309\">Efficient softmax approximation for GPUs</a> by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson\n",
    "Poisson loss function is a measure of how the predicted distribution diverges from the expected distribution, the poisson as loss function is a variant from Poisson Distribution, where the poisson distribution is widely used for modeling count data. It can be shown to be the limiting distribution for a normal approximation to a binomial where the number of trials goes to infinity and the probability goes to zero and both happen at such a rate that np is equal to some mean frequency for the process. In DL4J, the poisson loss function is computed by\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}=\\frac{1}{n}\\sum_{i=1}^{n}\\big(\\hat{y}^{(i)}-y^{(i)}\\centerdot\\log(\\hat{y}^{(i)})\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"binary-cross-entropy-bce-loss---nnbceloss\"><a href=\"https://www.youtube.com/watch?v=bj1fh3BvqSU&t=3207s\">Binary Cross Entropy (BCE) Loss - <code>nn.BCELoss()</code></a></h3>\n",
    "\n",
    "$$\\ell(x,y) = L = \\{l_1,...,l_N\\}^T, \\qquad l_n = -w_n[y_n\\log x_n+(1-y_n)\\log(1-x_n)]$$\n",
    "\n",
    "<p>This loss is a special case of cross entropy for when you have only two classes so it can be reduced to a simpler function. This is used for measuring the error of a reconstruction in, for example, an auto-encoder. This formula assume $x$ and $y$ are probabilities, so they are strictly between 0 and 1.</p>\n",
    "\n",
    "<h3 id=\"bce-loss-with-logits---nnbcewithlogitsloss\">BCE Loss with Logits - <code>nn.BCEWithLogitsLoss()</code></h3>\n",
    "\n",
    "$$\\ell(x,y) = L = \\{l_1,...,l_N\\}^T, \\qquad l_n = -w_n[y_n\\log \\sigma(x_n)+(1-y_n)\\log(1-\\sigma(x_n))]$$\n",
    "\n",
    "<p>This version of binary cross entropy loss takes scores that haven’t gone through softmax so it does not assume x is between 0 and 1. It is then passed through a sigmoid to ensure it is in that range. The loss function is more likely to be numerically stable when combined like this.</p>\n",
    "\n",
    "<h3 id=\"margin-ranking-loss---nnmarginrankingloss\">Margin Ranking Loss - <code>nn.MarginRankingLoss()</code></h3>\n",
    "\n",
    "$$L(x,y) = \\max(0, -y*(x_1-x_2)+\\text{margin})$$\n",
    "\n",
    "<p>Margin losses are an important category of losses. If you have two inputs, this loss function says you want one input to be larger than the other one by at least a margin. In this case $y$ is a binary variable $\\in { -1, 1}$. Imagine the two inputs are scores of two categories. You want the score for the correct category larger than the score for the incorrect categories by at least some margin.  Like hinge loss, if $y*(x_1-x_2)$ is larger than margin, the cost is 0. If it is smaller, the cost increases linearly. If you were to use this for classification, you would have $x_1$ be the score of the correct answer and $x_2$ be the score of the highest scoring incorrect answer in the mini-batch. If used in energy based models (discussed later), this loss function pushes down on the correct answer $x_1$ and up on the incorrect answer $x_2$.</p>\n",
    "\n",
    "<h3 id=\"triplet-margin-loss---nntripletmarginloss\">Triplet Margin Loss - <code>nn.TripletMarginLoss()</code></h3>\n",
    "\n",
    "$$L(a,p,n) = \\max\\{d(a_i,p_i)-d(a_i,n_i)+\\text{margin}, 0\\}$$\n",
    "\n",
    "<p>This loss is used for measuring a relative similarity between samples.  For example, you put two images with the same category through a CNN and get two vectors. You want the distance between those two vectors to be as small as possible. If you put two images with different categories through a CNN, you want the distance between those vectors to be as large as possible. This loss function tries to send the first distance toward 0 and the second distance larger than some margin.  However, the only thing that matter is that the distance between the good pair is smaller than the distance between the bad pair.</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tml.png\" width=\"30%\" /><br />\n",
    "<b>Fig. 1</b>: Triplet Margin Loss\n",
    "</center>\n",
    "\n",
    "<p>This was originally used to train an image search system for Google. At that time, you would type a query into Google and it would encode that query into a vector. It would then compare that vector to a bunch of vectors from images that were previously indexed. Google would then retrieve the images that were the closest to your vector.</p>\n",
    "\n",
    "<h3 id=\"soft-margin-loss---nnsoftmarginloss\">Soft Margin Loss - <code>nn.SoftMarginLoss()</code></h3>\n",
    "\n",
    "$$L(x,y) = \\sum_i\\frac{\\log(1+\\exp(-y[i]*x[i]))}{x.\\text{nelement()}}$$\n",
    "\n",
    "<p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor $x$  and target tensor $y$  (containing 1 or -1).</p>\n",
    "\n",
    "<ul>\n",
    "  <li>This softmax version of a margin loss. You have a bunch of positives and a bunch of negatives you want to pass through a softmax. This loss function then tries to make $\\text{exp}(-y[i]*x[i])$ for the correct $x[i]$ smaller than for any other.</li>\n",
    "  <li>This loss function wants to pull the positive values of $y[i]*x[i]$ closer together and push the negative values far apart but, as opposed to a hard margin, with some continuous, exponentially decaying effect on the loss .</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"multi-class-hinge-loss---nnmultilabelmarginloss\">Multi-Class Hinge Loss - <code>nn.MultiLabelMarginLoss()</code></h3>\n",
    "\n",
    "$$L(x,y)=\\sum_{ij}\\frac{max(0,1-(x[y[j]]-x[i]))}{x.\\text{size}(0)}$$\n",
    "\n",
    "<p>This margin-base loss allows for different inputs to have variable amounts of targets. In this case you have several categories for which you want high scores and it sums the  hinge loss over all categories. For EBMs, this loss function pushes down on desired categories and pushes up on non-desired categories.</p>\n",
    "\n",
    "<h3 id=\"hinge-embedding-loss---nnhingeembeddingloss\">Hinge Embedding Loss - <code>nn.HingeEmbeddingLoss()</code></h3>\n",
    "\n",
    "$$l_n =\n",
    "\\left\\{\n",
    "     \\begin{array}{lr}\n",
    "     x_n, &\\quad y_n=1,  \\\\\n",
    "     \\max\\{0,\\Delta-x_n\\}, &\\quad y_n=-1  \\\\\n",
    "     \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "<p>Hinge embedding loss used for semi-supervised learning by measuring whether two inputs are similar or dissimilar. It pulls together things that are similar and pushes away things are dissimilar. The $y$ variable indicates whether the pair of scores need to go in a certain direction. Using a hinge loss, the score is positive if $y$ is 1 and some margin $\\Delta$ if $y$ is -1.</p>\n",
    "\n",
    "<h3 id=\"cosine-embedding-loss---nncosineembeddingloss\">Cosine Embedding Loss - <code>nn.CosineEmbeddingLoss()</code></h3>\n",
    "\n",
    "$$l_n =\n",
    "\\left\\{\n",
    "     \\begin{array}{lr}\n",
    "     1-\\cos(x_1,x_2), & \\quad y=1,  \\\\\n",
    "     \\max(0,\\cos(x_1,x_2)-\\text{margin}), & \\quad y=-1\n",
    "     \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "<p>This loss is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p>\n",
    "\n",
    "<ul>\n",
    "  <li>Thought of another way, 1 minus the cosine of the angle between the two vectors is basically the normalised Euclidean distance.</li>\n",
    "  <li>The advantage of this is that whenever you have two vectors and you want to make their distance as large as possible, it is very easy to make the network achieve this by make the vectors very long. Of course this is not optimal. You don’t want the system to make the vectors big but rotate vectors in the right direction so you normalise the vectors and calculate the normalised Euclidean distance.</li>\n",
    "  <li>For positive cases, this loss tries to make the vectors as aligned as possible. For negative pairs, this loss tries to make the cosine smaller than a particular margin. The margin here should be some small positive value.</li>\n",
    "  <li>In a high dimensional space, there is a lot of area near the equator of the sphere. After normalisation, all your points are now normalised on the sphere. What you want is samples that are semantically similar to you to be close. The samples that are dissimilar should be orthogonal. You don’t want them to be opposite each other because there is only one point at the opposite pole. Rather, on the equator, there is a very large amount of space so you want to make the margin some small positive value so you can take advantage of all this area.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"connectionist-temporal-classification-ctc-loss---nnctcloss\"><a href=\"https://www.youtube.com/watch?v=bj1fh3BvqSU&t=4103s\">Connectionist Temporal Classification (CTC) Loss - <code>nn.CTCLoss()</code></a></h3>\n",
    "\n",
    "<p>Calculates loss between a continuous (unsegmented) time series and a target sequence.</p>\n",
    "<ul>\n",
    "  <li>CTC loss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node.</li>\n",
    "  <li>The alignment of input to target is assumed to be “many-to-one”, which limits the length of the target sequence such that it must less than or equal to the input length.</li>\n",
    "  <li>Useful when your output is a sequence of vectors, which is correspond to scores of categories.</li>\n",
    "</ul>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/Fig1.png\" width=\"30%\" /><br />\n",
    "<b>Fig. 2</b>: CTC Loss for speech recognition\n",
    "</center>\n",
    "\n",
    "<p>Application Example: Speech recognition system</p>\n",
    "<ul>\n",
    "  <li>Goal: Predict what word is being pronounced every 10 milliseconds.</li>\n",
    "  <li>Each word is represented by a sequence of sounds.</li>\n",
    "  <li>Depends on the person’s speaking speed, different length of the sounds might be mapped to the same word.</li>\n",
    "  <li>Find the best mapping from the input sequence to the output sequence. A good method for this is using dynamic programming to find the minimum cost path.</li>\n",
    "</ul>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/Fig2.png\" width=\"30%\" style=\"background-color:white;\" /><br />\n",
    "<b>Fig. 3</b>: Many-to-one mapping setup\n",
    "</center>\n",
    "\n",
    "<h1 id=\"energy-based-models-part-iv---loss-function\">Energy-Based Models (Part IV) - Loss Function</h1>\n",
    "\n",
    "<h2 id=\"architecture-and-loss-functional\">Architecture and Loss Functional</h2>\n",
    "\n",
    "<p>Family of energy functions: $\\mathcal{E} = {E(W,Y, X) : W \\in \\mathcal{W}}$.</p>\n",
    "\n",
    "<p>Training set: $S = {(X^i, Y^i): i = 1 \\cdots P}$</p>\n",
    "\n",
    "<p>Loss functional: $\\mathcal{L} (E, S)$</p>\n",
    "<ul>\n",
    "  <li>Functional means a function of another function. In our case, the functional $\\mathcal{L} (E, S)$ is a function of the energy function $E$.</li>\n",
    "  <li>Because $E$ is parametrised by $W$, we can turn the functional to a loss function of $W$: $\\mathcal{L} (W, S)$</li>\n",
    "  <li>Measures the quality of an energy function on training set</li>\n",
    "  <li>Invariant under permutations and repetitions of the samples.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Training: $W^* = \\min_{W\\in \\mathcal{W}} \\mathcal{L}(W, S)$.</p>\n",
    "\n",
    "<p>Form of the loss functional:</p>\n",
    "<ul>\n",
    "  <li>$L(Y^i, E(W, \\mathcal{Y}, X^i))$ is per-sample loss</li>\n",
    "  <li>$Y^i$ is desired answer, can be category or a whole image, etc.</li>\n",
    "  <li>$E(W, \\mathcal{Y}, X^i)$ is energy surface for a given $X_i$ as $Y$ varies</li>\n",
    "  <li>$R(W)$ is regulariser</li>\n",
    "</ul>\n",
    "\n",
    "$$\\mathcal{L}(E, S) = \\frac{1}{P} \\sum_{i=1}^P L(Y^i, E(W,\\mathcal{Y}, \tX^i)) + R(W)$$\n",
    "\n",
    "<h2 id=\"designing-a-good-loss-function\">Designing a Good Loss Function</h2>\n",
    "\n",
    "<p><strong>Push down</strong> on the energy of the correct answer.</p>\n",
    "\n",
    "<p><strong>Push up</strong> on the energies of the incorrect answers, particularly if they are smaller than the correct one.</p>\n",
    "\n",
    "<h2 id=\"examples-of-loss-functions\">Examples of Loss Functions</h2>\n",
    "\n",
    "<h3 id=\"energy-loss\">Energy Loss</h3>\n",
    "\n",
    "$$L_{energy} (Y^i, E(W, \\mathcal{Y}, X^i)) = E(W, Y^i, X^i)$$\n",
    "\n",
    "<p>This loss function simply pushes down on the energy of the correct answer. If the network is not designed properly, it might end up with a mostly flat energy function as you only trying to make the energy of the correct answer small but not pushing up the energy elsewhere. Thus, the system might collapses.</p>\n",
    "\n",
    "<h3 id=\"negative-log-likelihood-loss\">Negative Log-Likelihood Loss</h3>\n",
    "\n",
    "$$L_{nll}(W, S) = \\frac{1}{P} \\sum_{i=1}^P (E(W, Y^i, X^i) + \\frac{1}{\\beta} \\log \\int_{y \\in \\mathcal{Y}} e^{\\beta E(W, y, X^i)})$$\n",
    "\n",
    "<p>This loss function pushes down on the energy of the correct answer while pushing up on the energies of all answers in proportion to their probabilities. This reduces to the perceptron loss when $\\beta \\rightarrow \\infty$. It has been used for a long time in many communities for discriminative training with structured outputs.</p>\n",
    "\n",
    "<p>A probabilistic model is an EBM in which:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>The energy can be integrated over Y (the variable to be predicted)</li>\n",
    "  <li>The loss function is the negative log-likelihood</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"perceptron-loss\"><a href=\"https://www.youtube.com/watch?v=bj1fh3BvqSU&t=4998s\">Perceptron Loss</a></h3>\n",
    "\n",
    "$$L_{perceptron}(Y^i,E(W,\\mathcal Y, X^*))=E(W,Y^i,X^i)-\\min_{Y\\in \\mathcal Y} E(W,Y,X^i)$$\n",
    "\n",
    "<p>Very similar to the perceptron loss from 60+ years ago, and it’s always positive because the minimum is also taken over $Y^i$, so $E(W,Y^i,X^i)-\\min_{Y\\in\\mathcal Y} E(W,Y,X^i)\\geq E(W,Y^i,X^i)-E(W,Y^i,X^i)=0$. The same computation shows that it give exactly zero only when $Y^i$ is the correct answer.</p>\n",
    "\n",
    "<p>This loss makes the energy of the correct answer small, and at the same time, makes the energy for all other answers as large as possible. However, this loss does not prevent the function from giving the same value to every incorrect answer $Y^i$, so in this sense, it is a bad loss function for non-linear systems. To improve this loss, we define the <em>most offending incorrect answer</em>.</p>\n",
    "\n",
    "<h2 id=\"generalized-margin-loss\">Generalized Margin Loss</h2>\n",
    "\n",
    "<p><strong>Most offending incorrect answer</strong>: discrete case\n",
    "Let $Y$ be a discrete variable. Then for a training sample $(X^i,Y^i)$, the <em>most offending incorrect answer</em> $\\bar Y^i$ is the answer that has the lowest energy among all possible answers that are incorrect:</p>\n",
    "\n",
    "$$\\bar Y^i = \\text{argmin}_{y \\in \\mathcal Y \\text{and} Y \\neq Y^i} E(W, Y,X^i)$$\n",
    "\n",
    "<p><strong>Most offending incorrect answer</strong>: continuous case\n",
    "Let $Y$ be a continuous variable. Then for a training sample $(X^i,Y^i)$, the <em>most offending incorrect answer</em> $\\bar Y^i$ is the answer that has the lowest energy among all answers that are at least $\\epsilon$ away from the correct answer:</p>\n",
    "\n",
    "$$\\bar Y^i = \\text{argmin}_{Y \\in \\mathcal Y \\text{and} |Y-Y^i| \\gt;\\epsilon} E(W,Y,X^i)$$\n",
    "\n",
    "<p>In the discrete case, the <em>most offending incorrect answer</em> is the one with smallest energy that isn’t the correct answer. In the continuous case, the energy for $Y$ extremely close to $Y^i$ should be close to $E(W,Y^i,X^i)$. Furthermore, the $\\text{argmin}$ evaluated over $Y$ not equal to $Y^i$ would be 0. As a result, we pick a distance $\\epsilon$ and decide that only $Y$’s at least $\\epsilon$ from $Y_i$ should be considered the “incorrect answer”. This is why the optimization is only over $Y$’s of distance at least $\\epsilon$ from $Y^i$.</p>\n",
    "\n",
    "<p>If the energy function is able to ensure that the energy of the <em>most offending incorrect answer</em> is higher than the energy of the correct answer by some margin, then this energy function should work well.</p>\n",
    "\n",
    "<h2 id=\"examples-of-generalized-margin-loss-functions\">Examples of Generalized Margin Loss Functions</h2>\n",
    "\n",
    "<h3 id=\"hinge-loss\">Hinge Loss</h3>\n",
    "\n",
    "$$L_{\\text{hinge}}(W,Y^i,X^i)=\\max(0,m+E(W,Y^i,X^i))-E(W,\\bar Y^i,X^i)$$\n",
    "\n",
    "<p>Where $\\bar Y^i$ is the <em>most offending incorrect answer</em>. This loss enforces that the difference between the correct answer and the most offending incorrect answer be at least $m$.</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/hinge.png\" width=\"30%\" /><br />\n",
    "<b>Fig. 4</b>: Hinge Loss\n",
    "</center>\n",
    "\n",
    "<p>Q: How do you pick $m$?</p>\n",
    "\n",
    "<p>A: It’s arbitrary, but it affects the weights of the last layer.</p>\n",
    "\n",
    "<h3 id=\"log-loss\">Log Loss</h3>\n",
    "\n",
    "$$L_{\\log}(W,Y^i,X^i)=\\log(1+e^{E(W,Y^i,X^i)-E(W,\\bar Y^i,X^i)})$$\n",
    "\n",
    "<p>This can be thought of as a “soft” hinge loss. Instead of composing the difference of the correct answer and the most offending incorrect answer with a hinge, it’s now composed with a soft hinge. This loss tries to enforce an “infinite margin”, but because of the exponential decay of the slope it doesn’t happen.</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/log.png\" width=\"30%\" /><br />\n",
    "<b>Fig. 5</b>: Log Loss\n",
    "</center>\n",
    "\n",
    "<h3 id=\"square-square-loss\">Square-Square Loss</h3>\n",
    "\n",
    "$$L_{sq-sq}(W,Y^i,X^i)=E(W,Y^i,X^i)^2+(\\max(0,m-E(W,\\bar Y^i,X^i)))^2$$\n",
    "\n",
    "<p>This loss combines the square of the energy with a square hinge. The combination tries to minimize the energy and but enforce margin at least $m$ on the most offending incorrect answer. This is very similar to the loss used in Siamese nets.</p>\n",
    "\n",
    "<h2 id=\"other-losses\">Other Losses</h2>\n",
    "\n",
    "<p>There are a whole bunch. Here is a summary of good and bad loss functions.</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/other.png\" width=\"30%\" style=\"background-color:white;\" /><br />\n",
    "<b>Fig. 6</b>: Selection of EBM loss functions\n",
    "</center>\n",
    "\n",
    "<p>The right-hand column indicates if the energy function enforces a margin. The plain old energy loss does not push up anywhere, so it doesn’t have a margin. The energy loss doesn’t work for every problem. The perceptron loss works if you have a linear parametrisation of your energy but not in general. Some of them have a finite margin like the hinge loss, and some have an infinite margin like the soft hinge.</p>\n",
    "\n",
    "<p>Q: How is the most offending incorrect answer found $\\bar Y_i$ found in the continuous case?</p>\n",
    "\n",
    "<p>A: You want to push up on a point that is sufficiently far from $Y^i$, because if it’s too close, the parameters may not move much since the function defined by a neural net is “stiff”. But in general, this is hard and this is the problem that methods selecting contrastive samples try to solve. There’s no single correct way to do it.</p>\n",
    "\n",
    "<p>A slightly more general form for hinge type contrastive losses is:</p>\n",
    "\n",
    "$$L(W,X^i,Y^i)=\\sum_y H(E(W, Y^i,X^i)-E(W,y,X^i)+C(Y^i,y))$$\n",
    "\n",
    "<p>We assume that $Y$ is discrete, but if it were continuous, the sum would be replaced by an integral. Here, $E(W, Y^i,X^i)-E(W,y,X^i)$ is the difference of $E$ evaluated at the correct answer and some other answer. $C(Y^i,y)$ is the margin, and is generally a distance measure between $Y^i$ and $y$. The motivation is that the amount we want to push up on a incorrect sample $y$ should depend on the distance between $y$ and the correct sample $Y_i$. This can be a more difficult loss to optimize.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
