{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vacV4BIFI8l"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip install -q --upgrade nltk gensim bokeh pandas\n",
    "\n",
    "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
    "!unzip quora.zip\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIFSTdJG95SZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbpWIAreB6ky"
   },
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_M0mMOadG8aZ"
   },
   "source": [
    "PyTorch is one of the most famous frameworks for working with neural networks.\n",
    "\n",
    "Why precisely he? Well, it's nyashan, pitonach and easier to debug - compared to tensoflow type monsters (although tf 2.0 with eager execution will be about the same).\n",
    "\n",
    "And in general, we are not frameworks here, but we were going to learn grids :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsScdJ7DLZCm"
   },
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bY9FHLM-M4aW"
   },
   "source": [
    "### Графы вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkvCloDpNXdH"
   },
   "source": [
    "Calculation graphs are such a convenient way to quickly calculate the gradients of complex-implicit functions.\n",
    "\n",
    "For example, the function\n",
    "\n",
    "$$f = (x + y) \\cdot z$$\n",
    "\n",
    "introduce yourself as a graph\n",
    "\n",
    "![graph](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Circuit.png =500x)  \n",
    "*From [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/)*\n",
    "\n",
    "**Task** Set the values ​​of $ x, y, z $ (green in the picture). How to calculate $ \\ frac {\\ partial f} {\\ partial x}, \\ frac {\\ partial f} {\\ partial y}, \\ frac {\\ partial f} {\\ partial z} $? (* Recall what backpropagation is *)\n",
    "\n",
    "In PyTorch, such calculations are made very simply.\n",
    "\n",
    "First, a function is defined - just a sequence of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw4ASRktLdO4"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(-2., requires_grad=True)\n",
    "y = torch.tensor(5., requires_grad=True)\n",
    "z = torch.tensor(-4., requires_grad=True)\n",
    "\n",
    "q = x + y\n",
    "f = q * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-78COM99N8YL"
   },
   "source": [
    "\n",
    "![graph](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)  \n",
    "*From [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)*\n",
    "\n",
    "According to the described sequence of operations * on the fly * a computation graph is constructed, and the backward pass is performed on it.\n",
    "\n",
    "This is a key difference from tensoflow: the graph does not need to be compiled before the code is executed - this allows you to manage its structure more flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FOlPMIQMfbq"
   },
   "outputs": [],
   "source": [
    "f.backward()\n",
    "\n",
    "print('df/dz =', z.grad)\n",
    "print('df/dx =', x.grad)\n",
    "print('df/dy =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JotDf1naGU-R"
   },
   "source": [
    "Calling the method `backward ()` calculates gradients for all tensors that have `requires_grad == True`.\n",
    "\n",
    "There is another alternative way to not calculate gradients - use context managers. ([Locally disabling gradient computation](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation)):\n",
    "```python\n",
    "torch.autograd.no_grad()\n",
    "torch.autograd.enable_grad()\n",
    "torch.autograd.set_grad_enabled(mode)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQEJeqfnJPpA"
   },
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    x = torch.tensor(-2., requires_grad=True)\n",
    "    y = torch.tensor(5., requires_grad=True)\n",
    "    q = x + y\n",
    "\n",
    "z = torch.tensor(-4., requires_grad=True)\n",
    "f = q * z\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print('df/dz =', z.grad)\n",
    "print('df/dx =', x.grad)\n",
    "print('df/dy =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSiB1CGyJMzt"
   },
   "source": [
    "Read more about how autograd works, read here: [Autograd mechanics] (https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "In general, any tensor in pytorch is analogous to multidimensional matrices in numpy.\n",
    "\n",
    "It contains data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DY2CcCw2Gmgq"
   },
   "outputs": [],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYxD8N_9GpJl"
   },
   "source": [
    "Накопленный градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYCD5P24GufX"
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwLx4szvGwMb"
   },
   "source": [
    "Функцию, как градиент считать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTfGdUF_GzV8"
   },
   "outputs": [],
   "source": [
    "q.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgK1Esa6HHAB"
   },
   "source": [
    "И всякую дополнительную метаинформацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nazaer0AG4pL"
   },
   "outputs": [],
   "source": [
    "x.type(), x.shape, x.device, x.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvLFlc4iQOQv"
   },
   "source": [
    "Зачем... У меня один вопрос - зачем вот это вот нам нужно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlhLBWwHG3Xe"
   },
   "source": [
    "### Задача для разминки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaqtIIvJOEut"
   },
   "source": [
    "Чтобы разобраться - решим простенькую задачу на линейную регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDZpEHF8AKH2"
   },
   "outputs": [],
   "source": [
    "w_orig, b_orig = 2.6, -0.4\n",
    "\n",
    "X = np.random.rand(100) * 10. - 5.\n",
    "y_orig = w_orig * X + b_orig\n",
    "\n",
    "y = y_orig + np.random.randn(100)\n",
    "\n",
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, y_orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2K5MVtiSGuC"
   },
   "source": [
    "I want to fasten backpropagation here, yes.\n",
    "\n",
    "There are two parameters $ w $ and $ b $ - they need to be chosen so that they are as close as possible to the original $ w_ {orig}, b_ {orig} $.\n",
    "\n",
    "What will we optimize? We will optimize MSE:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - y_i(w, b)||^2 =\\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - (w \\cdot x_i + b)||^2. $$\n",
    "\n",
    "With such loss functions, we can run a simple gradient descent (not even stochastic yet):\n",
    "\n",
    "$$w_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial w}(w_t, b_t)$$\n",
    "$$b_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial b}(w_t, b_t)$$\n",
    "\n",
    "**Task** Implement optimization on pure numpy.\n",
    "\n",
    "For this you need:\n",
    "1. Calculate the value of the function on the forward pass: $ y (w, b) = w \\ cdot x + b $;\n",
    "2. To think and calculate the gradients $\\frac{\\partial J} {\\partial w}, \\frac {\\partial J} {\\partial b}$ on the back pass;\n",
    "3. Shift $w, b$ by antigradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKbqTNVXFB3A"
   },
   "outputs": [],
   "source": [
    "def display_progress(epoch, loss, w, b, X, y, y_pred):\n",
    "    clear_output(True)\n",
    "    print('Epoch = {}, Loss = {}, w = {}, b = {}'.format(epoch, loss, w, b))\n",
    "    plt.plot(X, y, '.')\n",
    "    plt.plot(X, y_pred)\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred = <calc it>\n",
    "\n",
    "    loss = <and it>\n",
    "\n",
    "    <find w_grad and b_grad>\n",
    "\n",
    "    w -= alpha * w_grad\n",
    "    b -= alpha * b_grad\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        display_progress(i + 1, loss, w, b, X, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8WgWrF4C2WK"
   },
   "source": [
    "On PyTorch, the same thing is somewhat simpler - the calculation of the straight passage is copied almost verbatim.\n",
    "\n",
    "We already know how to go back - you just need to call `loss.backward ()`.\n",
    "\n",
    "To update `w` and` b` you need to keep in mind the following. First, pytorch won't just update them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx4DoGeBMJd4"
   },
   "outputs": [],
   "source": [
    "w = torch.randn(1, requires_grad=True)\n",
    "\n",
    "w -= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OjoUh-SMPBt"
   },
   "source": [
    "The problem is the difficulty of supporting in-place operations for autograd ([in place operations with autograd] (https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd))\n",
    "\n",
    "But we do not need support for gradients! We will not do a backward pass through this operation - you just need to update the value of the variable. To do this, you can use the context `no_grad`, or you can update the buffer that the tensor uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zegkKd-cMOMj"
   },
   "outputs": [],
   "source": [
    "w.data -= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVlaIdvHNXR_"
   },
   "source": [
    "Another thing to remember is that the gradients in the tensors accumulate. Between calls to `loss.backward ()` we need to reset the gradients of `w` and` b`:\n",
    "\n",
    "```python\n",
    "w.grad.zero_ ()\n",
    "b.grad.zero_ ()\n",
    "```\n",
    "\n",
    "**Task** Implement linear regression on pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRqxypuEU2ig"
   },
   "outputs": [],
   "source": [
    "X = torch.as_tensor(X).float()\n",
    "y = torch.as_tensor(y).float()\n",
    "\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "for i in range(100):\n",
    "    <copy forward pass and add backward pass + parameters updates>\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        display_progress(i + 1, loss, w.item(), b.item(), \n",
    "                         X.data.numpy(), y.data.numpy(), y_pred.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaKTKN_fOvo-"
   },
   "source": [
    "Думать нужно уже гораздо меньше, да? :)\n",
    "\n",
    "Про другие фишки низкоуровнего pytorch можно почитать здесь: [PyTorch — ваш новый фреймворк глубокого обучения](https://habr.com/post/334380/) (статья веселая, но немного устарела, читать лучше с оглядкой на [PyTorch 0.4.0 Migration Guide](https://pytorch.org/blog/pytorch-0_4_0-migration-guide/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZNq6ujzPtvd"
   },
   "source": [
    "## Word embeddings и высокоуровневый API PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITLgcVz66AfV"
   },
   "source": [
    "Займёмся рассмотрением высокоуровневого API - в нем уже реализованы разные классы-запчасти для обучения нейронок.\n",
    "\n",
    "Будем решать всё ту же задачу, что и в прошлый раз - обучение словных эмбеддингов, только теперь мы будем учить их самостоятельно!\n",
    "\n",
    "Для начала нужно подготовить данные для обучения.\n",
    "\n",
    "Соберем и токенизируем тексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKKb9Ya8hzIb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quora_data = pd.read_csv('train.csv')\n",
    "\n",
    "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
    "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
    "\n",
    "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
    "\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYoj91iDDDfT"
   },
   "source": [
    "Соберем индекс самых частотных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PL471pGjuVN"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MIN_COUNT = 5\n",
    "\n",
    "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
    "word2index = {\n",
    "    '<unk>': 0\n",
    "}\n",
    "\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < MIN_COUNT:\n",
    "        break\n",
    "        \n",
    "    word2index[word] = len(word2index)\n",
    "    \n",
    "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
    "    \n",
    "print('Vocabulary size:', len(word2index))\n",
    "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
    "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
    "print('Most freq words:', index2word[1:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DF5mYpCsE9Uh"
   },
   "source": [
    "### Skip-Gram Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "om1IG5XEMGRa"
   },
   "source": [
    "Начнем с skip-gram модели обучения word2vec.\n",
    "\n",
    "Это простая модель всего из двух слоев. Ее идея - учить вектора эмбеддингов такими, чтобы по ним можно было как можно лучше предсказать контекст соответствующих слов. То есть если мы хорошо научились кодировать слова, с которыми встречается данное - значит, мы что-то знаем и о нем самом. Например, естественным образом получится, что слова, встречающиеся в одинаковых контекстах (скажем, `apple` и `orange`)  будут иметь близкие вектора эмбеддингов.\n",
    "\n",
    "![](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Word2vecExample.jpeg =600x)  \n",
    "*From cs224n, Lecture 2*\n",
    "\n",
    "Для этого мы моделируем вероятности $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, где $k$ - размер контекстного окна, $c$ - индекс центрального слова.\n",
    "\n",
    "Соберем такую модель: будем учить пару матриц $U$ - матрицу эмбеддингов, которую потом и возьмем для своих задач, и $V$ - матрицу выходного слоя.\n",
    "\n",
    "Каждому слову в словаре соответствует строка в матрице $U$ и столбец $V$.\n",
    "\n",
    "![skip-gram](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/SkipGram.png =500x)\n",
    "\n",
    "Что тут происходит? Слово отображается в эмбеддинг - строку $u_c$. Дальше этот эмбеддинг умножается на матрицу $V$. \n",
    "\n",
    "В итоге получаем набор числе $v_j^T u_c$ - степень похожести слова с номером $j$ и нашего слова.\n",
    "\n",
    "Преобразуем эти числа в что-то вроде вероятностей - воспользуемся функцией softmax: $P(i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$.\n",
    "\n",
    "А дальше будем считать кросс-энтропийные потери:\n",
    "\n",
    "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
    "\n",
    "В итоге, вектор $u_c$ будет приближаться к векторам $v_{c_j}$ из его контекста.\n",
    "\n",
    "Реализуем это всё, чтобы разобраться.\n",
    "\n",
    "#### Генерация батчей\n",
    "\n",
    "Для начала нужно собрать контексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocrsXgaynYPG"
   },
   "outputs": [],
   "source": [
    "def build_contexts(tokenized_texts, window_size)\n",
    "    contexts = []\n",
    "    for tokens in tokenized_texts:\n",
    "        for i in range(len(tokens)):\n",
    "            central_word = tokens[i]\n",
    "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n",
    "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
    "\n",
    "            contexts.append((central_word, context))\n",
    "            \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQBa6yQ9BXjp"
   },
   "outputs": [],
   "source": [
    "contexts = build_contexts(tokenized_texts, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyQNK-9SBdb9"
   },
   "outputs": [],
   "source": [
    "contexts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbQKln_6yC4l"
   },
   "source": [
    "Преобразуем слова в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOPRlKlLvUBA"
   },
   "outputs": [],
   "source": [
    "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n",
    "            for central_word, context in contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYmrAi9gyIe-"
   },
   "source": [
    "Реализуем генератор батчей для нашей нейронки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6opX5cEp8LxC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_skip_gram_batchs_iter(contexts, window_size, num_skips, batch_size):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * window_size\n",
    "    \n",
    "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    \n",
    "    batch_size = int(batch_size / num_skips)\n",
    "    batchs_count = int(math.ceil(len(contexts) / batch_size))\n",
    "    \n",
    "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(len(contexts))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(batchs_count):\n",
    "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
    "            batch_indices = indices[batch_begin: batch_end]\n",
    "\n",
    "            batch_data, batch_labels = [], []\n",
    "\n",
    "            for data_ind in batch_indices:\n",
    "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
    "                \n",
    "                words_to_use = random.sample(context, num_skips)\n",
    "                batch_data.extend([central_word] * num_skips)\n",
    "                batch_labels.extend(words_to_use)\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0D79MwB_gMe"
   },
   "outputs": [],
   "source": [
    "batch, labels = next(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=2, batch_size=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DXjZS3JyQZh"
   },
   "source": [
    "#### nn.Sequential\n",
    "\n",
    "Простейший способ реализовать модель на PyTorch - использовать модуль `nn.Sequential`. В нем нужно просто перечислить все слои, и он будет применять их последовательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRw9Z4G__46O"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Embedding(len(word2index), 32),\n",
    "    nn.Linear(32, len(word2index))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysn0DDpLyj1S"
   },
   "source": [
    "Еще одна особенность pytorch, о которой до сих пор не говорили - поддержка вычислений на видеокарте. На видеокарте большинство нейронок считается гораздо быстрее благодаря высокой параллелизации. Сказать pytorch'у, чтобы он считал на видеокарте, очень просто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfmaUi3Uy9YT"
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3c3UEa2zHhk"
   },
   "source": [
    "либо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHxAg5ZWzEKT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHi1CL2pzxOg"
   },
   "source": [
    "Создать тензоры на видеокарте можно, например, так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ycx1O3_SzvmC"
   },
   "outputs": [],
   "source": [
    "batch = torch.cuda.LongTensor(batch)\n",
    "labels = torch.cuda.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtLMvOO2z3c8"
   },
   "source": [
    "Заставить модель посчитать значение можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9wTpewTz3Dk"
   },
   "outputs": [],
   "source": [
    "logits = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWJmDy_uzJgD"
   },
   "source": [
    "Теперь нам нужна функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7rlD62_ykYl"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss().cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxLBiBua0OZM"
   },
   "source": [
    "Посчитать значение можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCaTB5cc0GVw"
   },
   "outputs": [],
   "source": [
    "loss = loss_function(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAwx-pck0RxX"
   },
   "source": [
    "А теперь, конечно же, backprop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWt6gL0_0Npp"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJkDOl6szRLm"
   },
   "source": [
    "И, наконец, оптимизатор.\n",
    "\n",
    "Будем использовать Adam. Интерфейс - передать список оптимизируемых параметров и learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-b5CIARzQ6m"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju5lO0Xi0hsV"
   },
   "source": [
    "Оптимизация идет просто - нужно вызвать `step()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9QK7nHu0Zw8"
   },
   "outputs": [],
   "source": [
    "print(model[1].weight)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(model[1].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hnxyk1ew0pSk"
   },
   "source": [
    "И последнее - нужно обнулить градиенты!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMsuvEP90svi"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PibTw33Azg7q"
   },
   "source": [
    "#### Реализация обучения skip-gram модели\n",
    "\n",
    "Наконец, напишем цикл обучения - как уже было с линейной регрессией.\n",
    "\n",
    " **Задание** Заполните цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewGMgYTXANzz"
   },
   "outputs": [],
   "source": [
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
    "    <1. convert data to tensors>\n",
    "    \n",
    "    <2. make forward pass>\n",
    "\n",
    "    <3. make backward pass>\n",
    "\n",
    "    <4. apply optimizer>\n",
    "    \n",
    "    <5. zero grads>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pqq9kee41L4P"
   },
   "source": [
    "#### Анализ\n",
    "\n",
    "Получить эмбеддинги можно, скаставав такое заклинание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWsYkNn-Hnl_"
   },
   "outputs": [],
   "source": [
    "embeddings = model[0].weight.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZtxY2D01RB6"
   },
   "source": [
    "Проверим, получилось ли хоть сколько-то адекватно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhDwuhDSHEDm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(embeddings, index2word, word2index, word):\n",
    "    word_emb = embeddings[word2index[word]]\n",
    "    \n",
    "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
    "    top10 = np.argsort(similarities)[-10:]\n",
    "    \n",
    "    return [index2word[index] for index in reversed(top10)]\n",
    "\n",
    "most_similar(embeddings, index2word, word2index, 'warm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VS1x-mO1WKS"
   },
   "source": [
    "И визуализируем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuXv2HxsAecb"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, index2word, word_count):\n",
    "    word_vectors = embeddings[1: word_count + 1]\n",
    "    words = index2word[1: word_count + 1]\n",
    "    \n",
    "    word_tsne = get_tsne_projection(word_vectors)\n",
    "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
    "    \n",
    "    \n",
    "visualize_embeddings(model[0], index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGfhLR6x8D3r"
   },
   "source": [
    "### Continuous Bag of Words (CBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UuVr2IsaYhX"
   },
   "source": [
    "Alternative model:\n",
    "\n",
    "! [] (https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/CBOW.png = 500x)\n",
    "\n",
    "Now, by the sum of the context vectors, the vector of the central word is predicted.\n",
    "\n",
    "** Task ** Implement part of the function to generate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NP5VmnnjtsXn"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fe791f4bb2d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fe791f4bb2d9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Alternative model:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Alternative model:\n",
    "\n",
    "! [] (https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/CBOW.png = 500x)\n",
    "\n",
    "Now, by the sum of the context vectors, the vector of the central word is predicted.\n",
    "\n",
    "** Task ** Implement part of the function to generate batches.def make_cbow_batchs_iter(contexts, window_size, batch_size):\n",
    "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "        \n",
    "    batchs_count = int(math.ceil(len(data) / batch_size))\n",
    "    \n",
    "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
    "    \n",
    "    while True:\n",
    "        <do batchs generation>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9HkY-VO61n3"
   },
   "source": [
    "Посмотрим на альтернативный вариант создания модели - им мы будем пользоваться чаще всего - отнаследоваться от `nn.Module`. Схематично её использование выглядит так:\n",
    "\n",
    "```python\n",
    "class MyNetModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyNetModel, self).__init__()\n",
    "        <initialize layers>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply layers>\n",
    "        return final_output\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mHKLbMwx4c5"
   },
   "outputs": [],
   "source": [
    "class CBoWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply layers>\n",
    "        return output\n",
    "      \n",
    "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "loss_function = <create loss function>\n",
    "optimizer = <create optimizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiIgaofEyyJ1"
   },
   "outputs": [],
   "source": [
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <copy-paste learning cycle>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTEWcyYmvips"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CON4VOyG3iET"
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "What is the hardest thing now? Calculating softmax and applying gradients to all words in $ V $.\n",
    "\n",
    "One way to handle this is to use * Negative Sampling *.\n",
    "\n",
    "In fact, instead of predicting the index of a word by context, it is predicted that such a word $ w $ can be in this context $ c $: $ P (D = 1 | w, c) $.\n",
    "\n",
    "You can use a regular sigmoid to get this probability:\n",
    "$$ P (D = 1 | w, c) = \\sigma (v_w ^ T u_c) = \\frac 1 {1 + \\exp (-v ^ T_w u_c)}. $$\n",
    "\n",
    "The learning process then looks like this: for each pair, the word and its context generate a set of negative examples:\n",
    "\n",
    "![Negative Sampling](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Negative_Sampling.png =350x)\n",
    "\n",
    "For CBoW, the loss function will look like this:\n",
    "\n",
    "$$ - \\log \\sigma (v_c ^ T u_c) - \\sum_ {k = 1} ^ K \\log \\sigma (- \\tilde v_k ^ T u_c), $$\n",
    "\n",
    "where $ v_c $ is the vector of the central word, $ u_c $ is the context vector (sum of context vectors), $ \\tilde v_1, \\ldots, \\tilde v_K $ are the sampled negative examples.\n",
    "\n",
    "Compare this formula with the usual CBoW:\n",
    "$$ - v_c ^ T u_c + \\log \\sum_ {i = 1} ^ {| V |} \\exp (v_i ^ T u_c). $$\n",
    "\n",
    "Usually words are sampled from $ U ^ {3/4} $, where $ U $ is the unigram distribution, that is, the frequency of occurrence of words divided by the total number of words.\n",
    "\n",
    "Frequencies we have already considered: they are obtained in `Counter (words)`. Simply convert them to probabilities and multiply these probabilities by $ \\frac 3 4 $. Why $ \\frac 3 4 $? Some intuition can be found in the following example:\n",
    "\n",
    "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
    "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
    "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
    "\n",
    "The probability for high-frequency words is not particularly increased (relatively), but low-frequency ones will fall out with a noticeably greater probability.\n",
    "\n",
    "**Task** Implement your Negative Sampling.\n",
    "\n",
    "First, let's set the distribution for sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcX4vRBLlXy6"
   },
   "outputs": [],
   "source": [
    "words_sum_count = sum(words_counter.values())\n",
    "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
    "# Вообще-то, тут нечестно сделанно, можно лучше\n",
    "word_distribution /= word_distribution.sum()\n",
    "\n",
    "indices = np.arange(len(word_distribution))\n",
    "\n",
    "np.random.choice(indices, p=word_distribution, size=(32, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o2pzsue16Lu"
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets, num_samples):\n",
    "        '''\n",
    "        inputs: (batch_size, context_size)\n",
    "        targets: (batch_size)\n",
    "        num_samples: int\n",
    "        '''\n",
    "        \n",
    "        <calculate u_c's>\n",
    "        \n",
    "        <calculate v_c>\n",
    "        \n",
    "        <sample indices>\n",
    "        <calculate negative vectors v'_c>\n",
    "        \n",
    "        <apply F.logsigmoid to v_c * u_c and to -v'_c * u_c>\n",
    "        \n",
    "        <calc result loss>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wz2iRanqzlq"
   },
   "outputs": [],
   "source": [
    "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <copy-paste (mostly) learning cycle>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFik_6djvg3F"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4G2X-TTpzwz"
   },
   "source": [
    "### Structured Word2Vec\n",
    "\n",
    "**Задание** В статье [Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf) рассматриваются два варианта улучшения эмбеддингов - *Structured Skip-gram Model* и *Continuous Window Model*:   \n",
    "![](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/StructuredWord2vec.png =600x)  \n",
    "*From Two/Too Simple Adaptations of Word2Vec for Syntax Problems*\n",
    "\n",
    "Отличие - матрицы для каждого слова контекста учатся свои. Это хорошо на больших корпусах, но на нашем маленьком зайдет не слишком хорошо - многовато параметров придется выучить.\n",
    "\n",
    "Идея этого в том, что порядок слов в предложении очень важен (особенно в английском, на котором они как всегда тестируются). Задавая порядок, они лучше учатся синтаксису.\n",
    "\n",
    "Почитайте статью и попробуйте реализовать один из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqDmuu7m_PB5"
   },
   "source": [
    "# Дополнительные материалы\n",
    "## Почитать\n",
    "### Блоги\n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
    "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) \n",
    "\n",
    "### Статьи\n",
    "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
    "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
    "\n",
    "### Улучшение эмбеддингов\n",
    "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
    "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
    "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
    "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
    "\n",
    "### Эмбеддинги предложений\n",
    "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
    "\n",
    "### Backpropagation\n",
    "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
    "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "## Посмотреть\n",
    "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
    "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM9C1i3Y-6kv"
   },
   "source": [
    "# Сдача задания\n",
    "\n",
    "[Сдача](https://goo.gl/forms/rzWjQQsGpqYNz5yt1)  \n",
    "[Опрос](https://goo.gl/forms/as640TWE058bFTpy2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 03 - Word Embeddings (Part 2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
