{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8LbImWBw4Zd"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip install -qq bokeh==0.13.0\n",
    "!pip install -qq eli5==0.8\n",
    "!wget -O surnames.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF1fio53UKN6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3lN5pl5Stpp"
   },
   "source": [
    "# Свёрточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J6KBv-cniLw"
   },
   "source": [
    "## Классификация фамилий\n",
    "\n",
    "Будем учиться предсказывать, является ли слово фамилией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7f2Sk0mMFJeW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('surnames.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    data = [line.strip().split('\\t')[0] for line in lines]\n",
    "    labels = np.array([int(line.strip().split('\\t')[1]) for line in lines])\n",
    "    del lines\n",
    "    \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfUIgqMznrXB"
   },
   "source": [
    "Посмотрим на данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YN1XYen8UauD"
   },
   "outputs": [],
   "source": [
    "list(zip(train_data, train_labels))[::1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFlOaCNt3fuV"
   },
   "source": [
    "Данные ещё и сильно несбалансированы - положительных примеров в несколько раз меньше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikKB2DaK08-n"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "positive_count = np.sum(train_labels == 1)\n",
    "negative_count = len(train_labels) - positive_count\n",
    " \n",
    "plt.bar(np.arange(2), [negative_count, positive_count], align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(2), ('Negative', 'Positive'))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MwgOy5M8qPf"
   },
   "source": [
    "Accuracy очень легко оптимизировать - просто предсказывайте всегда ноль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuqFWWu38UwV"
   },
   "outputs": [],
   "source": [
    "print('Accuracy = {:.2%}'.format((train_labels == 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtOFSmy93pDl"
   },
   "source": [
    "Однако это будет довольно бесполезно - всегда говорить, что слово не является фамилией. Это, конечно, вопрос - что хуже, зря объявить слово фамилией (ошибка первого рода) или не найти фамилию.\n",
    "\n",
    "<img src=\"https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg\" style=\"border:none;width:35%\">\n",
    "\n",
    "Будем замерять precision, recall и их комбинацию - $F_1$-меру.\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" style=\"border:none;width:35%\">\n",
    "\n",
    "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
    "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bl6O_bAtVqd"
   },
   "source": [
    "Начнём с бейзлайна на регулярках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Q6tUhBLUy0Wf"
   },
   "outputs": [],
   "source": [
    "#@title Супер-бейзлайн\n",
    "surname_indicators = \"^[А-Я][а-я], .*ский\" #@param {type:\"raw\"}\n",
    "\n",
    "surname_indicators = surname_indicators.split(', ')\n",
    "\n",
    "import re\n",
    "\n",
    "regexs = [re.compile(regex) for regex in surname_indicators]\n",
    "\n",
    "preds = np.array([any(regex.match(word) for regex in regexs) for word in test_data])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score = {:.2%}'.format(f1_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJJtiGMK1ATy"
   },
   "source": [
    "А теперь серьёзно - бейзлайн на логистической регрессии поверх N-грамм символов.\n",
    "\n",
    "**Задание** Сделать классификацию с LogisticRegression моделью. Посчитать F1-меру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8COAoh7b0TXs"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\n",
    "\n",
    "X_train, X_test = <convert data>\n",
    "\n",
    "model = <fit LogisticRegression>\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "print('F1-score = {:.2%}'.format(f1_score(test_labels, test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN46GwPrzeJW"
   },
   "source": [
    "Посмотрим на предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-P8OSD7yZQt"
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(model, vec=vectorizer, top=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYXljseDyd77"
   },
   "outputs": [],
   "source": [
    "sample_ind = np.random.randint(len(test_data))\n",
    "eli5.show_prediction(model, test_data[sample_ind], vec=vectorizer, targets=['surname'], target_names=['word', 'surname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQF3JlD7twQK"
   },
   "source": [
    "Кроме тупого подсчета F1-score можно посмотреть на precision-recall кривые. Во-первых, они красивые. Во-вторых, по ним видно, что можно повысить качество (F1-score), подобрав другой порог - **хотя на тесте это делать нельзя**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7oyvJ-2EG0m"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test_labels, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1 = {0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tL6Bnl8ftoYP"
   },
   "source": [
    "**Задание** Придумайте признаки, чтобы улучшить качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRhcCJcAS4S5"
   },
   "source": [
    "## Character-Level Convolutions\n",
    "\n",
    "### Общее описание сверток\n",
    "\n",
    "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
    "\n",
    "Работают они примерно так:  \n",
    "![Conv example](https://image.ibb.co/e6t8ZK/Convolution.gif)   \n",
    "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 & 1 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  1 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
    "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
    "\n",
    "\n",
    "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg =x300)  \n",
    "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-M3lCE1ealB"
   },
   "source": [
    "### Свёртки для текстов\n",
    "\n",
    "Для текстов свертки работают как n-граммные детекторы (примерно). Каноничный пример символьной сверточной сети:\n",
    "\n",
    "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png =x500)  \n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "*Сколько учится фильтров на данном примере?*\n",
    "\n",
    "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
    "\n",
    "Что это значит, если конкретнее?\n",
    "\n",
    "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
    "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
    "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
    "\n",
    "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
    "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
    "\n",
    "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
    "\n",
    "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz7ApwLZ05kc"
   },
   "source": [
    "### Игрушечный пример\n",
    "\n",
    "Посмотрим на примере, что там происходит. Возьмем слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7fbpo630iZa"
   },
   "outputs": [],
   "source": [
    "word = 'Смирнов'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1R4gvJVD0-0f"
   },
   "source": [
    "Для начала нужно перенумеровать символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x99Q65ot02Vi"
   },
   "outputs": [],
   "source": [
    "char2index = {symb: ind for ind, symb in enumerate(set(word))}\n",
    "\n",
    "char2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sxJYZEs1M8y"
   },
   "source": [
    "Каждому символу сопоставляется эмбеддинг. Простейший способ сделать эмбеддинги - взять единичную матрицу. Когда у нас были десятки тысяч слов, такие эмбеддинги были не оч, а сейчас всего несколько символов вполне адекватно присвоить им ортогональные вектора небольшой размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbApeJdb1EGq"
   },
   "outputs": [],
   "source": [
    "embeddings = torch.eye(len(char2index))\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP9i5pPx2BGs"
   },
   "source": [
    "Построим тензор индексов символов слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBIIlHy91ohP"
   },
   "outputs": [],
   "source": [
    "word_tensor = torch.LongTensor([char2index[symb] for symb in word])\n",
    "\n",
    "word_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL3qLw-p2Htk"
   },
   "source": [
    "Отобразим его в эмбеддинги. Получили такой же прямоугольничек, как на картинке (транспонирование нужно, чтобы смотрело в ту же сторону)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fkChxtt1yqY"
   },
   "outputs": [],
   "source": [
    "word_embs = embeddings[word_tensor].t()\n",
    "\n",
    "word_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL4o1EI-2O_I"
   },
   "source": [
    "Теперь дело дошло до сверток. Сделаем фильтр-детектор триграммы `нов`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToW8KCUY1723"
   },
   "outputs": [],
   "source": [
    "kernel_name = 'нов'\n",
    "\n",
    "kernel_indices = torch.LongTensor([char2index[symb] for symb in kernel_name])\n",
    "kernel_weights = embeddings[kernel_indices].t()\n",
    "\n",
    "kernel_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTxfchOy4c9w"
   },
   "source": [
    "Чтобы посчитать свёртку, воспользуемся функцией:\n",
    "\n",
    "```python\n",
    "F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "```\n",
    "\n",
    "input: input tensor of shape ($N \\times C_{in} \\times H_{in} \\times W_{in}$)  \n",
    "weight: filters of shape ($C_{out} \\times C_{in} \\times H_{out} \\times W_{out}$)\n",
    "\n",
    "$N$ - размер батча (1 у нас). $C_{in}$ - число каналов. В нашем случае оно всегда будет 1 (пока что). $C_{out}$ - число фильтров. Оно пока 1.\n",
    "\n",
    "Нам понадобятся четырехмерные тензоры, для этого воспользуемся `view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyd3Na7V3JMr"
   },
   "outputs": [],
   "source": [
    "word_embs = word_embs.view(1, 1, word_embs.shape[0], word_embs.shape[1])\n",
    "kernel_weights = kernel_weights.view(1, 1, kernel_weights.shape[0], kernel_weights.shape[1])\n",
    "\n",
    "conv_result = F.conv2d(word_embs, kernel_weights)[0, 0]\n",
    "\n",
    "print('Conv =', conv_result)\n",
    "print('Max pooling =', conv_result.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVAiQ2PG6SOd"
   },
   "source": [
    "Свертка сказала, что данный фильтр есть на последней позиции. Пулинг сказал, пофиг на какой позиции - главное, он есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7ZVbNOGoV5z"
   },
   "source": [
    "### Подготовка данных\n",
    "\n",
    "Первый шаг - определить, какой длины слова у нас. Ограничимся каким-то числом, а более длинные будем обрезать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEtZ6g78Wtj-"
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "    \n",
    "def find_max_len(counter, threshold):\n",
    "    sum_count = sum(counter.values())\n",
    "    cum_count = 0\n",
    "    for i in range(max(counter)):\n",
    "        cum_count += counter[i]\n",
    "        if cum_count > sum_count * threshold:\n",
    "            return i\n",
    "    return max(counter)\n",
    "\n",
    "word_len_counter = Counter()\n",
    "for word in train_data:\n",
    "    word_len_counter[len(word)] += 1\n",
    "    \n",
    "threshold = 0.99\n",
    "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
    "\n",
    "print('Max word length for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdKoOBdE8uj4"
   },
   "source": [
    "Соберем отображение из символов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyMoPEXGVs3s"
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for word in train_data:\n",
    "    chars.update(word)\n",
    "\n",
    "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
    "char_index['<pad>'] = 0\n",
    "    \n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4spzGF2CImtL"
   },
   "source": [
    "**Задание** Сконвертируйте данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qETmYKm8W_TX"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, max_word_len, char_index):\n",
    "    return <np array>\n",
    "\n",
    "X_train = convert_data(train_data, MAX_WORD_LEN, char_index)\n",
    "X_test = convert_data(test_data, MAX_WORD_LEN, char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VufrP006Vk-y"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        \n",
    "        batch_idx = indices[start: end]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkXlKB7lobYE"
   },
   "source": [
    "### MyFirstConvCharNN\n",
    "\n",
    "Теперь построим свёрточную модель.\n",
    "\n",
    "Пусть она будет строить триграммы - то есть применять фильтры на 3 символа.\n",
    "\n",
    "Начнем с последовательности: `nn.Embedding -> nn.Conv2d -> nn.ReLU -> max pooling -> nn.Linear`\n",
    "\n",
    "`nn.Conv2d` - это слой, содержащий создание и инициализацию фильтров, и вызов `F.conv2d` к ним и входу.\n",
    "\n",
    "*Лайфхак:* последовательности операций можно запаковывать в `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2T4AorsZ530"
   },
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embedding = ...\n",
    "        self._dropout = nn.Dropout(0.2)\n",
    "        self._conv3 = ...\n",
    "        self._out_layer = ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs - LongTensor with shape (batch_size, max_word_len)\n",
    "        outputs - FloatTensor with shape (batch_size,)\n",
    "        '''\n",
    "        \n",
    "        outputs = self.embed(inputs)\n",
    "        return self._out_layer(outputs).squeeze(-1)\n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        <calc word embedding>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iTDc9rEAHN6"
   },
   "source": [
    "Проверьте, что всё работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiAdrSaU_wjQ"
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iterate_batches(X_train, train_labels, 32))\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "model = ConvClassifier(len(char_index) + 1, 24, 64)\n",
    "logits = model(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x39p9w-tA_Ds"
   },
   "source": [
    "**Задание** Подсчитайте precision, recall и F1-score для полученных предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVg2Ws7mA40a"
   },
   "outputs": [],
   "source": [
    "<calc precision, recall, f1-score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TFsKlF9BV8X"
   },
   "source": [
    "**Задание** Напишем теперь цикл обучения, который не слишком сложно будет переиспользовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsCtTJucVjMH"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, epoch_tp, epoch_fp, epoch_fn = 0, 0, 0, 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), FloatTensor(y_batch)\n",
    "\n",
    "            logits = <calc logits>\n",
    "\n",
    "            loss = <calc loss>\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                <how to optimize the beast?>\n",
    "\n",
    "            <u can move the stuff to some function>\n",
    "            tp = <calc true positives>\n",
    "            fp = <calc false positives>\n",
    "            fn = <calc false negatives>\n",
    "\n",
    "            precision = ...\n",
    "            recall = ...\n",
    "            f1 = ...\n",
    "            \n",
    "            epoch_tp += tp\n",
    "            epoch_fp += fp\n",
    "            epoch_fn += fn\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
    "                  i, batchs_count, loss.item(), precision, recall, f1), end='')\n",
    "        \n",
    "    precision = ...\n",
    "    recall = ...\n",
    "    f1 = ...\n",
    "        \n",
    "    return epoch_loss / batchs_count, recall, precision, f1\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_recall, train_precision, train_f1 = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_recall, val_precision, val_f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss, train_recall, train_precision, train_f1,\n",
    "                                     val_loss, val_recall, val_precision, val_f1))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlq63hAXh0Gr"
   },
   "outputs": [],
   "source": [
    "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=200, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o-1AEcFCjAk"
   },
   "source": [
    "**Задание** Проверьте работу классификатора на вашей фамилии.\n",
    "\n",
    "Нужно не забыть перевести модель в режим инференса - некоторые слои на трейне и инференсе ведут себя по-разному."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svO9OrF4CiLI"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "surname = \"...\"\n",
    "surname_tensor = ...\n",
    "print('P({} is surname) = {}'.format(surname, torch.sigmoid(model(surname_tensor))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djkksZKcDPA1"
   },
   "source": [
    "**Задание** Постройте precision-recall curve для данного классификатора и предыдущей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZm7O56pDcH_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FPTfBzSDheP"
   },
   "source": [
    "## Визуализации\n",
    "\n",
    "### Визуализация эмбеддингов\n",
    "\n",
    "**Задание** Визуализируем эмбеддинги слов, как это делали раньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeW5av_ASrn4"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token, colors):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67KNX5lBTdrt"
   },
   "outputs": [],
   "source": [
    "word_indices = np.random.choice(np.arange(len(test_data)), 1000, replace=False)\n",
    "words = [test_data[ind] for ind in word_indices]\n",
    "labels = test_labels[word_indices]\n",
    "\n",
    "word_tensor = convert_data(words, max(len(x) for x in words), char_index)\n",
    "embeddings = <calc embeddings>\n",
    "\n",
    "colors = ['red' if label else 'blue' for label in labels]\n",
    "\n",
    "visualize_embeddings(embeddings, words, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOt1tcOkvDBY"
   },
   "source": [
    "### Визуализация полученных свёрток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tljoUWceE2lH"
   },
   "source": [
    "Кроме всего прочего у нас тут логистическая регрессия сверху. Можно визуализировать ее также, как в eli5.\n",
    "\n",
    "**Задание** Добиться этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJpHT_YkLvCf"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "word = 'Смирнов'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNMXpgh8FNIo"
   },
   "source": [
    "Посчитайте вероятность, что слово - фамилия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hl-VZs1mFKxM"
   },
   "outputs": [],
   "source": [
    "inputs = word -> LongTensor\n",
    "prob = torch.sigmoid(model(inputs)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2SZi1FIFj6o"
   },
   "source": [
    "Посчитайте результат свертки и пулинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1SdKbDFFhOk"
   },
   "outputs": [],
   "source": [
    "convs = ...\n",
    "maxs, positions = convs.squeeze().max(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLTu8mlfFtXJ"
   },
   "source": [
    "Домножьте выход пулинга на веса выходного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRMkbFXdFwys"
   },
   "outputs": [],
   "source": [
    "linear_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qV3dK-IzGFXh"
   },
   "source": [
    "Посчитайте веса символов: каждый фильтр прикладывается к какой-то позиции - прибавим его вес к накрываемым символам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0y8epEgGEU8"
   },
   "outputs": [],
   "source": [
    "symb_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utVltLCUGXTy"
   },
   "source": [
    "Визуализируем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2UKjz86F7fz"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def get_color_hex(weight):\n",
    "    cmap = plt.get_cmap(\"RdYlGn\")\n",
    "    rgba = cmap(weight, bytes=True)\n",
    "    return '#%02X%02X%02X' % rgba[:3]\n",
    "\n",
    "symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "for symb, weight in zip(word, symb_weights):\n",
    "    res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "res = '<p>' + res + '</p>'\n",
    "\n",
    "HTML(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGhfZsleGZi0"
   },
   "source": [
    "Объединим все в функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUi51X4fQIIT"
   },
   "outputs": [],
   "source": [
    "def calc_weights(word):\n",
    "    <calc>\n",
    "    \n",
    "    return prob, symb_weights\n",
    "\n",
    "def visualize(word):\n",
    "    prob, symb_weights = calc_weights(word)\n",
    "    \n",
    "    symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "    res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "    for symb, weight in zip(word, symb_weights):\n",
    "        res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "    res = '<p>' + res + '</p>'\n",
    "    return HTML(res)\n",
    "\n",
    "\n",
    "visualize('Королев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXee135DEJuy"
   },
   "source": [
    "## Улучшение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2_8bNRyG3CF"
   },
   "source": [
    "**Задание** Для улучшение стабильности модели стоит добавить дропаут `nn.Dropout` - способ занулять часть весов на каждой эпохе для регуляризации модели. Попробуйте добавить его после эмбеддингов и после свертки (а можно еще где-нибудь).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png =x300)\n",
    "\n",
    "\n",
    "**Задание** Другой способ регуляризовывать модель - использовать BatchNormalization (`nn.BatchNorm2d`). Попробуйте добавить его после свертки.\n",
    "\n",
    "**Задание** Еще способ улучшить модель - добавить сверток. Реализуйте модель как на картинке в начале ноутбука: со свертками на 2, 3, 4 символа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qstoDysVsSQ2"
   },
   "source": [
    "**Задание** Различают Narrow и Wide свёртки - по сути, добавляется ли нулевой паддинг или нет. Для текстов эта разница выглядит так:  \n",
    "![narrow_vs_wide](https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png)\n",
    "*From Neural Network Methods in Natural Language Processing.*\n",
    "\n",
    "Слева - паддинг отсутствует, справа - есть. Попробуйте добавить паддинг и посмотреть, что получится. Потенциально он поможет выучить хорошие префиксы слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v39_tgnMnr0J"
   },
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "## Почитать\n",
    "\n",
    "### Основы\n",
    "[Convolutional Neural Networks, cs231n](http://cs231n.github.io/convolutional-networks/)  \n",
    "[Understanding Convolutions, Christopher Olah](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)  \n",
    "[Understanding Convolutional Neural Networks for NLP, Denny Britz](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "### Статьи\n",
    "[Character-Aware Neural Language Models, Yoon Kim et al, 2015](https://arxiv.org/abs/1508.06615)  \n",
    "[Character-level Convolutional Networks for Text Classification, Zhang et al., 2015](https://arxiv.org/abs/1509.01626)  \n",
    "[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification Zhang et al., 2015](https://arxiv.org/abs/1510.03820)\n",
    "[Learning Character-level Representations for Part-of-Speech Tagging, dos Santos et al, 2014](http://proceedings.mlr.press/v32/santos14.pdf)\n",
    "\n",
    "## Посмотреть\n",
    "[cs224n \"Lecture 13: Convolutional Neural Networks\"](https://www.youtube.com/watch?v=Lg6MZw_OOLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JwcvGNSIJs4"
   },
   "source": [
    "# Сдача задания\n",
    "\n",
    "[Форма](https://goo.gl/forms/FfMnyNGI2P4xo0QD3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 04 - Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
