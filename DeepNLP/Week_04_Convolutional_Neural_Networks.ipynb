{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF1fio53UKN6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3lN5pl5Stpp"
   },
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J6KBv-cniLw"
   },
   "source": [
    "## Surname classification\n",
    "\n",
    "We will learn to predict whether a word is a surname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7f2Sk0mMFJeW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 25000\n",
      "Test size = 25000\n"
     ]
    }
   ],
   "source": [
    "# Raw data\n",
    "train_df = pd.read_csv(\".data/imdb_train.tsv\", delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\".data/imdb_test.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print('Train size = {}'.format(len(train_df)))\n",
    "print('Test size = {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dreamgirls, despite its fistful of Tony wins i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This show comes up with interesting locations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I simply love this movie. I also love the Ramo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Spoilers ahead if you want to call them that.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>My all-time favorite movie! I have seen many m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_positive                                             review\n",
       "0            0  Dreamgirls, despite its fistful of Tony wins i...\n",
       "1            0  This show comes up with interesting locations ...\n",
       "2            1  I simply love this movie. I also love the Ramo...\n",
       "3            0  Spoilers ahead if you want to call them that.....\n",
       "4            1  My all-time favorite movie! I have seen many m..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(train_df['review'], train_df['is_positive'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YN1XYen8UauD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Another big star cast, another glamour's set, another reputed director, another flick filled with songs that's topping the chart buster, but alas what's missing at the day end is a story that every moviegoer expects of from such a big budget motion picture. So much hype is what that was lurking around the movie before it's' red carpet premiere. A hype which went to an extent where Anil Kapoor envisages that the movie would be one of the finest love stories ever made after Dilwale Dulhaniya Le Jayenge. Well Anilji, which movie were you speaking of? Well the plot of the movie is about 6 different couples and 12 different people, who have a total different stance towards life, but despite their different approach towards life they all have one common problem, that's LOVE. Well indeed a luring theme. But little did we expect that the movie would be such boredom that it will let down the last expectation the audience would have from such a multistarrer movie. These are kinda movies which I totally abhor because after spending a hefty buck for a multiplex ticket I get locked in the theatre for 4 hours just waiting in agony for the climax.<br /><br />The trouble begins right from the start. The director gets so confused with the plot that somewhere even he gets baffled as to how to share the time slot to six different star casts. Some of the couples like Anil Kapoor-Juhi and Sohail Khan-(Whoever the female is opposite to him) just doesn't make any sense for their existence in the movie. Salman (Who calls himself rahul in a weird manner for the entire movie. Well something like Rahoooooool) again as usual tries to be extra cool with his Videsi kinda Hindi accent. Hey Sallu Bhai, now that Aish is getting married, at least go get some tip from Abhishek to improve your acting abilities. A simple striptease wouldn't make the movie a box office hit every time. And Anilji stop shaving your trade mark beard or you look totally like a eunuch. And smooching a girl of your daughters' age just looks as uncool as watching Jack Nicholson in a romantic movie. And please Nikhilji avoid putting such superfluous scenes in a movie that is totally not needed for the shot.<br /><br />The other bigger flaw in the movie was that there wasn't any perfect synchronization between the stories of different couples. Every story itself looks as if it is taken from different flicks, put together to form a sadistic plot of Salaam-E-Ishq. Bollywood still has to learn a lot from movies like Snatch, Memento where the director knows the perfect art of threading the different unrelated sequences to form a perfect blended storyline.<br /><br />Somewhere while I was evaluating the pre-release movie reviews someone predicted that the movie wouldn't do good because the title of this movie adds up to the number 28, and 28 is considered a bad number in Numerology. But I totally take my stand by saying the movie will fail not coz of its Numerology defects, but because of the myriads of flaw that persisted in the movie. And when director like Nikhil Advani can make such major blunders in the entire storyline of the movie, any wonder wouldn't have saved the movie from bombing at the Box Office.<br /><br />My suggestion for all you guys is, please avoid watching this movie at any cost. It isn't worth a pie that you pay for the ticket. There indeed are better movies on theater screens currently which are worth watching more than Salaam-E-Ishq.\",\n",
       "  0),\n",
       " (\"Wasn't sure what to expect from this movie considering its amazing collection of stars and directors but in the end it didn't disappoint.<br /><br />For me one of the highlights was the final episode with the American tourist speaking with a dreadful French accent (which made me feel better about mine) which was actually quite touching and a great way to wrap up the movie.<br /><br />The story of the paramedic and the stabbing victim was also very moving and for pure comedy the Coen Brothers and Steve Buscemi take the award. The Tom Tykwer clip was also impressive although rather ambitious in its scope.<br /><br />However, the Bob Hoskins segment was totally cringeworthy and the vampire story was completely farcical. The dialogue in Wes Craven's section also felt very forced and the Chinatown story was completely incomprehensible.<br /><br />On the whole this film is worth watching for the good bits and has a strong finish. It's not too painful to sit through the bad sections - they only last 5 minutes anyway.<br /><br />Ca vaut la peine!!!\",\n",
       "  1),\n",
       " ('This was the worst movie I\\'ve ever seen, yet it was also the best movie. Sci Fi original movie\\'s are supposed to be bad, that\\'s what makes them fun! The line, \"I like my dinosaur meat well done!\" is probably the best quote ever! Also, the plot sounds like something out of a pot induced dream. I can imagine it now, the writers waking up after a long night of getting high and playing dance dance revolution, then putting ideas together for this: Space marines got to alien planet, which is infested with dinosaurs and has medieval houses in it, to protect a science team studying the planet. Best idea ever! In fact, in fits the complete Sci Fi original movie checklist: guns dinosaurs medieval times space travel terrible acting<br /><br />So go watch this movie, but don\\'t buy it.',\n",
       "  1),\n",
       " ('My Comments for VIVAH :- Its a charming, idealistic love story starring Shahid Kapoor and Amrita Rao. The film takes us back to small pleasures like the bride and bridegroom\\'s families sleeping on the floor, playing games together, their friendly banter and mutual respect. Vivah is about the sanctity of marriage and the importance of commitment between two individuals. Yes, the central romance is naively visualized. But the sneaked-in romantic moments between the to-be-married couple and their stubborn resistance to modern courtship games makes you crave for the idealism. The film predictably concludes with the marriage and the groom, on the wedding night, tells his new bride who suffers from burn injuries: \"Come let me do your dressing\"<br /><br />V I V A H - showcases a lot of good things - beauty of arranged marriage, beauty of Indian culture, beauty of Indian woman, last but not least a nice IDEALISM of the about-to-be-couple waiting to get married .... playing by the rules ! Simple yet Beautiful; Such a Simple story .... no plot ... no villain - as is the case with most of Sooraj Barjatya films. Sooraj sir is back to what he does BEST. He has made the movie with FULL CONVICTION. Its a very sweet film - which teaches the current generation a lot of good things bout Arranged Marriage & the Union of 2 Families. I think AMRITA RAO - looks very good & she has acted very well. She has most of the good scenes - although i thought the last half hour was completely to Shahid Kapur - who for a change gives an awesomely restrained performance. I also liked the acting of all others for ex. the Choti i.e. Amrita Prakash, Alok Nath, Anupam Kher, Shahid\\'s bro & sis-in-law. It almost seemed as real and recognizable as it could. Sooraj sir has got another nice family film to his credit after Maine Pyar Kiya, HAHK & Hum Saath Saath Hain. The chemistry between Shahid & Amrita is AWESOME.<br /><br />Stuff like Sanctity in a Marriage/Relationship, Avoiding Courtship, Mutual Respect, Care & Space, Waiting for getting Married \"officially\", Praying/Sacrficing for Ur Beloved - all these and more get SHOWCASED in Vivah. There\\'s still some good audience who r going & enjoying this film. Some of the folks/audience are already excited after seeing, that they r thinking bout Arranged-Marriage :) Thats Success if you ask me. it seems AMRITA RAO - our actress-from-Vivah {Result for a nice performance} has been bestowed the prestigious DADSAHEB PHALKE award for 2006 !! Hats off to her for this achievement Chalo, even though Vivah , Shahid or Amrita didn\\'t get any of the film-fare & other awards; @ least this is news to CHEER about !! Congrats to AMRITA RAO- for showing us a visual of Indian Bride-to-be in the purest form and Of Course to Sooraj Barjatya for portraying her the best way :) Shudn\\'t forget Shahid Kapur and all others who make VIVAH as sweet and legendary as it is today !! Imagine, to share the same pedestal as the legendary Dilip Kumar .......... Its no mean achievement !! Congrats to Amrita Rao - for taking her Career to another level with this award .... I personally feel - she should keep doing movies only with Shahid Kapur !! They make a cute couple and their on-screen chemistry reminds me of {SRK-Kajol} or {Aamir Khan & Juhi Chawla} ................. <br /><br />Some points that I observed,few of the elements :- #1 If u notice carefully, Amrita Rao looks so good because shes always wearing traditional dresses. She gives every bit of the Indian Woman essence - in this film !! Perfect Fit #2 Shahid Kapur is like most of us - not exactly ready for marriage or early-marriage .... but PREM listens carefully to the step-wise talk given by his DAD - having full faith in Anupam Kher. Eventually \"Honesty\" & \"Trust\" are the keywords that he reflects in his first talk with Amrita. Most people would think such a first meeting with a total stranger plus for a limited time is never enough to judge a person. But according to what I saw in this film, I have a feeling - that Two people who are made for each other can connect within a 1st meet also, Its possible !!! #3 In the entire movie - there are basically 4 or 5 sequences where Shahid & Amrita are together - or shown to be together. Its unlike most other romantic/wedding-based movies where Hero & Heroine are always singing/dancing or nowadays - doing cheap stuff. But the beauty of each of these 5 sequences :- Characterized by restraint, innocence & respect for the other ! #4 I really liked the relationship shown between Chacha ALOK NATH & Amrita Rao. These kinda movies should highlight the indifference shown to daughters/girl-kids in some parts of India. #5 Romantic scenes between lead couple are shot very nicely - no cheap scenes,songs are beautifully pictured !! Words like \"Jal\",\"praarthana\" e.t.c. are going to be buzzwords for all girls who liked this film :) Personally, I really am fond of many dialogs in this film. #6 Last but not the least - The entire Hospital Scene where Shahid puts \"sindhoor\" to Amrita when shes struggling for Life - is terrific. Those dialogs between the couple are so touching and U feel the LOVE/I-cant-do-without-U ; Its a Hats-Off feeling !!! <br /><br />*** In many ways, VIVAH reminded me of Maine Pyar Kiya, DDLJ, Qayamat Se Qayamat Tak, Hum Dil De Chuke Sanam - for the freshness/on-screen-chemistry of the LEAD pair :) :) *** IF U ASK ME :- Along with films like Rang De Basanti, Lage Raho Munnabhai, DOR, CORPORATE and Kabul Express, V I V A H ranks among the best films made in 2006. IN FACT - i think Vivah does deserve better viewing/business than Dhoom2 or Fanaa or Golmaal or all those time-pass/fuzzy/style/crap movies !!',\n",
       "  1),\n",
       " ('Time and time again, I\\'ve stated that if people don\\'t want remakes or sequels made, they should stop seeing them and instead venture into the world of independent film. Having said that though, the last time I saw an independent film myself must have been easily six months ago. So here\\'s a review for an indie that I had my attention drawn to on Youtube; the Cure.<br /><br />Right away, you can tell that the film is going for an avant-garde film approach which is telegraphed in its use of extreme close ups, scopophilia and fast editing. It is proud of the way it looks - and it has a right to be. For the most part it is a very nicely composed little piece, save for one inexcusable disregard for the 180 degree rule and a comically bad gunshot effect which is a phenomenon that seems to be THE calling card for self funded projects.<br /><br />Still, despite these amateurish mistakes the majority of the shots are actually a pleasure to look at. We\\'re presented with a good use of props and locations, good visual acting and some very atmospheric, fluid editing, which is made more commendable as this is definitely something you won\\'t see very often at all from a Youtube submission. The plot is fragmented and although the basic premise is fairly simple some may find it hard to follow exactly what is happening, but what we are seeing here is avant-garde storytelling at work; you can\\'t really expect a straightforward three act structure and if you do you might not be ready for this kind of movie.<br /><br />Where the film is unfortunately let down however is the sound. What you\\'re going to hear throughout is a distorted voice-over which often sounds insincere and worse still is the continuous background music, which goes through minimal change and doesn\\'t add much to anything. So much attention is paid to the visuals that the audio frankly sounds neglected, and this becomes really apparent when you realize you\\'ve just missed about four sentences of the narration and have to backtrack to pick up what slipped past your attention.<br /><br />So, give it a watch, but do it with the sound turned off.<br /><br />Last thought; was anyone else reminded of the cover for Doug Naylor\\'s Red Dwarf novel \"Last Human\" early in the film? If you have the book you know what I mean.',\n",
       "  1),\n",
       " (\"An unusual film from Ringo Lam and one that's strangely under-appreciated. The mix of fantasy kung-fu with a more realistic depiction of swords and spears being driven thru bodies is startling especially during the first ten minutes. A horseback rider get chopped in two and his waist and legs keep riding the horse. Several horses get chopped up. It's very unexpected.<br /><br />The story is very simple, Fong and his Shaolin brothers are captured by a crazed maniac general and imprisoned in the Red Lotus temple which seems to be more of a torture chamber then a temple. The General has a similarity to Kurtz in Apocalypse Now as he spouts warped philosophy and makes frightening paintings with human blood. <br /><br />The production is very impressive and the setting is bleak. Blood is everywhere. The action is very well done and mostly coherent unlike many HK action scenes from the time. Sometimes the movie veers into absurdity or the effects are cheesy but it's never bad enough to ruin the film. <br /><br />Find this one, it's one of the best HK kung fu films from the early nineties. Just remember it's not child friendly.\",\n",
       "  1),\n",
       " ('Dead To Rights is about a Police Officer named Jack Slate who finds his murdered father and goes after the man that he thinks killed him.Jack is later shot and framed for the murder of the man he suspected of killing his father.<br /><br />Several months later on the day Jack is going to be executed he escapes from prison and searches the city for the man who framed him for murder.Jack\\'s search leads him through a trail of *beep* that doesn\\'t end until everyone is dead.<br /><br />Through out the game Jack uses weapons from M4 Carbines to his dog Shadow to kill endless streams of people in 15 levels.The game play is basically \"kill 30 people,find switch to open door.kill 50 people,find switch to open door\" over and over until the level is done.There are fun mini games too like playing as a stripper to distract bouncers at a Night Club so Jack can get to another area in the club,or bomb disarming.<br /><br />Dead To Rights is also a hard game.You will be put in an area swarming with bad guys armed with Sub Machine Guns while you only have a pistol.Near the end of the game skill turns to dust and you have to rely on luck.<br /><br />Dead To Rights is as gory as it is hard.If you shoot a guy in the face with a shot gun blood will splatter on the walls,ceiling and floor.And since there are several guys in each area the walls will be painted in blood.<br /><br />Family fun for everyone.',\n",
       "  1),\n",
       " ('Are you familiar with concept of children\\'s artwork? While it is not the greatest Picasso any three-year-old has ever accomplished with their fingers, you encourage them to do more. If painting is what makes them happy, there should be no reason a parent should hold that back on a child. Typically, if a child loves to paint or draw, you will immediately see the groundwork of their future style. You will begin to see their true form in these very primitive doodles. Well, this concept of children\\'s artwork is how I felt about Fuqua\\'s depressingly cheap and uncreative film Bait. While on all accounts it was a horrid film, it was impressive to see Fuqua\\'s style begin emerging through even the messiest of moments. If you have seen either Training Day or King Arthur, you will be impressed with the birth of this director in his second film Bait. While Foxx gives a horrid, unchained performance, there are certain scenes, which define Fuqua and demonstrate his brilliance behind the camera. Sadly it only emerged in the final thirty minutes of the film, but if you focus just on those scenes, you will see why Fuqua\\'s name appears on so many \"Best Of\\x85\" film lists.<br /><br />I will never disagree with someone that Fuqua\\'s eye behind the camera is refreshing and unique. His ability to place a camera in the strangest of places to convey the simplest of emotions is shocking. I am surprised that more of Hollywood hasn\\'t jumped aboard this bandwagon. Even in the silly feature Bait, you are witness to Fuqua\\'s greatness. Two scenes that come directly to mind are the explosion scene near the middle of the film and the horse scene close to the end. In both of these scenes I saw the director Fuqua at work. Alas, in the rest of this film, all I saw was a combination of nearly every action film created. The likable hero down on his luck that suddenly finds his life turned around by some unknown force is a classic structure that just needs to die in Hollywood. We have seen this two often, and no matter who you are (unless you are Charlie Kaufmann), you cannot recreate the wheel. It is just impossible with this genre, and it is proved with Bait. I was annoyed with Fuqua for just sitting back and allowing this to happen, which could explain why it took me three viewings to finish this film. I was just tired of the structure, and while I hoped that Fuqua would redefine it, he did not.<br /><br />Then, there was the acting. While Jamie Foxx has never impressed me as an actor, I was willing to give this helmed vehicle a try. I wanted to see if he could pull off another dramatic role similar to Collateral. I was under the impression that perhaps this was the film chosen to show producers that Foxx could handle the role in Collateral. Again, I was disappointed. Foxx was annoying. Not in the sense that it was the way that his character was to be, but in the sense that it felt as if neither Fuqua nor Foxx took the time to fully train Foxx on what should be ad-libed and what should be used to further the plot. Instead, we are downtrodden with scene over scene of Foxx just trying to make the audience laugh. Adding second long quips and culture statements just to keep his audience understanding that he was a comedian first, an actor second. Fuqua should have stopped this immediately. Foxx\\'s jokes destroyed his character, which in turn left me with nothing solid to grasp ahold of. Instead of character development, he would crack a joke. Neither style worked, no joke was funny. The rest of the cast was average. By this I mean I have seen them all in similar roles. They were brining nothing new to the table, nothing solid to the story, and nothing substantial to the overall themes of the film. They were pawns filling in dead air space. Fuqua had no control over this mess, and the final verdict only supports that accusation.<br /><br />Overall, this was a sad film. With no creativity in sight and unmanaged actors just trying to upstage themselves, what originally started as a decent story eventually sunk faster into the cinematic quicksand. Foxx was annoying, without character lines, and a complete bag of cheese. In each scene I saw no emotion, and when emotion was needed to convey a message, he chose to take his shirt off rather than tackle the issues. Are my words harsh? I don\\'t think so. When you watch any movie you want to see some creativity, some edible characters, and themes that seem to hit close to home. Bait contained none of these. While I will give Fuqua some credit for two of the scenes in this film, the remaining five hundred were disastrous. Apparently, I took the bait when renting this film, but now having seen it, hopefully I can stop others from taking that curious nibble.<br /><br />Grade: ** out of ***** (for his two scenes that were fun to watch)',\n",
       "  0),\n",
       " (\"Once you pick your jaw up from off the floor from the realization that they... somehow... managed to put this thing together so fast that it was released the same year the case ended, you'll find that it's not half bad. The plot is engaging and interesting, and the pacing is fast, with this covering many situations, and thus often jumping swiftly on to the next one after a line or two has been spoken. Where this really stands out is the acting. The performances are excellent. Neill and Streep are both impeccable. It's also cool to hear so much Australian spoken in a Hollywood film, and even those who don't come naturally to it at least attempt an accent. The cinematography and editing are nice enough, but they don't really go beyond the standard stuff. This movie's story is compelling and the fact that it is authentic just makes it all the more chilling. While I have not read the novel or heard of what happened outside of this picture, I understand that it is quite close to the truth. There is some moderate to strong language and disturbing content in this. It is, at times, a downright great courtroom drama. I recommend this to any fellow fan of such. 7/10\",\n",
       "  1),\n",
       " (\"I was pleasantly surprised to find that How to Lose Friends and Alienate People was nowhere near as 'gross-out' a comedy as the trailer had led me to expect. I rapidly became absorbed in the unfolding of the narrative and remained engrossed throughout. Pacing of the more visual humorous content was, I thought, spot on. (I mean I got the impression I was witnessing Pegg's attempts at restoring lost control very much 'in real time', so to speak.) At other moments there was time allowed to share the main protagonists' (i.e. Pegg's and Dunst's) reflection on how events were affecting them and what had led them to where they now found themselves. All the characters were well cast, to some extent interesting in and of themselves, and generally quite likable. (Any apparent ruthless ambition displayed tended to be tempered by a corresponding good natured resilience.) An entertaining, intelligently scripted, brilliantly directed and superbly acted film that I would thoroughly recommend.\",\n",
       "  1),\n",
       " (\"Unashamedly ambitious sci-fi from Kerry Conran, for whom this is clearly a labour of love. Unfortunately it's just not that good. It all starts well enough - with an epic but restrained score, a mixture of Lucas and Hitchcock style editing and the glossy cinematography of a Spielberg. The movie also references many pulp sci-fi novels, serials and films as diverse as The Day The Earth Stood Still, Superman, Metropolis, Planet Of The Apes, The Iron Giant, Star Wars and The Spy Who Loved Me. The film however, fails to be as good as any one of those for several reasons: the main being that it's such a labour of love, so concerned with throwing everything at the screen and creating a brave new world, Conran actually forgets about making a movie. There is little to no tension, atmosphere or magic on offer here despite aerial battles, dinosaurs and race-against-time set-pieces. Even the noir elements fall flat. This is a broad way of looking at things though - those elements mainly fail because nothing feels at all real and is so obviously fake - the green-screen just looks like a video game half the time and it's obvious the actors have been pasted on afterwards. The actors don't get to do much either - Jude Law is wooden, Gwyneth Paltrow is annoying and stupid, Angelina Jolie is wasted - and it's all because of an awful script - the sort that has to explain nearly everything. It is a decent experience and some might get a nostalgia feel but ultimately this is a pointless step into the world of yesterday. Nice ending though.\",\n",
       "  0),\n",
       " ('I was looking through the movie listings in my area on yahoo and seen a movie that had not been advertised. I looked closer and noticed that Peter Falk and Paul Reiser were in it. Having watched \"Mad about you\", once, I was not a fan of Paul Reiser. However, I am a big fan of Peter Falk. So the spouse and I took a chance. We were both swept into this story. The beautiful scenery, the heartfelt acting and the sense of family and moral values that are seldom seen in movies and the world today. Not that sappy emoted junk, but real life situations from real life-like people. I even have to say, Paul Reiser was excellent, although, I still won\\'t watch \"Mad about you\". I don\\'t know where this movie has gone. I heard it was put out in limited release. It should be shared with the world. It is one of the finest movies I have seen. M.',\n",
       "  1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_data, train_labels))[::1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFlOaCNt3fuV"
   },
   "source": [
    "The data is also very balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikKB2DaK08-n"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "positive_count = np.sum(train_labels == 1)\n",
    "negative_count = len(train_labels) - positive_count\n",
    " \n",
    "plt.bar(np.arange(2), [negative_count, positive_count], align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(2), ('Negative', 'Positive'))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuqFWWu38UwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 50.19%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = {:.2%}'.format((train_labels == 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtOFSmy93pDl"
   },
   "source": [
    "However, it will be rather useless to always say that the word is not a surname. This, of course, is a question - what is worse, in vain to declare a word a last name (a mistake of the first kind) or not to find a last name.\n",
    "\n",
    "<img src=\"https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg\" style=\"border:none;width:35%\">\n",
    "\n",
    "We will measure precision, recall and their combination - $ F_1 $ -measure.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" style=\"border:none;width:25%\">\n",
    "\n",
    "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
    "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bl6O_bAtVqd"
   },
   "source": [
    "Let's start with a baseline on the regulars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJJtiGMK1ATy"
   },
   "source": [
    "And now seriously - a baseline on logistic regression over N-gram characters.\n",
    "\n",
    "**Assignment** Make a classification with the LogisticRegression model. Calculate the F1-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8COAoh7b0TXs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newscred/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score = 86.21%\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(test_df['review'])\n",
    "print('F1-score = {:.2%}'.format(f1_score(test_df['is_positive'], test_preds)))\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# preds = model.predict(test_data)\n",
    "# print('Test accuracy = {:.2%}'.format(accuracy_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN46GwPrzeJW"
   },
   "source": [
    "Look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-P8OSD7yZQt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.699\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        wonderfully\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.688\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        refreshing\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.669\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        funniest\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.594\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        rare\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.569\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        surprisingly\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.537\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        superb\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.366\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        incredible\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.297\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        enjoyable\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.278\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        subtle\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.265\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        gem\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.243\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.76%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 37714 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.64%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 37096 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.262\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        redeeming\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.279\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        weak\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.52%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.280\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        mst3k\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.40%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.300\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        annoying\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.26%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.321\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        wooden\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.326\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        basically\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.363\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.371\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worse\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.376\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        mediocre\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.425\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        alright\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.55%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.438\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        unfunny\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.441\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        horrible\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.45%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.455\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        dull\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.475\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        forgettable\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.480\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        lame\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 87.16%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.503\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        badly\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.550\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        save\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.570\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pointless\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.40%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.631\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        laughable\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.665\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        avoid\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.736\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        fails\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.745\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.757\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        disappointing\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.960\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        mess\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.961\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 83.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.072\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        lacks\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.346\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poorly\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 81.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.591\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        disappointment\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.830\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(model, vec=vectorizer, top=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYXljseDyd77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=positive\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>1.000</b>, score <b>20.405</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +20.429\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 98.38%); opacity: 0.80\" title=\"-0.011\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.41%); opacity: 0.88\" title=\"0.431\">both</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.26%); opacity: 0.80\" title=\"-0.024\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.54%); opacity: 0.94\" title=\"0.789\">entertaining</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"1.113\">touching</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.36%); opacity: 0.82\" title=\"-0.125\">version</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.33%); opacity: 0.84\" title=\"0.215\">classic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.086\">tale</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 85.68%); opacity: 0.85\" title=\"0.256\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.86%); opacity: 0.81\" title=\"0.044\">quite</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.79%); opacity: 0.85\" title=\"0.254\">intelligent</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 87.79%); opacity: 0.84\" title=\"-0.204\">not</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> &#x27;</span><span style=\"background-color: hsl(0, 100.00%, 95.04%); opacity: 0.81\" title=\"-0.056\">me</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 87.01%); opacity: 0.84\" title=\"0.223\">you</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.20%); opacity: 0.85\" title=\"0.295\">jane</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 92.06%); opacity: 0.82\" title=\"0.110\">school</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.23%); opacity: 0.81\" title=\"-0.053\">at</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.24%); opacity: 0.80\" title=\"0.024\">all</span><span style=\"opacity: 0.80\">.&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;</span><span style=\"background-color: hsl(120, 100.00%, 89.90%); opacity: 0.83\" title=\"0.156\">it</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.82%); opacity: 0.84\" title=\"0.204\">famous</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.96%); opacity: 0.81\" title=\"0.058\">story</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 93.33%); opacity: 0.82\" title=\"-0.086\">child</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.43%); opacity: 0.80\" title=\"-0.022\">reared</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.69%); opacity: 0.82\" title=\"0.118\">manhood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.91%); opacity: 0.81\" title=\"-0.076\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.70%); opacity: 0.81\" title=\"-0.062\">apes</span><span style=\"opacity: 0.80\">. a </span><span style=\"background-color: hsl(0, 100.00%, 81.82%); opacity: 0.86\" title=\"-0.361\">titled</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.95%); opacity: 0.84\" title=\"0.225\">british</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.93%); opacity: 0.84\" title=\"-0.225\">couple</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.17%); opacity: 0.81\" title=\"0.054\">wife</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.66%); opacity: 0.85\" title=\"-0.283\">pregnant</span><span style=\"opacity: 0.80\">) </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.86%); opacity: 0.81\" title=\"0.029\">stranded</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.30%); opacity: 0.83\" title=\"0.147\">african</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.43%); opacity: 0.82\" title=\"-0.103\">wilds</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.51%); opacity: 0.81\" title=\"0.049\">after</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">shipwreck</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 95.51%); opacity: 0.81\" title=\"0.049\">after</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.98%); opacity: 0.91\" title=\"0.602\">parents</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 96.88%); opacity: 0.81\" title=\"0.029\">deaths</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.05%); opacity: 0.83\" title=\"-0.175\">baby</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.63%); opacity: 0.83\" title=\"0.162\">raised</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.91%); opacity: 0.81\" title=\"-0.076\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.70%); opacity: 0.81\" title=\"-0.062\">apes</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 80.83%); opacity: 0.87\" title=\"0.389\">twenty</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.84%); opacity: 0.80\" title=\"-0.017\">years</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.59%); opacity: 0.86\" title=\"0.339\">later</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 98.38%); opacity: 0.80\" title=\"-0.011\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.84%); opacity: 0.80\" title=\"0.000\">young</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.80%); opacity: 0.80\" title=\"0.018\">man</span><span style=\"opacity: 0.80\"> (i.e. </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\">) </span><span style=\"background-color: hsl(0, 100.00%, 92.57%); opacity: 0.82\" title=\"-0.100\">rescues</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 92.56%); opacity: 0.82\" title=\"-0.101\">wounded</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.39%); opacity: 0.83\" title=\"-0.167\">belgian</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.44%); opacity: 0.81\" title=\"-0.035\">explorer</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 94.82%); opacity: 0.81\" title=\"0.060\">nursing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.32%); opacity: 0.82\" title=\"0.125\">him</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.86%); opacity: 0.80\" title=\"-0.017\">back</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.04%); opacity: 0.80\" title=\"0.015\">health</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.39%); opacity: 0.83\" title=\"-0.167\">belgian</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.31%); opacity: 0.90\" title=\"0.559\">discovers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.34%); opacity: 0.83\" title=\"0.146\">evidence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.36%); opacity: 0.80\" title=\"-0.023\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.33%); opacity: 0.81\" title=\"0.037\">rescuer</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.84%); opacity: 0.80\" title=\"0.000\">young</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.73%); opacity: 0.82\" title=\"-0.117\">lord</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.44%); opacity: 0.80\" title=\"-0.022\">greystoke</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.35%); opacity: 0.84\" title=\"0.215\">returns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.32%); opacity: 0.82\" title=\"0.125\">him</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.54%); opacity: 0.81\" title=\"0.065\">rightful</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.55%); opacity: 0.86\" title=\"-0.340\">estate</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.51%); opacity: 0.81\" title=\"-0.065\">scotland</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 94.13%); opacity: 0.81\" title=\"-0.072\">where</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.59%); opacity: 0.80\" title=\"0.020\">he</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.48%); opacity: 0.83\" title=\"0.188\">must</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.47%); opacity: 0.82\" title=\"0.122\">adjust</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.074\">civilized</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.35%); opacity: 0.84\" title=\"0.215\">society</span><span style=\"opacity: 0.80\">. &lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;</span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.49%); opacity: 0.80\" title=\"0.010\">movie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.12%); opacity: 0.83\" title=\"-0.151\">sort</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.16%); opacity: 0.82\" title=\"0.089\">divided</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.40%); opacity: 0.82\" title=\"0.085\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.84%); opacity: 0.81\" title=\"0.060\">two</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.67%); opacity: 0.85\" title=\"0.283\">parts</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.69%); opacity: 0.82\" title=\"0.139\">first</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.98%); opacity: 0.89\" title=\"-0.474\">half</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 98.25%); opacity: 0.80\" title=\"0.013\">we</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.71%); opacity: 0.83\" title=\"0.160\">see</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.47%); opacity: 0.82\" title=\"-0.122\">environment</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 87.79%); opacity: 0.84\" title=\"-0.204\">not</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.074\">being</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.26%); opacity: 0.80\" title=\"-0.024\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.35%); opacity: 0.82\" title=\"0.125\">expert</span><span style=\"opacity: 0.80\">, i </span><span style=\"background-color: hsl(0, 100.00%, 79.98%); opacity: 0.87\" title=\"-0.414\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.01%); opacity: 0.82\" title=\"0.132\">unaware</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.00%); opacity: 0.81\" title=\"0.074\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.63%); opacity: 0.88\" title=\"0.424\">realism</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.35%); opacity: 0.82\" title=\"0.105\">its</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.01%); opacity: 0.81\" title=\"0.074\">depiction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.29%); opacity: 0.82\" title=\"-0.106\">ape</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.31%); opacity: 0.85\" title=\"0.292\">community</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.30%); opacity: 0.84\" title=\"0.241\">life</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 96.46%); opacity: 0.81\" title=\"-0.035\">but</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.90%); opacity: 0.83\" title=\"0.156\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.37%); opacity: 0.82\" title=\"0.104\">certainly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.54%); opacity: 0.94\" title=\"0.789\">entertaining</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.049\">for</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.04%); opacity: 0.81\" title=\"-0.056\">me</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.38%); opacity: 0.82\" title=\"0.124\">more</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.57%); opacity: 0.95\" title=\"0.861\">moving</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.56%); opacity: 0.84\" title=\"-0.234\">section</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.02%); opacity: 0.80\" title=\"-0.027\">second</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.98%); opacity: 0.89\" title=\"-0.474\">half</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 96.46%); opacity: 0.81\" title=\"-0.035\">when</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.48%); opacity: 0.83\" title=\"0.188\">must</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.09%); opacity: 0.83\" title=\"-0.174\">meet</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.44%); opacity: 0.84\" title=\"0.237\">real</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.64%); opacity: 0.85\" title=\"0.284\">family</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 89.45%); opacity: 0.83\" title=\"0.166\">develop</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.83%); opacity: 0.88\" title=\"0.448\">language</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.03%); opacity: 0.82\" title=\"-0.132\">skills</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.47%); opacity: 0.82\" title=\"0.122\">adjust</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.42%); opacity: 0.80\" title=\"0.011\">aristocratic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.95%); opacity: 0.84\" title=\"0.225\">british</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.35%); opacity: 0.84\" title=\"0.215\">society</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 97.24%); opacity: 0.80\" title=\"0.024\">all</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.09%); opacity: 0.81\" title=\"0.072\">while</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.05%); opacity: 0.81\" title=\"-0.056\">wooing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.20%); opacity: 0.85\" title=\"0.295\">jane</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(120, 100.00%, 98.47%); opacity: 0.80\" title=\"0.011\">andie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.28%); opacity: 0.80\" title=\"-0.004\">macdowell</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(120, 100.00%, 97.59%); opacity: 0.80\" title=\"0.020\">he</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.53%); opacity: 0.85\" title=\"-0.260\">portrayed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.00%); opacity: 0.81\" title=\"0.074\">as</span><span style=\"opacity: 0.80\"> a &#x27;</span><span style=\"background-color: hsl(120, 100.00%, 93.25%); opacity: 0.82\" title=\"0.088\">noble</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.11%); opacity: 0.86\" title=\"0.352\">savage</span><span style=\"opacity: 0.80\">&#x27;, </span><span style=\"background-color: hsl(0, 100.00%, 82.31%); opacity: 0.86\" title=\"-0.347\">whether</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.23%); opacity: 0.85\" title=\"-0.268\">wild</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.25%); opacity: 0.80\" title=\"-0.024\">or</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.55%); opacity: 0.83\" title=\"0.164\">elegant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.60%); opacity: 0.80\" title=\"-0.002\">edwardian</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.67%); opacity: 0.82\" title=\"-0.099\">parlors</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 93.91%); opacity: 0.81\" title=\"-0.076\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.01%); opacity: 0.91\" title=\"0.601\">contrast</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.12%); opacity: 0.84\" title=\"-0.196\">upper</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.62%); opacity: 0.81\" title=\"-0.033\">crust</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.69%); opacity: 0.90\" title=\"-0.546\">depicted</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.00%); opacity: 0.81\" title=\"0.074\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.80%); opacity: 0.86\" title=\"0.333\">often</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.24%); opacity: 0.85\" title=\"-0.268\">far</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.38%); opacity: 0.82\" title=\"0.124\">more</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.21%); opacity: 0.80\" title=\"-0.013\">barbaric</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.42%); opacity: 0.82\" title=\"-0.103\">than</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.78%); opacity: 0.86\" title=\"-0.334\">left</span><span style=\"opacity: 0.80\">.&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;</span><span style=\"background-color: hsl(0, 100.00%, 98.61%); opacity: 0.80\" title=\"-0.009\">christopher</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.96%); opacity: 0.80\" title=\"0.006\">lambert</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.76%); opacity: 0.94\" title=\"0.782\">fantastic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.82%); opacity: 0.94\" title=\"0.779\">sympathetic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.89%); opacity: 0.91\" title=\"0.605\">portrayal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.41%); opacity: 0.88\" title=\"0.431\">both</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.074\">civilized</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.03%); opacity: 0.80\" title=\"-0.027\">environments</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 97.59%); opacity: 0.80\" title=\"0.020\">he</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.79%); opacity: 0.84\" title=\"0.229\">conveys</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(120, 100.00%, 86.44%); opacity: 0.84\" title=\"0.237\">real</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.78%); opacity: 0.82\" title=\"0.137\">sense</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.03%); opacity: 0.82\" title=\"-0.111\">confusion</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.93%); opacity: 0.86\" title=\"-0.330\">conflict</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 81.75%); opacity: 0.87\" title=\"0.363\">torn</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.00%); opacity: 0.81\" title=\"0.074\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.59%); opacity: 0.80\" title=\"0.020\">he</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.93%); opacity: 0.82\" title=\"-0.094\">between</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.84%); opacity: 0.81\" title=\"0.060\">two</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.72%); opacity: 0.82\" title=\"0.138\">very</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.24%); opacity: 0.83\" title=\"0.171\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 70.98%); opacity: 0.93\" title=\"0.703\">worlds</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.70%); opacity: 0.85\" title=\"-0.282\">original</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.29%); opacity: 0.82\" title=\"-0.106\">ape</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.64%); opacity: 0.85\" title=\"0.284\">family</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.26%); opacity: 0.81\" title=\"0.069\">new</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.10%); opacity: 0.84\" title=\"0.246\">human</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.75%); opacity: 0.81\" title=\"-0.079\">one</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 81.18%); opacity: 0.87\" title=\"0.379\">sir</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.84%); opacity: 0.92\" title=\"0.674\">ralph</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.44%); opacity: 0.80\" title=\"-0.011\">richardson</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 93.75%); opacity: 0.81\" title=\"-0.079\">one</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.91%); opacity: 0.83\" title=\"0.155\">old</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.95%); opacity: 0.84\" title=\"0.225\">british</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.75%); opacity: 0.87\" title=\"-0.363\">legends</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.26%); opacity: 0.96\" title=\"0.910\">brilliant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.00%); opacity: 0.81\" title=\"0.074\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.63%); opacity: 0.86\" title=\"0.310\">always</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.03%); opacity: 0.82\" title=\"-0.111\">role</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 90.61%); opacity: 0.83\" title=\"-0.140\">grandfather</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.78%); opacity: 0.82\" title=\"-0.137\">sixth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.12%); opacity: 0.82\" title=\"0.109\">earl</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.44%); opacity: 0.80\" title=\"-0.022\">greystoke</span><span style=\"opacity: 0.80\">. &lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;&lt;</span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.037\">br</span><span style=\"opacity: 0.80\"> /&gt;</span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.00%); opacity: 0.82\" title=\"0.092\">film</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.71%); opacity: 0.91\" title=\"0.611\">focuses</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.38%); opacity: 0.82\" title=\"0.124\">more</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.37%); opacity: 0.80\" title=\"-0.023\">on</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.27%); opacity: 0.83\" title=\"0.193\">tarzan</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 88.78%); opacity: 0.83\" title=\"-0.181\">struggles</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.76%); opacity: 0.80\" title=\"0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.25%); opacity: 0.83\" title=\"0.148\">adapting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.65%); opacity: 0.80\" title=\"0.019\">civilization</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.083\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.49%); opacity: 0.87\" title=\"-0.370\">inner</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.93%); opacity: 0.86\" title=\"-0.330\">conflict</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.42%); opacity: 0.82\" title=\"-0.103\">than</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.37%); opacity: 0.80\" title=\"-0.023\">on</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.59%); opacity: 0.82\" title=\"0.100\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.83%); opacity: 0.85\" title=\"-0.253\">jungle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.69%); opacity: 0.81\" title=\"-0.062\">exploits</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 98.38%); opacity: 0.80\" title=\"-0.011\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.30%); opacity: 0.85\" title=\"0.266\">unusual</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.71%); opacity: 0.82\" title=\"0.118\">take</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.37%); opacity: 0.80\" title=\"-0.023\">on</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.91%); opacity: 0.83\" title=\"0.155\">old</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.33%); opacity: 0.84\" title=\"0.215\">classic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.88%); opacity: 0.85\" title=\"0.304\">makes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.90%); opacity: 0.83\" title=\"0.156\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.41%); opacity: 0.88\" title=\"0.431\">both</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.065\">typical</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.39%); opacity: 0.84\" title=\"0.214\">dramatic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.79%); opacity: 0.85\" title=\"0.254\">adventure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.46%); opacity: 0.81\" title=\"-0.035\">but</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.68%); opacity: 0.85\" title=\"0.256\">also</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 82.06%); opacity: 0.86\" title=\"0.354\">above</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.24%); opacity: 0.80\" title=\"0.024\">all</span><span style=\"opacity: 0.80\">, a </span><span style=\"background-color: hsl(120, 100.00%, 66.57%); opacity: 0.95\" title=\"0.861\">moving</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.56%); opacity: 0.87\" title=\"-0.368\">personal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.96%); opacity: 0.81\" title=\"0.058\">story</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 89.82%); opacity: 0.83\" title=\"-0.158\">wasn</span><span style=\"opacity: 0.80\">&#x27;t </span><span style=\"background-color: hsl(120, 100.00%, 64.77%); opacity: 0.97\" title=\"0.928\">surprised</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.17%); opacity: 0.80\" title=\"-0.004\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.43%); opacity: 0.88\" title=\"-0.430\">note</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.63%); opacity: 0.83\" title=\"-0.184\">here</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.36%); opacity: 0.80\" title=\"-0.023\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.35%); opacity: 0.82\" title=\"0.105\">its</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.48%); opacity: 0.85\" title=\"-0.262\">director</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.77%); opacity: 0.81\" title=\"0.045\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.034\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.27%); opacity: 0.84\" title=\"-0.217\">same</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.48%); opacity: 0.80\" title=\"0.010\">individual</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 93.30%); opacity: 0.82\" title=\"-0.087\">hugh</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.36%); opacity: 0.84\" title=\"0.239\">hudson</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 89.80%); opacity: 0.83\" title=\"0.158\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.68%); opacity: 0.85\" title=\"0.256\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.48%); opacity: 0.88\" title=\"-0.459\">directed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.96%); opacity: 0.81\" title=\"-0.028\">chariots</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.25%); opacity: 0.82\" title=\"-0.088\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.30%); opacity: 0.80\" title=\"-0.024\">fire</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 95.02%); opacity: 0.81\" title=\"-0.057\">another</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.26%); opacity: 0.96\" title=\"0.910\">brilliant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.49%); opacity: 0.80\" title=\"0.010\">movie</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQF3JlD7twQK"
   },
   "source": [
    "In addition to the blunt counting of the F1-score, you can look at precision-recall curves. First, they are beautiful. Secondly, it is clear from them that you can improve the quality (F1-score) by selecting another threshold - ** although it is impossible to do this on the test **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7oyvJ-2EG0m"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAG5CAYAAAD2yo9EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xu4nHdd7/339z7OeWadkrZJaAIplRRoKbEUQQWsiAUKotZS0M1+FJ6Nom4qPLuiF7opUnRvwe0DujdUxRMUUNuiFFvAFgSEEqBAkkKbQqFJ2mQdZs35PL/9xxyYlaxkzVpZs2bume/rutaVNZN7Zn6zZub+zO8sxhiUUkqpSWQNuwBKKaXUsGgIKqWUmlgagkoppSaWhqBSSqmJpSGolFJqYmkIKqWUmlgagkqdgYgcEpHnrXHME0QkLyL2FhVr4ETkERG5qv3774vI3w27TEoNioagCpz2SbrUDp8TIvIBEYlt9uMYYy4xxty7xjHfN8bEjDGNzX78dgDV2s9zWUS+ICLP3uzHORcikhCRPxGR77fL+XD78uywy6ZUPzQEVVC91BgTAy4H9gO/e+oB0hL09/iH289zFrgH+OiQy9MlIh7waeAS4EVAAng2sAhcsYH7cza1gEr1IegnCDXhjDHHgE8ATwUQkXtF5A9E5PNAEXiiiCRF5C9E5DEROSYib+9tvhSR14rIAyKSE5HDInJ5+/reZsErROSAiGTbtc93ta/fLSKmcwIXkQtE5GMisiQiR0TktT2P8/si8hER+Zv2Yx0Skf19Ps868PfADhGZ67nPl4jI/T01xaf3/N8uEfknEZkXkUUReU/7+ieJyL+1r1sQkb8XkdQG/vy/BDwB+BljzGFjTNMYc9IYc5Mx5s72YxkR2dtTpg+IyNvbvz9PRI6KyH8TkceBv2q/Di/pOd5pl7/zmlzZfp7LIvL1tZqrlVqLhqAKNBHZBVwNfK3n6l8EXgfEge8BHwDqwF7gGcALgV9p3/7ngd+ndUJPANfQqsmc6n8B/8sYkwCeBHzkDEW6FTgKXAD8HPAOEXlBz/9f0z4mBXwMeE+fz9Nrl3ERSLevewbwl8D/C8wA/wf4mIj47ZD/l/bz3w3saD8ugAA3t8v4FGBX+2+wXlcB/2qMyW/gth3nAdPAhbResw8Br+z5/58CFowxXxWRHcDHgbe3b/Mm4B97vxQotV4agiqobheRZeBzwGeAd/T83weMMYfatadpWiH5X40xBWPMSeDdwHXtY38F+CNjzJdNyxFjzPdWebwasFdEZo0xeWPMF089oB3IzwH+mzGmbIy5H7iFVnh1fM4Yc2e7D/FvgUvXeJ7Xtp9nCXgt8HPt5wWt0Pg/xpgvGWMaxpi/BirAlbSaIy8A3tx+3mVjzOcA2s/xk8aYijFmHngX8ONrlGM1M8BjG7hdrybwe+2ylIAPAteISKT9/9fTCkaAVwN3tv9+TWPMJ4EDtF5fpTZEQ1AF1cuNMSljzIXGmF9tn0A7Hu35/ULABR5rN6Et06oxbWv//y7g4T4e75eBJwPfEpEv9zbZ9bgAWDLG5Hqu+x6tWljH4z2/F4FQu8nvVe2BJXkR+UTPMR8xxqSA7cBB4JmnPLff6jyv9nPb1S7HLuB7PYHZJSLbReTWdtNwFvg7Wn2O67UInL+B2/WaN8aUOxeMMUeAB4CXtoPwGlrBCK3n+/OnPN/nbkIZ1ATTjmg1jnq3RnmUVu1odrVAaP//k9a8Q2MeAl7ZHmjzCuAfRGTmlMOOA9MiEu8JwicAx/q4/7+n1ed3pv9fEJHXAQdE5IPGmMfaZf8DY8wfnHp8exTpE0TEWeV5v4PW3+hpxpglEXk5fTbLnuJTwNtFJGqMKZzhmCIQ6bl8Hq3m4u5TW+U2nSZRCzjcDkZoPd+/Nca8dpXbKLUhWhNUY60dFncDf9wezm+1B4Z0mv9uAd4kIs9sjybdKyIXnno/IvJqEZkzxjSB5fbVzVMe61HgC8DNIhJqD1L5ZVo1rc14Lt8G7gL+v/ZV7wf+i4g8q132qIi8WETiwH20mirf2b4+JCLPad8uDuSBTLuf7c0bLNLf0gqmfxSRH2r/bWdE5C0i0mmivB+4XkRsEXkR/TW73kqr3/b1/KAWCK2/40tF5Kfa9xdqD67ZucHyK6UhqCbCLwEecJjWoJJ/oN2EZoz5KPAHtE62OeB2Wv2Ip3oRcEhE8rQGyVx3ShNsxytpDUQ5DtxGq7/rU5v4XP4H8DoR2WaMOUCrn/A97ed1BHgNQLvP8aW0BgN9n1bt6xfa9/HfaU0tydAaaPJPGymIMaZCa3DMt4BPAlla4TsLfKl92G+2y7EMvIrW33et+30M+A/gR4AP91z/KPAy4C3APK0AfjN6HlPnQHRTXaWUUpNKv0EppZSaWBqCSimlJpaGoFJKqYmlIaiUUmpiBW6e4OzsrNm9e/ewizGxms0mjUYD13WHXZShM8ZQr9dxHAcRGXZxxlKtVsOyLGx7bHaqUgPyla98ZcEYs+4l9AIXgrt37+bAgQPDLsbEyufzZLNZzj///Ik/8dfrdU6ePEkqlSISiax9A7VuS0tLVKtVtm/fPvHvN3V2IrLacodr0uZQtS6dE1Gz2VzjyPHXqQHWarVhF2VsRaNRms0m5XJ57YOV2gANQbUunRDU+aUtrutqCA6Q7/s4jkOxWBx2UdSY0hBU62JZrbeMhmCL67rU66stSao2SyQSoVKp6N9ZDYSGoFoXrQmu5DhOd7CQGoxIJIKIUCicaY1upTZOQ1Cti4bgSp1RstokOjiWZREKhSiVSvq+U5tOQ1Cti4bgShqCW6MzQKZUWm3NcqU2TkNQrYuODl1JRHAcR/urBszzPBzH0SZRtek0BNW6aE3wdDpCdGtEo1FqtRrVanXYRVFjRENQrYuODj1dpyaof5PBCofDWJZFPp8fdlHUGNEQVOuiNcHTab/g1rAsi2g0Srlc1uZntWk0BNW6iYiGYA8Nwa0TjUYREa0Nqk2jIajWTUNwJdu2sSxLaydboFMbLJVK+vdWm0JDcICMMWMZFpZl6ejQUziOozXBLRKNRgF0pKjaFAMLQRH5SxE5KSIHz/D/IiJ/KiJHROQbInL5oMoyDOVymccee2wsv61qTfB0OkJ069i2TTgcplgs6ko96pwNsib4AeBFZ/n/nwYuav+8DvjzAZZly43zABINwdO5rtvdX1ANXiwWwxijtUF1zgYWgsaYzwJLZznkZcDfmJYvAikROX+t+82Vg3WSGcew0BA8nQ6O2VqO4xAOhykUCto0r87JMPsEdwCP9lw+2r7uNCLyOhE5ICIHji4FY1SY1gQni+O09qfWmuDW0dqg2gyBGBhjjHmfMWa/MWa/bdvDLk5fNAQnS2f5NK0Jbh3XdQmFQlobVOdkmCF4DNjVc3ln+7qxMM4hqKNDV6eDY7ZeLBaj2Wzqprtqw4YZgh8Dfqk9SvRKIGOMeWyI5dlU4xyCWhNcneu6NBoN/YKwhTzPw/d9CoWCvifVhjiDumMR+RDwPGBWRI4Cvwe4AMaY/w3cCVwNHAGKwH8eVFmGQUNw8vT2C3qeN+TSTI5YLMbi4iLFYrE7h1Cpfg0sBI0xr1zj/w3wa4N6/GHrhOA46g34cX6e69U7QlRDcOv4vo/neeTz+e4u9Er1KxADY4JIRMa2xqR7Cq6us3ya9gtuvVgsRqPR0E131boFLgSDFinjHILj+NzOlQ6OGY5QKITruuTzeX1fqnUJXAgGKQXHtSaoewqemeu6urfgkMRiMer1utYG1boELwQDZFxDUGuCZ+Y4DsYYXdNyCMLhMJ7nkcvl9L2p+qYhOEAagpNHl08brkQiQaPR0FVkVN8CF4JBOu1qCE4ex3EQEarV6rCLMpE8zyMUCpHP53XglupL4EIwSMY9BPUkczoRwfM8DcEhSiQSGGPI5XLDLooKAA3BARr3EBzH57YZPM+jVqvpl4QhcRyHSCRCsVjUBc3VmjQEB2hcQ7AzOlRP8qvrTJTX2uDwxONxRIRsNjvsoqgRpyE4QOMagiKii2ifhed52i84ZJZlEYvFKJfLVCqVYRdHjbDghWCAMmVcQxBaq6PoNIDVdfoF9eQ7XNFoFNu2tTaozipwIWgClIIagpNL+wWHT0SIx+PUajWdQK/OKHAhGCTjHILaHHp22i84GiKRCK7rks1mx/azqM6NhuAAjXMIdmqC4/r8zpX2C44OnUCvzkZDcIDGeSqBjhA9OxHBdV3tFxwBvu/rBHp1RhqCAzTOIWjbNoD2C56F7/vaLzgi4vE4zWZTJ9Cr0wQuBIMUJ5MQgnqCPzPtFxwdrut2J9Druq6qV+BCMEjGOQQ7zaFaEzwz7RccLYlEAhEhk8kMuyhqhGgIDtA4h6Bt24iIhuBZdPoFNQRHg2VZJBIJqtWqDpJRXcELwQDlyTiHIOg0iX5ov+BoiUQi+L5PLpfTL3AKCGIIBtC4hqBOmF+b53kYY7QfaoQkk0mMMdosqoAAhmCQ4kRrgqrTL6hTJUaH4zjE43HK5TLlcnnYxVFDFrgQDJJxD0GtCa5N+wVHUzQaxXVdMpmMfpGbcBqCAzQJIdhsNsf2+W2Wzjqi+ncaHSJCMpmk0Wjo3MEJpyE4QOMegjpNoj++72OM0drgiPE8j2g0SqFQ0NdmgmkIDtC4h6BOmO+P9guOrkQigW3bLC8vj+3nVJ1d4EIwaFspjTOtCfZH+wVHV6dZtF6vk8/nh10cNQSBC8EAZSAiMvY7SYCGYD+0X3B0hUIhwuEw+Xyeer0+7OKoLRa8EAygcT3xWZaFiGhzaB+0X3C0dZZU02bRyaMhOGDjXBMEnSbRL11HdLTZtq1Lqk0oDcEBG/cQ1Anz/dH9BUdfJBIhHA6Ty+X0y8oE0RAcsHEPQa0J9k/7BUdfMpnEsiyWl5f1y92ECFwIBu30oSGoOrRfcPRZlsXU1BT1ep1sNjvs4qgtELgQDJpxD0HLsjDG6LfmPriuq/2CAeB5HvF4nGKxSKlUGnZx1IBpCA7YuIegTpjvn2VZ2i8YELFYDM/zyGQyOm1izGkIDtikhKA2ifZH+wWDQUSYmpoC0GkTY05DcMDGPQR11Zj16ewvqE2io8+2bZLJJNVqVVeTGWOBDMEghcq4h6A2h66P7/uIiO5jFxDhcJhIJEIul9Nm7DEVyBBsBihTxj0ERQTLsrQm2CcRwfd9DcEASSaTOI6j0ybGVCBDMEihMu4hCGgIrlMoFKLRaGiTaEB0+gebzSbLy8vDLo7aZMEMwWEXYB0mIQQ7m+uq/oRCIW0SDRjXdUkkEpTLZd2Ed8wEMwQDlCnjvqcg6IT59bIsC8/zNAQDJhqNdvsH9bUbH8EMwQDVBSchBHX90PULh8PU63Vqtdqwi6LWIZlM4nke6XRaX7sxEcwQDFCeTEII2ratq8ask+/7AFqjCJhO/6BlWaTTaX3Pj4FAhmCQTEoIgs4VXA/btrVJNKBs22ZqaopGo0E6nR7rz/YkCGQIBuk9NwkhqBPmNyYcDlOr1XRZrgDyPI9kMkmlUtGBMgEXzBDUPsGRohPmNyYUCgHaJBpUkUiEaDRKPp+nWCwOuzhqg4IZggHKk0kIQa0JbkynSVR3KgiuRCKB7/tkMhmd9xlQwQzBYRdgA8Y5BHXVmI0LhULUajX92wXUqQNl9HUMnmCGYIACZRJqgqAT5jeq0ySqtcHgsiyL6elpms0mS0tLY/9ZHzeBDMGgrR0KkxGC+i14/RzHwXVd7RcMONd1uzvSaxAGSyBDMEjtoZMSgjphfuNCoRDValW/RARcKBTqjhjVNUaDI5AhqKNDR0+nJjjuz3MQdJTo+IhEIiQSCUqlEplMZtjFUX0IZggG6Dw7SSEIOk1iI1zXxXEcDcExEYvFiEajFAoF3Yw3AIIZgsMuwDqISDcIx5lOkzg3nSZR/RIxHpLJJOFwmGw2q3MIR1wwQzBgtapJ2U4JtCa4UeFwGGOM1gbHSCqVwvd9lpeX9XUdYcEMwWEXYAMmJQS1Jrgxruti27aeLMeIiDA9Pd3ddUIn04+mYIZgwPJkEmqClmUhIhqC5yAcDlOpVLQ2PUY6QWjbNktLS7r90ggKZggGrC44CSEIOk3iXIVCIYwxVCqVYRdFbSLLspiZmUFEWFxc1CAcMYEMwYBl4MSEoE6YPzee52mT6JiybVuDcEQFMgSDFieTEoK6fui5C4VClMvliXi/TBrHcTQIR1AwQzBg54dJCUFdP/TcdZpEtTY4njQIR08gQ7AZsECZtBCchOc6KL7v4zgOhUJh2EVRA+I4DrOzs90g1FGjwxXIEAzaKXZSQlAnzG+OSCRCtVrVWsIYs22b2dlZLMtiaWlJg3CIBhqCIvIiEfm2iBwRkRtX+f8niMg9IvI1EfmGiFzdz/0GLVAmJQR1wvzmiEQiiIiuNDLmOoNlNAiHa2AhKCI28F7gp4F9wCtFZN8ph/0u8BFjzDOA64A/6+e+g5YnkxaCWhM8N5ZlEQqFKJVKE/G+mWS9QahNo8MxyJrgFcARY8x3jDFV4FbgZaccY4BE+/ckcHyA5RmaSQlBbQ7dPNFolGazqZvtToBOENq2zeLior7mW2yQIbgDeLTn8tH2db1+H3i1iBwF7gR+fbU7EpHXicgBETkAWhMcVZ1VY7Q59Nx5nofrujpAZkJ0+ghd1yWdTuvrvoWGPTDmlcAHjDE7gauBvxWR08pkjHmfMWa/MWY/BHPFGAheX+ZG6IT5zROJRKjVajpAZkJ0VpYJhUJkMhmy2eywizQRBhmCx4BdPZd3tq/r9cvARwCMMf8BhIDZte44aFkySSGoE+Y3TzgcRkS0VjBBRISpqSmi0Sj5fJ7l5eWJOG8M0yBD8MvARSKyR0Q8WgNfPnbKMd8HfgJARJ5CKwTn17rjoL0lJikEdcL85rEsi3A4TKlU0r/pBBERkskk8XicYrHI0tKSvv4DNLAQNMbUgTcAdwEP0BoFekhE3iYi17QP+y3gtSLydeBDwGtMH0kRtDCZtBDUmuDmiUajGGN0sMQEisfjpFIpqtUqi4uL+rkaEGeQd26MuZPWgJfe697a8/th4Dnrvt9zL9qWmqQQtCwLYwzNZrM7WlRtnOu6eJ5HoVAgGo0Ouzhqi0UiESzLIp1Os7CwwMzMDI4z0NP2xAnkWSpoWTJJIagT5jdfJBKhXq/rHLIJFQqFmJ2dxRjDwsKCriu7yQIagsEKk0kMQW262TzhcBjLsnSAzARzXZe5ubnu5ry5XG7YRRobwQzBYRdggyYhBHXC/OYTEcLhMOVyWWvYE6wzlzAcDpPL5Uin0xNxThm0YIZgwF73SawJ6sl6c3UGyOh6opOtM4UikUhQKpVYWFigXq8Pu1iBFswQDFhdcJJCUER0ruAAOI6D7/saggqAWCzGzMwMjUaDhYUFKpXKsIsUWMEMwYBlySSFIOg0iUHpDJDRE56C1t6TvdsxaZ/xxmgIboFJC0HLsrQ5dABCoZAOkFErdDbo9X2fTCZDOp3Wz946BTMEtTl0pGlNcDBEhEgkQrlc1n4g1WVZFtPT0yQSCcrlMgsLCzqdZh2CGYIByxIRmZidJEBrgoMUjUYREfL5/LCLokZMp5/QGMPi4qK+R/oUyBAMok5tcBLYto0xRmuDA2DbNpFIhFKppLVBdRrP85ibm8P3fbLZrK472odAhmBQK1STUhPUaRKDFYvFAPSbvlpVp3k0mUxSqVSYn5/X5tGzCGYIBqxPECZnY13QCfODprVB1Y9oNMrs7CwiwsLCArlcbmLOQesRzBAM4Os4SSHYWeBXT9CDo7VB1Q/XdVesMrO4uKify1MEMgSbAQyTSQpBy7KwbVt3RB8grQ2qflmWxdTUFFNTU9Trdebn58/py5Nt21x22WXdn0ceeYTFxUWe//znE4vFeMMb3rBpZb/55pvZu3cvF198MXfdddeqx3z605/m8ssvB9gnIp8Tkb3reYxA7skRxCiZpBCE1jdQPTkPViwWo1gsks/nSaVSwy6OGnHhcBjP88hkMmSzWcrlMqlUat1bM4XDYe6///4V1xUKBW666SYOHjzIwYMHN6W8hw8f5tZbb+XQoUMcP36cq666igcffLA75qDj9a9/PXfccQf79u07DHwQ+F3gNf0+TiBrgkHMkkkNwUl6zltNa4NqvWzbZnp6mlQq1a0VbsbiC9FolOc+97mEQqFNKGXLHXfcwXXXXYfv++zZs4e9e/dy3333nXaciJDNZjsXk8Dx9TxOIGuCQawLishEjZZ0HAdjDPV6Hdd1h12csaW1QbURkUgE3/dZXl4mk8l0a4Wn1rJWUyqVuOyyywDYs2cPt912W9+P+8Y3vpF77rnntOuvu+46brzxxhXXHTt2jCuvvLJ7eefOnRw7duy0295yyy1cffXVAE8HIsCVpx10FoEMwSBWLiaxJghQq9U0BAfItm2i0SiFQoFYLKa7jqu+2bbNzMwMhUKBbDbL/Pw88XicSCRy1nnNqzWH9uvd7373Rot71vu88847ufLKK78BfBh4F/Ar/d4+kJ+YIEbJpIWgbduIiDbTbYFOCGptUG1ENBrtrj2ayWQoFoukUqmBfHldT01wx44dPProo93LR48eZceOHSuOmZ+f5+tf/zrPetazOld9GPjX9ZQpmCEYwCyZtBAUEVzX1RGiW6BTG8zn81obVBviOA4zMzOUSqVurTAajRKPx7vzfjfDemqC11xzDddffz033HADx48f56GHHuKKK65YcczU1BSZTIYHH3ywc9VPAg+sp0yB/LQEMUwmLQSh9cEql8vDLsZE0Nqg2gzhcBjf98nlchQKBcrlMslksq8BL7t37yabzVKtVrn99tu5++672bdv34bLcskll3Dttdeyb98+HMfhve99b7fP8uqrr+aWW27hggsu4P3vfz8/+7M/C7AP+EXg/1nP40jQTsz++ReZz3z+i1z5xJlhF2VdcrkcuVyOCy64YNhF2TKFQoFMJsP27dv76nBX5yabzZLP59m2bZvWBtU5q1arZDIZarUaoVCIZDI50p9jEfmKMWb/em+nUyS2yKRtpwS6csxWi8ViusOE2jSe5zE7O0sikaBSqXDy5MmxXHotmCEYwKExkxiCvSNE1eBZlkU0GqVYLOrfXG0KESEWi7Ft2zZCoRC5XI6TJ09SKpWGXbRNE8wQDGCOTGII6vJpWy8Wi2HbNplMZthFUWPEtm2mpqaYnZ3FsizS6fTYbN6rIbhFJjEEQZdP22qWZRGPx6lWqxSLxWEXR42Zzn6FqVSKRqPBwsICy8vLgd4xJpghqM2hgeE4ji6ftsUikQie55HNZidqlSK1dSKRCHNzc8RiMUqlUqD7C4MZgsH7O09sCLqu210+TW2dZDKJMaZ3TUWlNpVlWSQSCebm5rr9hSdOnKBQKATqPBfMEBx2ATZgkkMQdHDMVnNdtztIZhz6bdTochyn21/oui6ZTIaTJ09SLBYDcb4LZggG4A97qkkNQV0+bXh6B8lM2vtObT3P85iZmWFmZgbLslheXmZhYWHkF8wIZggOuwDnYNJORiKC4zhaExyCTnNVrVbTQTJqy/i+z9zcHFNTUxhjWFpaYmFhgUqlMuyirSqQIRjEFJzUmiCga4gOUe8yWEEewaeCJxwOrxhJuri4OJJhGMgQ1NGhweK6Ls1mU0/CQ6KDZNSwiAiRSIRt27aRTCZXhOGoNJMGMwQDmCOTHIK6fNpwOY7THco+at/C1WQQEaLR6IowXFpaYn5+fuhhqCG4RSY5BHWE6PDpIBk1CnrDMJVKdfsM5+fnKZVKQ3lvBjMEh12ADRCRidxOCXT5tFEgIiSTSer1OoVCYdjFUROu00za6TM0xpBOp5mfn9/yeYaB3G+lGdAgmdQQBF0+bRSEQqHupOZQKKTbLamh64RhOBymXC53t1/L5XJEo1Gi0eimbuq7mmDWBAOaI50m0Umky6eNhmQyiYiQTqf1tVAjQ0QIh8PMzs4yOzuL53ndFWgymcxAv0AHMgSD2SDaMqknHl0+bTTYtk0qlaJWq5HL5YZdHKVO43ke09PTbNu2jXA4TLFY5OTJkywtLQ1kYFcg20OCmiOT3hwKrcExnd/VcIRCIaLRKPl8Ht/38X1/2EVS6jSO45BKpYjH4xQKBYrFIuVyubskYDgc3pTWtUDWBIMaI5Mcgrp82mhJJBK4rsvy8rLuNKFGmm3bJBIJtm/fTiqVAmB5eZkTJ06QzWbP+ZyiNcEtNMkhqMunjRYRIZVKdfeDm56eHnaRlDqrziCaSCRCtVqlUChQKBTI5/OEQqEN328wQzCgdcFJDkFoNYkOe2Ks+gHXdUkkEmQyGQqFAtFodNhFUqovnufheR6NRoNisXhO036C2Rwa0BzRENTl00ZNNBolFAqRzWa1lq4Cx7Zt4vE427dv3/B9BDMEh12ADZr0ENTl00ZTKpXCsiydNqEC61wGyAQzBAP6QZ30ENTl00aTZVmkUinq9bousq0mTiBDMKgmPQR1+bTR5fs+sViMQqGg/bZqogQyBIOaI5MegqDLp42yeDzenTahr5GaFIEMQV07NLh0+bTRJSJMT08jIiwtLen8QTURAhmCQT1/TvJ2Sh26fNpos22bqamp7n5vk/xeVZMhmCE47AJskIagDo4JAs/zSKVSVKtVMpnMsIuj1EAFc7J8QENEQ1CXTwuKcDhMvV4nl8t1d6ZXahwFMwSHXYAN0hDU5dOCJB6Pd6dNOI5zTktTKTWqAtkcGtQU1BBscV1XQzAgUqkUnueRTqf1NVNjKZAhGOS1Q0FDUJdPCw4RYWpqCsuyWFpa0tdMjZ1ghmBAM0RDsKUzOKZarQ65JKoftm0zPT2NMUZHjKqxE8wQHHYBNkhDsMV1XSzLGsgu0WowXNft7kiva4yqcRLMEAzo509DsEVE8H1fQzBgQqEQyWSScrmsQajGRjBDMLB1wRY9ebTWqmw0GjrYImCi0Wg3CJeXl/W9rAIvmFMkAvq505rgD/i+D0ClUun2EapgiEajGGO6O06kUqlz2spGqWEKZk0woCGiIfgDtm3juq42iQZULBYjkUhQKpV0VRkVaMGsCQ67ABukIbiS7/sUCgWazSaWFcjvYxMtFothjCFpOjRQAAAgAElEQVSXywGtGqFSQRPIM09QM0REdCeJHr7vY4zRqRIBFo/HicfjFItFrRGqQBpoCIrIi0Tk2yJyRERuPMMx14rIYRE5JCIf7Od+7/32SV7+3s/TaAYvTDQEf8DzPEREm0QDLh6Pdzfk1SBUQTOw5lARsYH3Aj8JHAW+LCIfM8Yc7jnmIuC3gecYY9Iisq2f+77n2/MAZEs1pqLeppd9kHQAwQ90pkqUy2WSyeSwi6POQSKRwBhDoVAA0NdTBcYga4JXAEeMMd8xxlSBW4GXnXLMa4H3GmPSAMaYk+t5AMsKXqBoTXClzlQJ3VUi+JLJJNFolEKhoPMIVWAMMgR3AI/2XD7avq7Xk4Eni8jnReSLIvKi1e5IRF4nIgdE5MCK/wjoZ0xPDj/Q2ZlAm0THQzKZ7I4a1d3pVRAMe2CMA1wEPA94JfB+ETltiJkx5n3GmP3GmP291zcCGCZaE1zJtm0cx6FcLg+7KGqTxGIxpqamqFarLC4u6qLbaqQNMgSPAbt6Lu9sX9frKPAxY0zNGPNd4EFaodiXaj143zI1BE/n+z7ValX/LmMkHA4zPT1NvV5nYWFBVwZSI2uQIfhl4CIR2SMiHnAd8LFTjrmdVi0QEZml1Tz6nX4f4IP3fX9zSrqFNARPFwqFMMZok+iY8X2f2dlZABYXF3UqjBpJAwtBY0wdeANwF/AA8BFjzCEReZuIXNM+7C5gUUQOA/cAbzbGLPb7GFoTHA86VWJ8ua7LzMwMlmWxuLiozd5q5Ax0xRhjzJ3Anadc99ae3w1wQ/tn3TKl4DWxaAieTneVGG+O4zA7O8vS0hJLS0skEglisdiwi6UUMPyBMefkQ9ocOjZ836der+tUiTFlWRYzMzOEw2Gy2axOoVAjI9AhGEQagqvr3VVCjScRYWpqqjuFYn5+Xr/0qKHruzlURHYAF/bexhjz2UEUapxpCK7OcRwcx6FSqRCNRoddHDVAsVgM13VJp9MsLCyQSqW680WV2mp9haCI/CHwC8BhoDPpxwAaguukIXhmvu9TLBYxxujycmPO933m5ua6/YSxWIx4PK6vu9py/dYEXw5cbIzRtqpz1Ludkn7gV+psrVStVrvNo2p82bbN7OwsmUyGfD5PrVZjampKt9VSW6rfd9t3AN3+exPonoJn5vu+TpWYMCJCKpUilUpRrVaZn5/X+YRqS/VbEywC94vIp4HuGcoY8xsDKdUY0xA8MxHB8zzK5TKJRGLYxVFbKBKJ4DgO6XSaxcVFYrEYsVhMW0vUwPUbgh/j9NVeRsKx5RI7UuFhF6NvGoJn5/s+2WyWRqOBbdvDLo7aQp7nMTc3RzabJZfLUS6XmZqawnEGOp1ZTbi+mkONMX8NfAj4Svvng+3rhu4dH39g2EVYFw3Bs9NdJSabZVmkUimmpqZoNBrMz8939yhUahD6HR36POCvgUcAAXaJyH8ahSkSJ7LBWoZJQ/DsHMfBtm3K5TKRSGTYxVFDEg6H8TyP5eVlMpkM5XKZVCqlrQNq0/XbzvDHwAuNMd8GEJEn06oZPnNQBetXMhys8ToagmsLhUKUSiUdQTvhbNtmZmaGQqFANptlfn6eZDJJOByc7g81+vodHep2AhDAGPMgIzJa9Bd+eNfaB40QDcG1+b5Ps9nUxZYVANFolLm5OWzbJp1Os7S0pHsUqk3TbwgeEJFbROR57Z/3AwfWvNUAveIZrU3qbStYNQUNwbX5vo9t2xSLxWEXRY2IziLciUSCSqXS7SvUz5E6V/2G4OtprRbzG+2fw+3rhuYbxzIA/PHdDw6zGBumH94zExEikQiVSkXXllRdIkIsFmNubg7XdclkMrphrzpnffUJtleKeVf7ZyQcOZkH4PBj2SGXZH20JtifSCRCPp+nWCzqnEG1guM4zMzMUCqVun2F0WiUeDyuq82odTtrCIrIR4wx14rIN2mtFbqCMebpAyvZmNIQ7I9t24RCIYrFoq4pqVYVDofxfZ9cLkehUOgusqADZ9R6rFUT/M32vy8ZdEEmhYZg/yKRCKVSiXK5rCc2tSrLsrojRjOZDOl0utt64LojMXZPjbizth0YYx5r/7oAPGqM+R7gA5cCxwdctr48Z+/MsIuwLiKiO0n0yfd9HMfRydJqTZ7nMTs7SzKZpFarMT8/z/Lyso4iVWvqtwH9s0Covafg3cAvAh8YVKH6se/8Vj/R548sDrMYG6Ih2L9IJEK1WtXBD2pNIkI0GmXbtm3EYjFKpRInT54kl8vRbDaHXTw1ovoNQTHGFIFXAH9mjPl54JLBFWttL730gu7vX3h4YYglWT8Nwf5FIhFERKdLqL5ZlkUikWDbtm2EQiFyuRzz8/PdvSqV6tV3CIrIs4FXAR9vXzfU9Yte/LTzu79f//4vMZ8LzlqTOsijf5ZlrVhBRql+2bbN1NQUs7Oz2LbN8vIyCwsLlEqlYRdNjZB+Q/C/Ar8N3GaMOSQiTwTuGVyx1uY6wlPO/8HQ+Y8ceHSIpVk/PaH3LxqN0mw29eSlNqTTXzg1NYUxhnQ6zfz8vK5IpID+5wl+BvhMz+Xv0Jo0PzSOZfFAzxzBcq2BMYZPHHycF+7bjmOP7nwhbQ5dH8/zcF2XYrGoi2qrDQuHw4RCIcrlMrlcjqWlJVzXJR6Pd3cvUZPnrEkhIn/S/vefReRjp/5sTRFX550Sck+YjvCPXz3Gr/79V/nAFx4ZTqH6pCG4fjpARm0GESEcDjM3N0cqlcIYw9LSktYMJ9haNcG/bf/7PwddkPVy7JX9am/+h290f3/7xx/goRN5/vDnRnMuv4bg+oXDYbLZLIVCgVQqNeziqIDrLM0XDocplUrk8/luzTAWixEKhbTvfkKsNU/wK+1fDwD/boz5TLtp9HPAlwdduLNxbGE66p3x/z88wn2EGoLrZ1lW94Slw93VZumEYW/NsNNnqAt0T4Z+O84+DfR2xoSBT21+cfrnWhaXXHD2NSXTheoWlWZ9NAQ3JhqNYozRATJq03XCcNu2bUxPT2NZFplMhhMnTug8wzHXbwiGjDH5zoX270MdoWD1sYXSM276JLtv/Dhf+356C0rU8shCgd03fpw/+dSDNJsrg+5bj2c5cjKHiFBvnPuHatKC1HXd7gAZpQYlFAoxOzvL7OwsnueRy+U4ceIEmUxGdzUZQ9LPiVREPg/8ujHmq+3LzwTeY4x59oDLdxr//IvM+f/pT3jknS/msUyJD933KH/66YfWvN0j73zxQMrz1e+necWffQGAN171ZN79qZVbO+2/cIo/feUz+JF3/tuqtz/033+KiGevWFP08WwZQTAYnn3zvxHzHT7xmz/KXNznIwce5a13HDrtfg787lXMxnz+92ce5p2f+NZZy3zfW36CubhPo2loGINnW+Qrdb63WOSpO5J8+oETLBaqXLt/9DYsLhaLLC8vd09QSg1avV4nn89356r6vk80GsX3fe03HCEi8hVjzP51367PEPxh4FZa64UKcB7wCz19hlumNwQ7dt/48e7vn7/xBbz6li/x3YWV601+66YXEXI3Z35/vdHkvu8u8eZ/+AbHlse/ae5Lb/kJtidaQ8iNMRxbLnE0XeL4colitcG2uM/D8wWed/Ecu6YjRD2bLzy8yH88vMgXv7PIdNTjRLZMrlLn7S97Klc+caavmvxqjDGcOHGita5oOMZbbjvIP3999WVsn3/xHK+4fCc/uW873zyW4QnTke7zOFeNpqHebGKLjPR0HGMMD57Iky5WuSAZZrFQIRXxuHC61ZCzWKiSLdcoVRs8dDJH3HdJRVwMrQ2rBZiN+RSrDb67kKfRhG8cW+ZTh0+wZzZKpd7k3x9a4KWXXsDDJ/MUqnWu3DPDw/N5XvCUbdz7rXnmEj6VWgNjWs2Ou6bDnJcIcXy5xMHjWb55LMPumQi/+OzdFCp1ipU68ZDL0XSRXdMRGk1DrdHk2HKZmG+TDLtMRT3u+dY8xhhSEQ/ftWg0DA+ezHF+MsSuqQiPZcqUaw1KtQY7p8JEPIewa5MIt8YDhlybqOfg2MJysYbvWJRqDTzHomnAsYRHFgscS5fIletsi3v4Nnz2oQWefn6UkGtTx+bJ5yXIVRqUqq11SrPlGtlSjb3b4hw5mePKJ7bWN56JeWRLdcq1BuclQ3i2RSLskgi5FKp1CpU6+UqdLxxZ5NF0kYfn8zzzwimmox5f/d4yl+5K8cBjWSKeTbHa4BtHl9m7LcaDJ1qNdE/bkeSyXSm+fnSZvXMxLEv46vfShFybw49l+Ykf2kbDGB7PlPnW4zm2xX2aBn7solku2ZFkRyqMYwk7psLMxnxCrkWp2mA+X8F3bHzH6r4WtiUcWy4hCPVmk1rDMBvziHgOtiVsS/hE2793NJuGpWKVXLmOYwmJkEul0SDiORQqdXLlGo0m5Cs1KrVWS1mtaciX61QbDTzbJuLZVOpNEiEH37XIlupMRT2SYZcnzsUGF4IAIuICF7cvftsYM5Sx6quF4H88vMirbvki//LrP8q+CxIrQrHj337rx3niXGzDj5sp1rj0bXevedxLnn4+77n+cppNwxPfcueK/3v4HVfz+r/7CncfPsH7fv7JvO6jG9sQ+CVPP5+3veypTEc9vvb9ND/Trol23PyKp/HCfdv5sT+6h0K1wauvfAJXP+18Ln/CFC/4n/dyPDNaQ8Gvesp2fufFT+H8ZIh0sco3jmaYjXl8b7HIzZ/41sBWA4r7DuenQrz8GTtYLtbIlevc991FHp4v4NkW1UaTuO+Qq9S5/Akp9u+e5n2f/c6q9xX1bArVBtsTPieyPyhvKuJyfjLMjlSYF/zQNh46mePgsQxffiTNFXumOXIyj+9Y7Ds/wa+9YC+HjmX43mKRBx7P8shCkSdvj/GcvbN86btLfPLwCV64bzt3Hz7BSy+9gPlcGc+x2Rb3sUX43JEFkmGX6ajH544EaynBQem8fuOiE4AdtiU0mhvvFrEEzuHmI+V7f/iSgdYEI8ANwIXGmNeKyEXAxcaYf1l/Uc/NRZdcat71wU+sWDv0VCezZa54x6f55Bt/jBs+8nW+eSzDLb+0n/27p0hF+m9CSxeqPOOmT3L9s57AB7/0/VWPecEPbeMvX/PDlKoNQq7VV/NIPp8nm81y/vmtpd9EhC9+Z5FEyGVbwmc25q843hhDodog5p95RosxhqZhxTevc1WuNfjPf/Vl/uM7qy9S/sTZKN9p17gvuSDBoeMrNzjef+EUy6VadwPkPbPR02ro5yLm27zphRczFfV4+s4UvmMR9R1qjSaHjmc5eCzD/7jr25v2eEEUDzk87+Jt/PPXjzMX97l0Z6s2cWy5xFN3JHg8U2bvthgxv/WtPeo7fHehQKNpePaTZoj7DncdOsHPPGMHDzyW5fILp7hsV4qo5+A6gi0CAr5tIxYs5avUm03qTUOh0sB3LDzHYjrqMR3xqDWbOFarxpUuVAl7Np5jYQycyJY5ma0QclvN8988muGi7XF81yLmO5SqDSr1JhekQsR8h0TIpdZoYllCqdqgaQyObRHzHEKehe+0Wn6MMcznK/z7gwvEQw6NpiHk2RxdKuI5FhdtjyO0aveObbXuU4RcucbF58Wp1JoYIBV2eXg+j9uuwTWNYSlfplGtsJQrMhd1uCAVIRaLEAlHqDfh2HKJerPJIwtF0sUql+5MkS5WyZRqPJYp49nC9kTr+YQ8G8+2CHs2e2aiLBaqxENO3y1Yxhgq9SaNpqFUa5AKu321UpRrDY4vlyjVGjzwWA5jDJ8/ssAji0XOS4T40SfPciJbQYBSrcGnHzjBCy85j71zMRy79bd/PFvmvESIWtOQKVZ58ESeL313kVTY4wkzETLFGlHfZibmU6k32TkV5vuLRWZiHseXy+xIhdg1HSHiOTSaTZaLNZ58XpzjyyXOT4ZwbYtPHj5BMuyys92KMRfzMAa+t1RkLubzc/t3DTQEPwx8BfglY8xT26H4BWPMZet9wHO1f/9+c+DAgb6P/6N//RZ/du/D3cv99g3WG032/s4nVv0/1xYe+oOr+y7DqQqFAplMhvPOO28id8JuNlsnpWypxs/82RfIn+Wb+h/93NO55tILTjsRZLNZ8vk8c3NzG9o3rlpv8rZ/OcSxdInlUo1MscaPXjTLq6+8kETYpVpvkgi5FGt1pqMehUqDRMg540mlVG20bhN2qDaa3Zrk45kyMd/h/f/+XaKejQhcu38XU1EPAXLlOsulGjf+4zeIh1x2ToXZNR1hJurx/aVWk9iLn3Y+pVqDHakw2xMhZmM+tiU8mi7y7cdz5Ct1nv3EGXakwiwUWk1XiZCj/VVbqLOsX7FYpFarISKEQiEikQie5+lrsQUG3Sd4wBizX0S+Zox5Rvu6rxtjLt1AWc/JekPwyMkcV73rsyuu2xb3+eJv/8QZ+6XOFIDfvfnqTXkzdwZ3bN++Hdse6jrkgdVsNjl58iSu6zIzE6w9JdV4q9VqFIvF7pxW27a7E/Mdp6+VKtUGbDQE+31FqiISBkz7wZ4EBGLbhr3b4qdddzJXOa2/7l9+/bk8linz2r9ZGbD3vul57JqObGozo+4uf+4syyIej5PJZCiXy7r2oxoZruuSTCZJJBKUy2VKpRK5XI5cLofneYTDYcLh8ES2Ao2ifkPw94B/BXaJyN8DzwFeM6hCbbZ/+60f5wV//JmzHvOS//9zp1334Nt/Gs/Z/DeqhuDmiEQiFAoFcrmcDldXI6ezTmk4HKbRaHSbSzOZDNlsFt/3CYfD+L6vgThEa/7lpXVm+RatDXVfA3wI2G+MuXegJdtET5yL8cg7X8ylu1I888Kpvm5z26/+yEACEDQEN4uIEI/HqdVquoqMGmm2bROLxdi2bRtzc3NEo1FqtRrpdJoTJ06QTqcpl8uBOCfYts1ll13W/XnkkUdYXFzk+c9/PrFYjDe84Q2b9lg333wze/fu5eKLL+auu+5a9RhjDL/zO78D8FQReUBE1rXD0Zo1QWOMEZE7jTFP4wcb6gbSHb/2nO7vnzx8gn/66lHecvVTqNSbXPWuVk3xnjc9jz2z0YGWQ0Nw84TD4W5tMBwOa21QjbzOykeJRIJqtUqpVOr+WJa1ooY4iu/ncDjM/fffv+K6QqHATTfdxMGDBzl48OCmPM7hw4e59dZbOXToEMePH+eqq67iwQcfPG0cxQc+8AEeffRRgIPtsSvb1vM4/VZ1vtqeMD82fnLfdv781c9k13SEvdt+MH9w0AEIGoKbLZFI0Gg0yOfzax+s1AjxPI9kMsn27duZnp4mFApRqVRYWlri8ccfZ2lpKRCLxkejUZ773Oduat/8HXfcwXXXXYfv++zZs4e9e/dy3333nXbcn//5n/PWt761e9kYc3I9j9Nvn+CzgFeLyCNAgdaqMcYYM5p7FW3AoJZVW42G4ObyPI9QKEQ+nycSieiIWxU4nSkVoVAIYwzVapVyudz9ERF83ycUCuH7/lDf46VSicsua82O27NnD7fddlvft33jG9/IPffcc9r11113HTfeeOOK644dO8aVV17Zvbxz506OHTt22m0ffvhhPvzhDwM8RUQ+AfyGMWbttTTb+g3Bn+r3DtXaNAQ3XyKRYH5+nnw+TzKZHHZxlNqwTuD5vk8ymewGYqlU6m7867puNzQ3Mk/2XKzWHNqvd7/73ZtcGqhUKp0a6APA+4G/BH6039ufNQRFJAT8F2Av8E3gL4wx47MG0ZBpCG4ex3GIRCIUi0Wi0ajOx1Jjw/M8PM8jkUhQq9WoVCqUy+XutAvbtlfUEkexH7FjPTXBHTt2dPr6ADh69Cg7duw47bY7d+7kFa94BW9605sAbgP+aj1lWutM8ddADfh34KeBfcBvrucB1Om0JjgY8XicUqlENptlenp62MVRatN1BtXEYjGazSblcplKpdKdfiEieJ7XrUludS1xLeupCV5zzTVcf/313HDDDRw/fpyHHnqIK6644rTjXv7yl/cG648D61qUea0Q3NceFYqI/AVweq+kWjcNwcGwLItYLEY2m6VSqeD7/to3UiqgLMsiEokQiUS6/YiVSoVKpUI221rHt1NL7PwMcj7i7t27yWazVKtVbr/9du6++2727du34fu75JJLuPbaa9m3bx+O4/De97632xd69dVXc8stt3DBBRdw44038qpXvQpalbSbgV9Zz+Ocddk0EfmqMebyM10ehvUumzaqHnvsMaLRKIlEYthFGSvGGE6ePIllWczOzo5005BSg9JoNLqBWKlUuqNLXdfF9/1uE+s4TdIf1LJpl4pIZ2sAAcLty53RoXoG3yAR0ZrgAIgIiUSCdDpNLpfTLxlqInXWK+3UEjt9idVqlUKhQD6fR0RwXbfbfDqpC32fNQSNMTrWfEA0BAcnHA5TqVTI5/PdZiClJlWnn9DzWtvIdZpOO82nZwpF13XHqqZ4JjqEbkg0BAerM7R8eXmZubm5ifgwK9WP3ikY8XicZrPZDcXemiJwWk1xHD9HGoJDMonNDltJRJiammJhYYHl5WUdLarUGViW1Z1zCCtritVqlWKxSKHQ2gzbcRw8z+uGo+MEf99KDcEh0prgYLmuSzweJ5vNUiwWiUQiwy6SUiOvt6YIdPsUO6FYLpcpFotAK0A7gdgJx6DVFjUEh0SbQ7dGLBajUqmQyWS631yVUv07tU8RoF6vrwjGXC7X/T/HcbrB2JnXOMq1RT0jDImG4NZJpVLMz8+TTqd12oRSm8BxHBzHIRwOA9BsNqnVat2fzu4Y0DrXdYJxFJtRNQSHRERGfmX4cWHbNqlUiqWlJZ02odQAdLaA6h2J3Wg0VoRibzPqqcE4zBqjhuCQaE1wa4VCIaLRqE6bUGqL2LaNbdsrtlfqNKN2fnqDETgtGB3HGfiOGRqCQ6IhuPUSiQSVSkWnTSg1JKc2o8LKGuOpTanQCtNOOPb+u1m1Rg3BIdEQ3Hq90ybS6TTT09Mj0y+h1KRarcbY6WPsrTkWCoXuOVNEsG17RTBulIbgkGgIDofruiSTSZaXl8lkMqRSqWEXSSl1itX6GI0x3Vpjbzj21ho3QkNwSDohaIzR2sgWi0QiNBoNcrkclmXpQBmlAqAzmObUaU7NZpN6fePb3GoIDokG33B1lovK5/PdLZiUUsFjWdaKOYzrpSE4JJ0QbDabAx/9pFaXSCRoNptks1ls217RWa+Umgw6PG5IOlX6c6nGq3MjIqRSKXzfZ3l5mUqlMuwiKaW2mIbgkHRGM2kIDldnxKjjOCwtLVGtVoddJKXUFtIQHBLLsrAsi1qtNuyiTDzLspiensa2bZaWlvSLiVITRENwiFzX1RPuiLBtu7vd0uLiIo1GY8glUkpthYGGoIi8SES+LSJHROTGsxz3syJiRGT/IMszahzH0ZrgCHEch5mZGYwxLC4u6hcUpSbAwEJQRGzgvcBPA/uAV4rIvlWOiwO/CXxpUGUZVa7rYozRk+0IcV2X6elpms0mCwsL+iVFqTE3yJrgFcARY8x3jDFV4FbgZascdxPwh0B5gGUZSTpCdDR5ntfdcmlhYUFHjSo1xgYZgjuAR3suH21f1yUilwO7jDEfP9sdicjrROSAiByYn5/f/JIOiYbg6HIch9nZ2e6o0d6V7pVS42NoA2NExALeBfzWWscaY95njNlvjNk/Nzc3+MJtEcuysG1bm9xGlG3bzMzM4Hkey8vL5PP5YRdJKbXJBhmCx4BdPZd3tq/riANPBe4VkUeAK4GPTdrgGB0hOto60yfC4TDZbJZMJqMLnys1RgYZgl8GLhKRPSLiAdcBH+v8pzEmY4yZNcbsNsbsBr4IXGOMOTDAMo0cx3Go1+t6Yh1hnQn1sViMQqHA8vKyvl5KjYmBhaAxpg68AbgLeAD4iDHmkIi8TUSuGdTjBo2OEA2ORCJBIpGgVCqxsLCgr5lSY2CgC2gbY+4E7jzluree4djnDbIso6p3cMy5bAyptkYsFsNxHJaXl1lYWCCZTOrC20oFmK4YM2SO4yAiWqsIkFAoxNzcHI7jkE6ntXlUqQDTEBwyEdERogHUGTkai8UoFovaPKpUQGkIjgAdIRpMIkIikWBmZoZGo8H8/LzOJ1QqYDQER4COEA023/eZm5vrzidMp9M0m81hF0sp1QcNwRHQGRCjTaLB1dmFIh6PUyqVmJ+fp1QqDbtYSqk1aAiOAF0+bTyICPF4nNnZWSzLIp1Os7S0pNsyKTXCNARHgG3bOkJ0jHQW4E4kElQqFU6ePEk+n9fmbqVG0EDnCar+iIjuLThmRIRYLEY4HCaTyZDNZimVSiSTSTzPG3bxlFJtWhMcETpCdDx1+gp79yjMZDI6cEapEaE1wRHhOA7FYpFms4ll6XeTcRMKhfA8j1wuR6FQoFQqEY1GicViiMiwi6fUxNKz7YjQEaLjz7IskslkdzpFLpfjxIkTFAoF7S9Uakg0BEeEjhCdHK7rMj093d20N5PJdKdUaBgqtbW0OXRE2LaNZVkaghOkM4q0UqmQzWZJp9O4rks8HicUCg27eEpNBA3BEaIjRCdTZ8WZUqlELpdjaWkJ13WJxWKEQiHtM1RqgDQER4jrurrKyAQLh8OEQiFKpRL5fJ50Oo1t20SjUSKRiA6YUmoANARHiOM4NJtNGo0Gtm0PuzhqCESESCRCJBKhXC6Tz+fJZrPk83kikQjRaFTfG0ptIg3BEdI7QlRPdCoUChEKhahWqxQKBfL5PIVCgXA4TDQa1U2YldoEGoIjREeIqtV4nofnecTjcQqFAsVikWKxiOu6RCIRwuGwNpUqtUEagiPEsixs29YQVKtyHIdkMtndqaJYLHaXZNPaoVIboyE4YnSEqFqLZVlEo1Gi0SjVapVisdgNRa0dKrU+GoIjxnXd7goiOjReraXTVJpIJE6rHfq+3x1xqu8lpVanIThiHMfBGEOj0ej2ESq1lt7aYa1Wo1QqUSqVKJfLiAihUIhwOIzv+5wUpysAABW5SURBVBqISvXQs+yI6R0hqiGoNsJ1XVzXJZFIUK1Wu4FYKpWwLKs76lQDUSkNwZGjI0TVZuptLq1UKt0wLBaL3RpiJxC1D1FNIg3BEdPZYFdDUG2m3sAzxlCpVCiXy91gFBE8z+seo/NU1aTQEBxBOkJUDVJvIAJUq1XK5TLlcplMJkMmk8FxHHzf7+6DqM2malxpCI4g13WpVCo6QlRtid4m03q93q0hFotFCoVCt5bo+z6+7+tcRDVWNARHUGeEaL1e1xOO2lKO4xCLxYjFYhhjurXEznZP0Nr2qxOcvu/rAC4VaPruHUGd4NMQVMMkIt3aH0Cj0aBSqVCpVLqjTkFDUQWbvltHkG3biAi1Wo1wODzs4igFtN6XnR0uoPUlrVqtnhaKlmV1Q9HzPFzX1WZ9NbI0BEeQjhBVQeA4Do7jnBaKnZ9yuQy03s+duYudUNTaohoV+k4cUY7jUK1Wh10Mpfp2aig2m81uINZqte5AG2jVFntD0XVdnZahhkJnx44oz/NoNBo6VUIFVmd1mkQiwczMDOeddx5zc3Mkk0lCoRDNZpN8Ps/S0hInTpzgxIkTLC0tkcvlKJfL2hIyomzb5rLLLuv+PPLIIywuLvL85z+fWCzGG97whk17rJtvvpm9e/dy8cUXc9ddd611+C4Rya/3MbQmOKLC4TDZbJZisUgymRx2cZQ6Z73Noh3GGGq1WventxkVWkHqOE73dp2mVO1jHJ5wOMz999+/4rpCocBNN93EwYMHOXjw4KY8zuHDh7n11ls5dOgQx48f56qrruLBBx9ctcXgwIEDsME80xAcUZZlEQ6HKZVKJBIJ/dCrsdSZg+h5Xve6TjDW6/VuOBaLRYwx3WM6Ta+dUOz86OdkOKLRKM997nM5cuTIpt3nHXfcwXXXXYfv++zZs4e9e/dy33338exnP3vFcY1Ggze/+c0AR4EnrfdxNARHWCQS6e4V1+lnUWrcrRaMQDcU6/V69/fOohKd29m2vSIUOz+6LurmKZVKXHbZZQDs2bOH2267re/bvvGNb+See+457frrrruOG2+8ccV1x44d48orr+xe3rlzJ8eOHTvttu95z3u45ppruPfeezfUd6QhOMI8z8NxHIrFooagmnidQOvVWVSiNxjr9fqKcARWhOOpv2vtcX1Waw7t17vf/e5NLcvx48f56Ec/yr333ssNN9ywofvQEBxx0WiUTCZDrVbTifNKnWK1fkaguydnb0DW63VKpRLNZnPF7W3bXjUkNSA333pqgjt27ODRRx/tXj569Cg7duxYcczXvvY1jhw5wt69ewGeBrgicsQYs7ffMmkIjjgdIKPU+nXm2q42H7HZbHZDsTcoTw1IYEVAnvq7ZVkakuu0nprgNddcw/XXX88NN9zA8ePHeeihh7jiiitWHPPiF7+Yxx9/HAAR+SbwQ+sJQNAQHHmdYeY6QEapzdG7os2pOgHZCcfOv5VKhUajseJYEemOXu0E5Kk/k/R53b17N9lslmq1yu23387dd9/Nvn37Nnx/l1xyCddeey379u3Dcf5ve3cfI0d933H8/d3du73bvSdzdkqxie3KxMpBWic6uaZCaVEQIifV/NEWGUwTWlJKWtoKp6mQUqkBVKEWtUhtrCbUoLbQFkJUwFUbmTZxS1tBwMgOsh0FXIKC7Qpz5/M97N7d7t1++8c+sLe3d/vg29vdm89LGu3M7Ozs1z/d3cfzm5nfRDh48GDhytCRkREOHTrEVVddddl1W3G/eTsYHh723OWwgTE3N8fY2BgDAwM6NyjSJPku1vyUD8niqVQoFCobjsXrgxSUjWRmb7j7cK2f05FgG8gPSqwLZESaZ6UuVlgakvkpk8mwsLBAKpVa0t0Ki4MyP1+6Tl2vjaMQbBOxWIzJyUnm5+c17qJIC6oUkpDtbs2HYmlI5keIKndECSwJydLAzK/T7SC10V/TNhGLxZiamiKRSOgCGZE2lQ+plYLS3QvBuNxrOp0mk8lQ7nRW/lxluXAsXdYRpkKwbegCGZFgKL5to5JyIVl6tLlSYAJLQrHSZGbr6u+PQrCNxGIxZmZmmJ2d1XMGRaSm7s/igCwOytLlSqFZ/L3FwVhpXauGp0KwjRRfIKMQFJFa1Hq+MN8tW2laWFhYtG2lGorDsdJ86bpGUAi2GV0gIyJroZZu2Tx3XxKelZarOfLM11MajsWv9dJf0TbT3d3N1NQUyWSSvr6+ZpcjIlJQHFS1Kg3QcvOlr/kQvZz73RWCbSYcDhONRkkmk/T29rZkH7uISK0uJ0Avh24oaUPxeJxMJrPo4aMiIlI7hWAbikajhMNhEolEs0sREWlrCsE21dPTQyqVUhCKiFwGhWCbisfjRKPRwpWiIiJSO4VgGxsYGMDMGB8fv6yro0REgkoh2MbC4TADAwOk02mmpqaaXY6ISNtRCLa5rq4u4vE409PTzM3NNbscEZG2ohBcB/r6+ohEIly6dKnisEUiIvIhheA6YGZs2LCBTCbDxMREs8sREWkbCsF1oqOjg97eXmZmZkgmk80uR0SkLSgE15H8bRMTExO6bUJEpAoKwXXEzAq3TVy6dEm3TYiIVKAQXGfC4TD9/f2kUindNiEiUkFDQ9DMbjGzH5rZGTN7oMz7B8zstJm9aWbfMbOtjawnKLq7u4nFYkxPT+tCGRGRFTQsBM0sDBwEPgsMAbeb2VDJZseBYXf/aeBbwJ82qp6gGRgYIB6Pk0gkNKKMiMgyGnkkuBs44+7vuHsKeAa4tXgDdz/q7vlLGV8FtjSwnsDp7++nr6+PmZkZLl68qHsIRURKNDIENwPvFS2fza1bzt3At8u9YWb3mNkxMzv2wQcfrGKJ619PTw8DAwOkUinGxsZYWFhodkkiIi2jJS6MMbM7gWHg0XLvu/vj7j7s7sObNm1a2+LWgVgsxhVXXMH8/DxjY2O6fUJEJKeRIXgOuLpoeUtu3SJmdhPwFWCvu2vwywaJRqMMDg6SyWQYHR0lnU43uyQRkaZrZAi+DlxjZtvNrBPYBxwu3sDMPgl8g2wAXmhgLQJ0dnayceNGzIzR0VFmZ2ebXZKISFM1LATdfR64DzgC/AD4prufMrOHzGxvbrNHgR7gOTM7YWaHl9mdrJJIJMLGjRuJRCJcvHiR8fFxnScUkcCydrt0fnh42I8dO9bsMtqeu5NIJJiamsLM6O3tJRaLYWbNLk1EpGZm9oa7D9f6uZa4MEbWnpnR09PDpk2b6OjoYGJigrGxMZ0rFJFAUQgGXCQSYXBwkA0bNjA/P8/o6CiTk5O6uV5EAiHS7AKkNXR3dxONRpmcnGR6epqZmRn6+vro6upSF6mIrFsKQSkIhUIMDAwQi8WYmJhgfHyccDhMT0+PzheKyLqkEJQlOjs72bRpE7Ozs4VBuKempojH48TjcUIh9aKLyPqgEJRldXV10dXVRSqVYnp6mqmpKaanp4nFYsTjcSIR/fiISHvTXzGpqLOzszDs2vT0NMlkkkQiQTQapbu7m66uLh0dikhbUghK1SKRCAMDA/T29pJMJkkmk1y6dAkzo6urq3Bxjc4diki7UAhKzcLhML29vfT29pJKpZiZmSlMoVCoEIidnZ0KRBFpaQpBuSydnZ10dnbS19e3KBCTySShUIhoNFqYwuFws8sVEVlEISirwswKYdff38/s7Cxzc3PMzc0xMzMDZLtTo9EoXV1dOkoUkZagEJRVZ2Z0d3fT3d0NwPz8fCEU8xfVmBkdHR2FI8mOjg4dKYrImtMlfdJwkUiEnp4eBgcHufLKKxkcHCQejwOQSCS4ePEi77//PhcuXGB8fJxEIkE6ndbQbSItKBwOs2vXrsL07rvvMjY2xo033khPTw/33Xffqn3XI488wo4dO9i5cydHjhwpu83+/fvZuXMnwLVm9qSZddTyHToSlDVV3G0K2adZpNNpUqlU4TXffWpmRCIROjo6Cq8dHR26HUOkibq7uzlx4sSidYlEgocffpiTJ09y8uTJVfme06dP88wzz3Dq1CnOnz/PTTfdxFtvvbWkx2j//v08/fTThEKhU0A38AXgr6r9HoWgNJWZFbpE8xYWFgqhOD8/X+hGzQuHw4VgLJ4UjiLNEY/HueGGGzhz5syq7fPFF19k3759RKNRtm/fzo4dO3jttde4/vrrF203MjJSvPgasKWW71EISssJh8OLzilCNhjn5+dJp9OLwrG4yzQUCi0JxnA4TDgcVkCKrJKZmRl27doFwPbt23n++eer/uz999/P0aNHl6zft28fDzzwwKJ1586dY8+ePYXlLVu2cO7cuZV2b8CvAr9XdUEoBKVN5MMs340K2a7UfDgWT7Ozs2QymUWfzwdkOBxeFI75SVeqilSnXHdotR577LFVrmaRjwJ/7+7/VcuHFILStvLnDMuNYZrJZAoBWfyaTqcL5xyLhUKhJcGYX1f8KiL1q+VIcPPmzbz33nuF5bNnz7J58+ay+33wwQchm2cHaq1JISjrUigUIhQK0dGx9EIxdy+EZLkplUotOZKEbOjmw7A4GEvn85OILFbLkeDevXu54447OHDgAOfPn+ftt99m9+7dS7Y7dOhQ/srRd9x96S9uBQpBCZx8mK10X2K+q7U4LPPzmUyG+fn5ZcMy/x2loVg6lW5jZuqWlba1bds2JicnSaVSvPDCC7z00ksMDQ3Vvb9rr72W2267jaGhISKRCAcPHiz8zo6MjHDo0CGuuuoq7r33XrZu3QrwcTM7AfyTuz9U7fdYu92LNTw87MeOHWt2GSLAh0eV+SkfkitNKykNyOKgLF1X7lUhKkFlZm+4+3Ctn9ORoMhlqOaospi7LwnO0uXSdel0urCumnpKQ3G5+ZXWFa8XWc8UgiJrqDhgalUcoCu9ls4vLCwsWV9PzZVCs3gq97lqthNZawpBkTZxOQFaqjQoy4VnpWm5z6/Gv7FSUFa7XO988ausbwpBkQDKd+M2wnKhudJ7y71fui4fsstts9qqDcpyr7VsW8trvdtIeQpBEVlVzezaXC4Yy4Vmpe2rfb/4tTSkV9p2rdUamPW8v9bz5ZZrpRAUkXWjXc4tVgrIcoF5Oduu5rqV3q9mX61GISgissaC2lVZTXBWu91qBatCUERE1sRK3ZrNorGdREQksBSCIiISWApBEREJLIWgiIgElkJQREQCSyEoIiKBpRAUEZHAUgiKiEhgKQRFRCSwFIIiIhJYCkEREQkshaCIiASWQlBERAJLISgiIoGlEBQRkcBSCIqISGApBEVEJLAUgiIiElgKQRERCSyFoIiIBJZCUEREAkshKCIigaUQFBGRwFIIiohIYCkERUQksBSCIiISWApBEREJLIWgiIgElkJQREQCSyEoIiKBpRAUEZHAUgiKiEhgKQRFRCSwFIIiIhJYCkEREQmshoagmd1iZj80szNm9kCZ96Nm9mzu/e+Z2bZG1iMiIlKsYSFoZmHgIPBZYAi43cyGSja7Gxh39x3AY8CfNKoeERGRUo08EtwNnHH3d9w9BTwD3Fqyza3A3+bmvwV8xsysgTWJiIgURBq4783Ae0XLZ4GfXW4bd583swlgEBgt3sjM7gHuyS3OmdnJhlS8vm2kpF2lKmq3+qjd6qe2q8/Oej7UyBBcNe7+OPA4gJkdc/fhJpfUdtRu9VG71UftVj+1XX3M7Fg9n2tkd+g54Oqi5S25dWW3MbMI0A+MNbAmERGRgkaG4OvANWa23cw6gX3A4ZJtDgOfz83/MvBdd/cG1iQiIlLQsO7Q3Dm++4AjQBh40t1PmdlDwDF3Pww8ATxlZmeAi2SDspLHG1XzOqd2q4/arT5qt/qp7epTV7uZDrxERCSoNGKMiIgElkJQREQCq2VDUEOu1aeKdjtgZqfN7E0z+46ZbW1Gna2mUrsVbfdLZuZmpkvYqa7dzOy23M/cKTP7h7WusRVV8Xv6UTM7ambHc7+rI82os9WY2ZNmdmG5e8Ut6y9y7fqmmX2q4k7dveUmshfS/C/wU0An8H1gqGSb3wK+npvfBzzb7LqbPVXZbjcCsdz8F9Vu1bVbbrte4GXgVWC42XU3e6ry5+0a4DiwIbf8kWbX3eypynZ7HPhibn4IeLfZdbfCBHwa+BRwcpn3R4BvAwbsAb5XaZ+teiSoIdfqU7Hd3P2ouydzi6+SvX8z6Kr5eQN4mOz4trNrWVwLq6bdfgM46O7jAO5+YY1rbEXVtJsDfbn5fuD8GtbXstz9ZbJ3EiznVuDvPOtVYMDMfnKlfbZqCJYbcm3zctu4+zyQH3ItyKppt2J3k/1fU9BVbLdct8rV7v4va1lYi6vm5+1jwMfM7H/M7FUzu2XNqmtd1bTbV4E7zews8K/A76xNaW2v1r+B7TFsmqw+M7sTGAZ+vtm1tDozCwF/DtzV5FLaUYRsl+gvkO11eNnMPuHul5paVeu7Hfgbd/8zM7ue7P3U17l7ptmFrTeteiSoIdfqU027YWY3AV8B9rr73BrV1soqtVsvcB3wH2b2LtlzDYd1cUxVP29ngcPunnb3HwFvkQ3FIKum3e4Gvgng7q8AXWQH1paVVfU3sFirhqCGXKtPxXYzs08C3yAbgDo/k7Viu7n7hLtvdPdt7r6N7LnUve5e14C960g1v6cvkD0KxMw2ku0efWcti2xB1bTbj4HPAJjZx8mG4AdrWmV7Ogx8LneV6B5gwt3/b6UPtGR3qDduyLV1rcp2exToAZ7LXUf0Y3ff27SiW0CV7SYlqmy3I8DNZnYaWAC+7O6B7rGpst2+BPy1md1P9iKZu/SffDCzfyT7n6qNufOlfwR0ALj718mePx0BzgBJ4Ncq7lPtKiIiQdWq3aEiIiINpxAUEZHAUgiKiEhgKQRFRCSwFIIiIhJYCkGRJjOzBTM7YWYnzeyfzWxglfd/l5l9LTf/VTP7/dXcv0g7UwiKNN+Mu+9y9+vI3vP6280uSCQoFIIireUVigb8NbMvm9nruWejPVi0/nO5dd83s6dy634x92zN42b272b2E02oX6SttOSIMSJBZGZhskNlPZFbvpnsOJu7yT4f7bCZfZrsGLl/CPycu4+a2RW5Xfw3sMfd3cy+APwB2ZFHRGQZCkGR5us2sxNkjwB/APxbbv3Nuel4brmHbCj+DPCcu48CuHv++WpbgGdzz0/rBH60NuWLtC91h4o034y77wK2kj3iy58TNOCR3PnCXe6+w92fWGE/fwl8zd0/Afwm2UGXRWQFCkGRFuHuSeB3gS/lHg92BPh1M+sBMLPNZvYR4LvAr5jZYG59vju0nw8fG/N5RKQidYeKtBB3P25mbwK3u/tTucfovJJ74sc0cGfuiQN/DPynmS2Q7S69i+zTyJ8zs3GyQbm9Gf8GkXaip0iIiEhgqTtUREQCSyEoIiKBpRAUEZHAUgiKiEhgKQRFRCSwFIIiIhJYCkEREQms/weXEwhHNCfG1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test_df['is_positive'], model.predict_proba(train_df['review'])[:, 1])\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1 = {0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tL6Bnl8ftoYP"
   },
   "source": [
    "**Task** Come up with signs to improve the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRhcCJcAS4S5"
   },
   "source": [
    "## Character-Level Convolutions\n",
    "\n",
    "### General description of convolutions\n",
    "\n",
    "Let me remind you that convolutions are what started the HYIP of neural networks in the area of the 2012th.\n",
    "\n",
    "They work like this:\n",
    "<img src=\"https://image.ibb.co/e6t8ZK/Convolution.gif\" width=\"50%\">\n",
    "\n",
    "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "Formally - filter sets are learned, each of which is scalar multiplied by elements of the matrix of attributes. In the picture above, the original matrix collapses with the filter.\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 & 1 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  1 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "But we must not forget that convolutions usually have another dimension, such as the number of channels. For example, pictures usually have three channels: RGB.\n",
    "It clearly demonstrates how filters look like [here] (http://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "After convolutions, pooling layers usually follow. They help reduce the dimension of the tensor with which to work. The most common is max-pooling:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-M3lCE1ealB"
   },
   "source": [
    "### Text wrappers\n",
    "\n",
    "For convolution texts, they work as n-gram detectors (approximately). A canonical example of a symbolic convolution network:\n",
    "\n",
    "<img src=\"https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png\" width=\"50%\">\n",
    "\n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "* How many filters does the example use? *\n",
    "\n",
    "The picture shows how 2, 3, and 4 grams are extracted from a word. For example, yellow is trigrams. A yellow filter is applied to all trigrams in a word, and then the strongest signal is extracted using global max-pooling.\n",
    "\n",
    "What does this mean, if specifically?\n",
    "\n",
    "Each character is displayed using embeddings in some vector. And their sequences are in the concatenation of embeddings.\n",
    "\n",
    "For example, \"abs\" $ \\ to [v_a; v_b; v_s] \\in \\mathbb {R} ^ {3 d} $, where $ d $ is the embedding dimension. The yellow filter $ f_k $ has the same dimension $ 3d $.\n",
    "\n",
    "Its attachment is the scalar product $ \\left ([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R $ (one of the yellow squares in the feature map for this filter).\n",
    "\n",
    "Max-pooling selects $ max_i \\left ([v_ {i-1}; v_ {i}; v_ {i + 1}] \\odot f_k \\right) $, where $ i $ runs over all indexes of the word from 1 to $ | w | - $ 1 (or over a larger range, if there are paddings).\n",
    "This maximum corresponds to the trigram that is closest to the filter over the cosine distance.\n",
    "\n",
    "As a result, after max-pooling, the vector encodes information about which of the n-grams met in the word: if a trigram close to our $ f_k $ met, then the vector will have a great value in the $ k $ position of the vector, otherwise small\n",
    "\n",
    "And we just learn the filters. That is, the network must learn to determine which of the n-grams are significant and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz7ApwLZ05kc"
   },
   "source": [
    "### Toy example\n",
    "\n",
    "Let's look at an example of what is happening there. Take the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7fbpo630iZa"
   },
   "outputs": [],
   "source": [
    "word = 'Hello Word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x99Q65ot02Vi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': 0, 'd': 1, 'H': 2, ' ': 3, 'e': 4, 'o': 5, 'l': 6, 'r': 7}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index = {symb: ind for ind, symb in enumerate(set(word))}\n",
    "\n",
    "char2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sxJYZEs1M8y"
   },
   "source": [
    "Each character is associated with embedding. The easiest way to do embeddingings is to take a single matrix. When we had tens of thousands of words, such embeddings were not very good, but now only a few characters are quite adequate to assign them orthogonal vectors of small dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbApeJdb1EGq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.eye(len(char2index))\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP9i5pPx2BGs"
   },
   "source": [
    "Construct the tensor of the indexes of the characters of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBIIlHy91ohP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 6, 5, 3, 0, 5, 7, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tensor = torch.LongTensor([char2index[symb] for symb in word])\n",
    "\n",
    "word_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL3qLw-p2Htk"
   },
   "source": [
    "Let's display it in embeddings. Got the same rectangle as in the picture (transposition is necessary to look in the same direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fkChxtt1yqY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs = embeddings[word_tensor].t()\n",
    "\n",
    "word_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL4o1EI-2O_I"
   },
   "source": [
    "Now it came to convolutions. Let's make the filter-detector of the trigram `new`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToW8KCUY1723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_name = 'Hel'\n",
    "\n",
    "kernel_indices = torch.LongTensor([char2index[symb] for symb in kernel_name])\n",
    "kernel_weights = embeddings[kernel_indices].t()\n",
    "\n",
    "kernel_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTxfchOy4c9w"
   },
   "source": [
    "To calculate the convolution, use the function:\n",
    "```python\n",
    "F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "```\n",
    "\n",
    "input: input tensor of shape ($N \\times C_{in} \\times H_{in} \\times W_{in}$)  \n",
    "weight: filters of shape ($C_{out} \\times C_{in} \\times H_{out} \\times W_{out}$)\n",
    "\n",
    "$ N $ - the size of the batch (1 for us). $ C_ {in} $ - the number of channels. In our case, it will always be 1 (for now). $ C_ {out} $ - the number of filters. It is still 1.\n",
    "\n",
    "We will need four-dimensional tensors, for this we use `view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyd3Na7V3JMr"
   },
   "outputs": [],
   "source": [
    "word_embs = word_embs.view(1, 1, word_embs.shape[0], word_embs.shape[1])\n",
    "kernel_weights = kernel_weights.view(1, 1, kernel_weights.shape[0], kernel_weights.shape[1])\n",
    "\n",
    "conv_result = F.conv2d(word_embs, kernel_weights)[0, 0]\n",
    "\n",
    "print('Conv =', conv_result)\n",
    "print('Max pooling =', conv_result.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVAiQ2PG6SOd"
   },
   "source": [
    " ,       .  ,     - ,  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7ZVbNOGoV5z"
   },
   "source": [
    "###  \n",
    "\n",
    "  - ,     .  - ,     ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEtZ6g78Wtj-"
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "    \n",
    "def find_max_len(counter, threshold):\n",
    "    sum_count = sum(counter.values())\n",
    "    cum_count = 0\n",
    "    for i in range(max(counter)):\n",
    "        cum_count += counter[i]\n",
    "        if cum_count > sum_count * threshold:\n",
    "            return i\n",
    "    return max(counter)\n",
    "\n",
    "word_len_counter = Counter()\n",
    "for word in train_data:\n",
    "    word_len_counter[len(word)] += 1\n",
    "    \n",
    "threshold = 0.99\n",
    "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
    "\n",
    "print('Max word length for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdKoOBdE8uj4"
   },
   "source": [
    "     ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyMoPEXGVs3s"
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for word in train_data:\n",
    "    chars.update(word)\n",
    "\n",
    "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
    "char_index['<pad>'] = 0\n",
    "    \n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4spzGF2CImtL"
   },
   "source": [
    "****  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qETmYKm8W_TX"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, max_word_len, char_index):\n",
    "    return <np array>\n",
    "\n",
    "X_train = convert_data(train_data, MAX_WORD_LEN, char_index)\n",
    "X_test = convert_data(test_data, MAX_WORD_LEN, char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VufrP006Vk-y"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        \n",
    "        batch_idx = indices[start: end]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkXlKB7lobYE"
   },
   "source": [
    "### ConvCharNN\n",
    "\n",
    "Now we build a convolutional model.\n",
    "\n",
    "Let it build trigrams - that is, apply filters for 3 characters.\n",
    "\n",
    "Let's start with the sequence: `nn.Embedding -> nn.Conv2d -> nn.ReLU -> max pooling -> nn.Linear`\n",
    "\n",
    "`nn.Conv2d` is the layer containing the creation and initialization of filters, and calling` F.conv2d` to them and the input.\n",
    "\n",
    "* Life hacking: * sequences of operations can be packed in `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2T4AorsZ530"
   },
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embedding = ...\n",
    "        self._dropout = nn.Dropout(0.2)\n",
    "        self._conv3 = ...\n",
    "        self._out_layer = ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs - LongTensor with shape (batch_size, max_word_len)\n",
    "        outputs - FloatTensor with shape (batch_size,)\n",
    "        '''\n",
    "        \n",
    "        outputs = self.embed(inputs)\n",
    "        return self._out_layer(outputs).squeeze(-1)\n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        <calc word embedding>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iTDc9rEAHN6"
   },
   "source": [
    ",   :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiAdrSaU_wjQ"
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iterate_batches(X_train, train_labels, 32))\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "model = ConvClassifier(len(char_index) + 1, 24, 64)\n",
    "logits = model(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x39p9w-tA_Ds"
   },
   "source": [
    "****  precision, recall  F1-score   ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVg2Ws7mA40a"
   },
   "outputs": [],
   "source": [
    "<calc precision, recall, f1-score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TFsKlF9BV8X"
   },
   "source": [
    "****    ,      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsCtTJucVjMH"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, epoch_tp, epoch_fp, epoch_fn = 0, 0, 0, 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), FloatTensor(y_batch)\n",
    "\n",
    "            logits = <calc logits>\n",
    "\n",
    "            loss = <calc loss>\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                <how to optimize the beast?>\n",
    "\n",
    "            <u can move the stuff to some function>\n",
    "            tp = <calc true positives>\n",
    "            fp = <calc false positives>\n",
    "            fn = <calc false negatives>\n",
    "\n",
    "            precision = ...\n",
    "            recall = ...\n",
    "            f1 = ...\n",
    "            \n",
    "            epoch_tp += tp\n",
    "            epoch_fp += fp\n",
    "            epoch_fn += fn\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
    "                  i, batchs_count, loss.item(), precision, recall, f1), end='')\n",
    "        \n",
    "    precision = ...\n",
    "    recall = ...\n",
    "    f1 = ...\n",
    "        \n",
    "    return epoch_loss / batchs_count, recall, precision, f1\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_recall, train_precision, train_f1 = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_recall, val_precision, val_f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss, train_recall, train_precision, train_f1,\n",
    "                                     val_loss, val_recall, val_precision, val_f1))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlq63hAXh0Gr"
   },
   "outputs": [],
   "source": [
    "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=200, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o-1AEcFCjAk"
   },
   "source": [
    "****      .\n",
    "\n",
    "        -         -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svO9OrF4CiLI"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "surname = \"...\"\n",
    "surname_tensor = ...\n",
    "print('P({} is surname) = {}'.format(surname, torch.sigmoid(model(surname_tensor))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djkksZKcDPA1"
   },
   "source": [
    "****  precision-recall curve      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZm7O56pDcH_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FPTfBzSDheP"
   },
   "source": [
    "## \n",
    "\n",
    "###  \n",
    "\n",
    "****   ,    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeW5av_ASrn4"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token, colors):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67KNX5lBTdrt"
   },
   "outputs": [],
   "source": [
    "word_indices = np.random.choice(np.arange(len(test_data)), 1000, replace=False)\n",
    "words = [test_data[ind] for ind in word_indices]\n",
    "labels = test_labels[word_indices]\n",
    "\n",
    "word_tensor = convert_data(words, max(len(x) for x in words), char_index)\n",
    "embeddings = <calc embeddings>\n",
    "\n",
    "colors = ['red' if label else 'blue' for label in labels]\n",
    "\n",
    "visualize_embeddings(embeddings, words, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOt1tcOkvDBY"
   },
   "source": [
    "### Visualization of the received bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tljoUWceE2lH"
   },
   "source": [
    "Among other things, we have a logistic regression from above. You can visualize it as in eli5.\n",
    "\n",
    "** Assignment ** Achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJpHT_YkLvCf"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "word = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNMXpgh8FNIo"
   },
   "source": [
    " ,   - ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hl-VZs1mFKxM"
   },
   "outputs": [],
   "source": [
    "inputs = word -> LongTensor\n",
    "prob = torch.sigmoid(model(inputs)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2SZi1FIFj6o"
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1SdKbDFFhOk"
   },
   "outputs": [],
   "source": [
    "convs = ...\n",
    "maxs, positions = convs.squeeze().max(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLTu8mlfFtXJ"
   },
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRMkbFXdFwys"
   },
   "outputs": [],
   "source": [
    "linear_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qV3dK-IzGFXh"
   },
   "source": [
    "  :     -  -      ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0y8epEgGEU8"
   },
   "outputs": [],
   "source": [
    "symb_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utVltLCUGXTy"
   },
   "source": [
    " :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2UKjz86F7fz"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def get_color_hex(weight):\n",
    "    cmap = plt.get_cmap(\"RdYlGn\")\n",
    "    rgba = cmap(weight, bytes=True)\n",
    "    return '#%02X%02X%02X' % rgba[:3]\n",
    "\n",
    "symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "for symb, weight in zip(word, symb_weights):\n",
    "    res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "res = '<p>' + res + '</p>'\n",
    "\n",
    "HTML(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGhfZsleGZi0"
   },
   "source": [
    "   :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUi51X4fQIIT"
   },
   "outputs": [],
   "source": [
    "def calc_weights(word):\n",
    "    <calc>\n",
    "    \n",
    "    return prob, symb_weights\n",
    "\n",
    "def visualize(word):\n",
    "    prob, symb_weights = calc_weights(word)\n",
    "    \n",
    "    symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "    res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "    for symb, weight in zip(word, symb_weights):\n",
    "        res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "    res = '<p>' + res + '</p>'\n",
    "    return HTML(res)\n",
    "\n",
    "\n",
    "visualize('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXee135DEJuy"
   },
   "source": [
    "## Model improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2_8bNRyG3CF"
   },
   "source": [
    "**Task** To improve the stability of the model, it is worth adding the dropout `nn.Dropout` - a way to nullify a part of the scales at each epoch to regularize the model. Try adding it after embeddings and after convolution (or else somewhere else).\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=\"50%\">\n",
    "\n",
    "**Task** Another way to regularize a model is to use BatchNormalization (`nn.BatchNorm2d`). Try adding it after convolution.\n",
    "\n",
    "**Task** Another way to improve the model is to add a bundle. Implement the model as in the picture at the beginning of the laptop: with convolutions of 2, 3, 4 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qstoDysVsSQ2"
   },
   "source": [
    "**Assignment** Distinguish between Narrow and Wide convolutions - in fact, whether zero padding is added or not. For texts, this difference looks like this:<img src=\"https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png\" width=\"50%\">\n",
    "\n",
    "*From Neural Network Methods in Natural Language Processing.*\n",
    "\n",
    "On the left, there is no padding, on the right, there is. Try adding padding and see what happens. Potentially, it will help to learn good word prefixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v39_tgnMnr0J"
   },
   "source": [
    "# Referrence\n",
    "[Convolutional Neural Networks, cs231n](http://cs231n.github.io/convolutional-networks/)  \n",
    "[Understanding Convolutions, Christopher Olah](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)  \n",
    "[Understanding Convolutional Neural Networks for NLP, Denny Britz](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "[Character-Aware Neural Language Models, Yoon Kim et al, 2015](https://arxiv.org/abs/1508.06615)  \n",
    "[Character-level Convolutional Networks for Text Classification, Zhang et al., 2015](https://arxiv.org/abs/1509.01626)  \n",
    "[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification Zhang et al., 2015](https://arxiv.org/abs/1510.03820)\n",
    "[Learning Character-level Representations for Part-of-Speech Tagging, dos Santos et al, 2014](http://proceedings.mlr.press/v32/santos14.pdf)\n",
    "\n",
    "[cs224n \"Lecture 13: Convolutional Neural Networks\"](https://www.youtube.com/watch?v=Lg6MZw_OOLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 04 - Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
