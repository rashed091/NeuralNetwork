{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfeVtEQwRmsR"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip install -qq bokeh==0.13.0\n",
    "!wget -O surnames.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKoTq9xW-PdW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Q5wMjxQMeQS"
   },
   "source": [
    "# Recurrent neural networks, part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPPJoWEpSN_B"
   },
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "with open('surnames.txt') as f:\n",
    "    for line in f:\n",
    "        surname, lang = line.strip().split('\\t')\n",
    "        data.append(surname)\n",
    "        labels.append(lang)\n",
    "\n",
    "for i in np.random.randint(0, len(data), 10):\n",
    "    print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bVJlxzYhWuf"
   },
   "source": [
    "Test your knowledge - try to independently predict which language the surname belongs to :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7JquuckaAGb"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def test_generator():\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight('balanced', classes, labels)\n",
    "    classes = {label: ind for ind, label in enumerate(classes)}\n",
    "\n",
    "    probs = np.array([weights[classes[label]] for label in labels])\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    ind = np.random.choice(np.arange(len(data)), p=probs)\n",
    "    yield data[ind]\n",
    "    \n",
    "    while True:\n",
    "        new_ind = np.random.choice(np.arange(len(data)), p=probs)\n",
    "        yield labels[ind], data[new_ind]\n",
    "        ind = new_ind\n",
    "        \n",
    "gen = test_generator()\n",
    "question = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KNeNxm1hsKs"
   },
   "source": [
    "Start, look at the name that appears - and select the language in the drop-down list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "-Q3OSXpAY8BS"
   },
   "outputs": [],
   "source": [
    "#@title Проверим себя (или адекватность данных) { run: \"auto\" }\n",
    "answer = \"Vietnamese\" #@param [\"Arabic\", \"Chinese\", \"Czech\", \"Dutch\", \"English\", \"French\", \"German\", \"Greek\", \"Irish\", \"Italian\", \"Japanese\", \"Korean\", \"Polish\", \"Portuguese\", \"Russian\", \"Scottish\", \"Spanish\", \"Vietnamese\"]\n",
    "\n",
    "correct_answer, question = next(gen)\n",
    "\n",
    "if 'correct_count' not in globals():\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "else:\n",
    "    if answer == correct_answer:\n",
    "        print('You are correct', end=' ')\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"No, it's\", correct_answer, end=' ')\n",
    "\n",
    "    total_count += 1\n",
    "    print('({} / {})'.format(correct_count, total_count))\n",
    "    \n",
    "print('Next surname:', question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Yz20u6dhll0"
   },
   "source": [
    "### Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfP4Sb68TWlY"
   },
   "source": [
    "First you need to build a split data on the train / test. The difficulty is that the classes are distributed unevenly, and you need to cut off from each class a proportional amount of data per test. To do this, use the `stratify` function parameter` train_test_split` (or `StratifiedShuffleSplit`, or, if you so desire,` GroupShuffleSplit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eE-s7q7RmAM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data, labels, test_size=0.3, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzX5zHobUHc0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "langs = set(labels)\n",
    "\n",
    "train_distribution = Counter(labels_train)\n",
    "train_distribution = [train_distribution[lang] for lang in langs]\n",
    "\n",
    "test_distribution = Counter(labels_test)\n",
    "test_distribution = [test_distribution[lang] for lang in langs]\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(langs)), train_distribution, bar_width, align='center', alpha=0.5, label='train')\n",
    "plt.bar(np.arange(len(langs)) + bar_width, test_distribution, bar_width, align='center', alpha=0.5, label='test')\n",
    "plt.xticks(np.arange(len(langs)) + bar_width / 2, langs)\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDgB4iVCWbNE"
   },
   "source": [
    "You should always start with baseline - let's use our favorite pair of vectorizer-logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLxUE4thXyV2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer='char', ngram_range=(1, 4))),\n",
    "    ('log_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wz9ngs_lWn3p"
   },
   "source": [
    "\n",
    "What metrics will we count? There is a multi-class classification, so everything is very ambiguous.\n",
    "\n",
    "It makes sense to look at accuracy and F1-scores for each class .:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJZt8sKM6zEA"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "preds = model.predict(data_test)\n",
    "\n",
    "print('Accuracy = {:.2%}'.format(accuracy_score(labels_test, preds)))\n",
    "print('Classification report:')\n",
    "print(classification_report(labels_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNRhUTNaW45M"
   },
   "source": [
    "F1-scores can be aggregated in different ways:\n",
    "\n",
    "- weighted is as considered by classification_report - if it is more important for us to predict well more frequency surnames\n",
    "- macro - simple averaging - if it is important to predict everything, regardless of how much each class is in the test sample\n",
    "- micro - normal F1-score calculation for all true positive, false positive and negative negative\n",
    "\n",
    "Weighted and micro - two metrics that take into account class imbalances. But in our case it is not obvious, is there an imbalance, yes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0SwCbWN8ZQd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "label_names = list(set(labels_test))\n",
    "confusion = confusion_matrix(labels_test, preds, labels=label_names).astype(np.float)\n",
    "confusion /= confusion.sum(axis=-1, keepdims=True)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion, cmap='Reds')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + label_names, rotation=45)\n",
    "ax.set_yticklabels([''] + label_names)\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo0lsnzV-i-P"
   },
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hb4_VaBIMVFD"
   },
   "source": [
    "\n",
    "The main charm of RNN is the shared parameters. Look at the picture:\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\">\n",
    "\n",
    "*From [(The Unreasonable Effectiveness of Recurrent Neural Networks)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "The first example is the usual full mesh network. Each following demonstrates the processing of a certain sequence of arbitrary length (red rectangles) and the generation of the output sequence, also of arbitrary length (blue rectangles).\n",
    "\n",
    "In this case, the green rectangles in each figure are the same weights. So, on the one hand, we are training a very, very deep network (if you look at it upside down), and on the other, a strictly limited number of parameters.\n",
    "\n",
    "---\n",
    "Write a simple RNN right away!\n",
    "\n",
    "Let me remind you, she does something like this:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\">\n",
    "\n",
    "\n",
    "Generally speaking, you can come up with many variations on this implementation. In our case, the processing will be as follows:\n",
    "\n",
    "$$ h_t = tanh (W_h [h_ {t-1}; x_t] + b_h) $$\n",
    "\n",
    "$ h_ {t-1} $ is the hidden state obtained in the previous step, $ x_t $ is the input vector. $ [h_ {t-1}; x_t] $ is a simple concatenation of vectors. Just like in the picture!\n",
    "\n",
    "Let's check our network on a very simple task: make it say the index of the first element in the sequence.\n",
    "\n",
    "Those. for the sequence `[1, 2, 1, 3]` the network must predict `1`.\n",
    "\n",
    "Let's start with the generation of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOI4JGgHT-z3"
   },
   "outputs": [],
   "source": [
    "def generate_data(batch_size=128, seq_len=5):\n",
    "    data = torch.randint(0, 10, size=(seq_len, batch_size), dtype=torch.long)\n",
    "    return data, data[0]\n",
    "\n",
    "X_val, y_val = generate_data()\n",
    "X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQ0Gsr4SFNtB"
   },
   "source": [
    "Please note that the batch has the dimension `(sequence_length, batch_size, input_size)`. All `RNN` in pytorch work with this default format.\n",
    "\n",
    "This is done for performance reasons, but you can change this behavior with the help of the `batch_first` argument if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqS7HPRhZSBC"
   },
   "source": [
    "**Task** Implement the `SimpleRNN` class, which performs the calculation using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed1b2TUvZRs0"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        <create Linear layer>\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        seq_len, batch_size = inputs.shape[:2]\n",
    "        if hidden is None:\n",
    "            hidden = inputs.new_zeros((batch_size, self._hidden_size))\n",
    "         \n",
    "        for i in range(seq_len):\n",
    "            <apply linear layer to concatenation of current input (inputs[i]) and hidden>\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS2xw2YIZ_EU"
   },
   "source": [
    "\n",
    "It should be clear why it is useful to have the first dimension seq_len - you need to be able to take `inputs [i]` - the subbatch related to this timestamp. If the data were located differently, this operation would be much more expensive.\n",
    "\n",
    "** Task ** Implement the `MemorizerModel` class, with the sequence` Embedding -> SimpleRNN -> Linear `. You can use `nn.Sequential`\n",
    "\n",
    "To make embeddings, you can use `nn.Embedding.from_pretrained`. For simplicity, we will do a one-hot-encoding representation — to do this, we simply need to initialize the network with the unit matrix `torch.eye (N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEUr4Xa9Z81I"
   },
   "outputs": [],
   "source": [
    "# u can use nn.Sequential too\n",
    "class MemorizerModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        <create layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply 'em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiDRoQWDawaW"
   },
   "source": [
    "Run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbVk7zUjUQ_v"
   },
   "outputs": [],
   "source": [
    "rnn = MemorizerModel(hidden_size=32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "\n",
    "total_loss = 0\n",
    "epochs_count = 1000\n",
    "for epoch_ind in range(epochs_count):\n",
    "    X_train, y_train = generate_data(seq_len=25)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    rnn.train()\n",
    "    \n",
    "    logits = rnn(X_train)\n",
    "\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if (epoch_ind + 1) % 100 == 0:\n",
    "        rnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = rnn(X_val)\n",
    "            val_loss = criterion(logits, y_val)\n",
    "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
    "                                                             total_loss / 100, val_loss.item()))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQJg3FROIq4v"
   },
   "source": [
    "**Task** Look at how sequence length affects network performance.\n",
    "\n",
    "First, look at how long the network is able to learn. Secondly, try to train a network with a short sequence length, and then apply it to longer ones.\n",
    "\n",
    "**Assignment** It is stated that `relu` fits RNN better. Try it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSHNuT5b61Ky"
   },
   "source": [
    "## Training RNN\n",
    "\n",
    "\n",
    "<img src=\"https://image.ibb.co/cEYkw9/rnn_bptt_with_gradients.png\">\n",
    "\n",
    "*From [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)*\n",
    "\n",
    "\n",
    "If everything went according to plan, we had to look at how RNN's were forgotten.\n",
    "\n",
    "To understand the reason, it is worth remembering exactly how the RNN learning takes place, for example, here: [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) или здесь - [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/).\n",
    "\n",
    "In short, one of the problems of learning recurrent networks is * explosion of gradients *. It manifests itself when the matrix of weights is such that it increases the norm of the gradient vector during the reverse pass. As a result, the rate of the gradient grows exponentially and it \"explodes.\"\n",
    "\n",
    "This problem can be solved using clipping gradients: `nn.utils.clip_grad_norm_ (rnn.parameters (), 1.)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13x5erUgTjDC"
   },
   "source": [
    "## LSTM vs GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAjZh9YkYAMH"
   },
   "source": [
    "Another problem is * attenuation of gradients *. It is connected the opposite - with the exponential decay of gradients. And now it is solved in more complicated ways.\n",
    "\n",
    "Namely - use gate'ovye architecture.\n",
    "\n",
    "The idea of gate is simple, but important, they are used not only in recurrent networks.\n",
    "\n",
    "If you look at how our SimpleRNN works, you will notice that each time the memory (ie, $ h_t $) is overwritten. I want to be able to make this rewrite controlled: do not discard any important information from the vector.\n",
    "\n",
    "Let's get for this the vector $g \\in \\{0,1 \\}^n $, which will say which $h_{t-1}$ cells are good, and instead of which ones it is worth substituting new values:\n",
    "\n",
    "$$ h_t = g \\odot f (x_t, h_{t-1}) + (1 - g) \\odot h_{t-1}. $$\n",
    "\n",
    "For example:\n",
    "$$\n",
    " \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  11 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  0\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  7 \\\\\n",
    "  11 \\\\\n",
    "  6 \\\\\n",
    "  5\n",
    " \\end{bmatrix}\n",
    " +\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  1\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  5 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To achieve differentiability, we use sigmoid: $ \\sigma(f (x_t, h_ {t-1})) $.\n",
    "\n",
    "As a result, the network itself will, looking at the inputs, decide which cells of its memory and how much it costs to rewrite.\n",
    "\n",
    "### LSTM\n",
    "\n",
    "It seems that the first architecture that applied this mechanism was LSTM (Long Short-Term Memory).\n",
    "\n",
    "In it, we also add $ c_ {t-1} $ to $ h_ {t-1} $: $ h_ {t-1} $ is all the same hidden states obtained in the previous step, and $ c_ {t -1} $ is a memory vector.\n",
    "\n",
    "Schematically - something like this:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n",
    "\n",
    "*From [(Understanding LSTM Networks)](http://colah.github.io/posts/2015-08-Understanding-LSTMs)*\n",
    "\n",
    "\n",
    "For a start, we can in the same way, as before, calculate a new hidden state (we denote it by $ \\tilde c_{t} $):\n",
    "\n",
    "$$ \\tilde c_{t} = tanh(W_h [h_ {t-1}; x_t] + b_h) $$\n",
    "\n",
    "In normal RNNs, we would simply overwrite the value of the latent state with this value. And now we want to understand how much information we need from $ c_ {t-1} $ and from $ \\tilde c_ {t} $.\n",
    "\n",
    "Rate it sigmoid:\n",
    "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f),$$\n",
    "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i).$$\n",
    "\n",
    "The first is about how much you want to forget the old information. The second is how interesting is the new one. Then\n",
    "\n",
    "$$ c_t = f \\odot c_ {t-1} + i \\odot \\tilde c_t. $$\n",
    "\n",
    "We will also weigh the new hidden state:\n",
    "\n",
    "$$ o = \\sigma (W_o [h_ {t-1}; x_t] + b_o), $$\n",
    "$$ h_t = o \\odot tanh (c_t). $$\n",
    "\n",
    "Another picture:\n",
    "\n",
    "<img src=\"https://image.ibb.co/e6HQUU/details.png\">\n",
    " \n",
    "*From [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/)*\n",
    "\n",
    "Why is the problem of damped gradients solved? Because look at the derivative $ \\frac {\\partial c_t} {\\partial c_ {t-1}} $. It is proportional to the $ f $ gate. If $ f = 1 $ - gradients flow unchanged. Otherwise - well, the network itself learns when it wants to forget something.\n",
    "\n",
    "It is highly recommended to read the article: [Understanding LSTM Networks] (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more information and fun pictures.\n",
    "\n",
    "Why did I write these formulas? The main thing is to show how much more parameters you need to learn in LSTM compared to a regular RNN. Four times more!\n",
    "\n",
    "For those who fell asleep - [video, as forgets RNN (bottom)](https://www.youtube.com/watch?v=mLxsbWAYIpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9KWgbwQMatn"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7-fQPwJUKtV"
   },
   "outputs": [],
   "source": [
    "symbols = set(symb for word in data_train for symb in word)\n",
    "char2ind = {symb: ind + 1 for ind, symb in enumerate(symbols)}\n",
    "char2ind[''] = 0\n",
    "\n",
    "lang2ind = {lang: ind for ind, lang in enumerate(set(labels_train))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcFgEy7YeFw0"
   },
   "source": [
    "Convert dataset.\n",
    "\n",
    "**Task** Write a batch generator that will select a random set of words on the fly and convert them into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWWClVsTRVuA"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, labels, char2ind, lang2ind, batch_size):\n",
    "    # let's do the conversion part first\n",
    "    labels = np.array([lang2ind[label] for label in labels])\n",
    "    data = [[char2ind.get(symb, 0) for symb in word] for word in data]\n",
    "    \n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, len(data), batch_size):\n",
    "        end = min(start + batch_size, len(data))\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        \n",
    "        max_word_len = max(len(data[ind]) for ind in batch_indices)\n",
    "        X = np.zeros((max_word_len, len(batch_indices)))\n",
    "        <fill X>\n",
    "            \n",
    "        yield X, labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfeR4B_hbH9P"
   },
   "source": [
    "Лень передавать `char2ind, lang2ind`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA-_jRNdaCM3"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "iterate_batches = partial(iterate_batches, char2ind=char2ind, lang2ind=lang2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD5i7WmTVlGk"
   },
   "outputs": [],
   "source": [
    "next(iterate_batches(data, labels, batch_size=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnkAUetgs6Tr"
   },
   "source": [
    "**Задание** Реализуйте простую модель на `SimpleRNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk3OSidVS_px"
   },
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        <set layers>\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        'embed(inputs) -> prediction'\n",
    "        <implement it>\n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        'inputs -> word embedding'\n",
    "        <and it> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vXN-QIrZs95"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(len(data) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size=batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(i, batchs_count, loss.item()), end='')\n",
    "                \n",
    "    return epoch_loss / batchs_count\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9vBDF2gbypR"
   },
   "outputs": [],
   "source": [
    "model = SurnamesClassifier(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=128, train_data=(data_train, labels_train),\n",
    "    val_data=(data_test, labels_test), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jC76XyGjigFx"
   },
   "source": [
    "**Задание** Напишите функцию для тестирования полученной сети: пусть она принимает слово и говорит, в каком языке с какой вероятностью это может быть фамилией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsGNbpBVJ3xO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWqOPgdbIYcl"
   },
   "source": [
    "**Задание** Оцените качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dT2QE6IycXo9"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_test, y_pred = [], []\n",
    "<calc 'em>\n",
    "\n",
    "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred, \n",
    "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_FZ9x0NInft"
   },
   "source": [
    "## Визуализация эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJkyBV2bAK05"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.colors import RGB\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    if isinstance(color, np.ndarray):\n",
    "        color = [RGB(*x[:3]) for x in color]\n",
    "    print(color)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token, colors):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u-74cv7IrH9"
   },
   "source": [
    "Мы опять получили эмбеддинги - символьного уровня теперь.\n",
    "\n",
    "Хочется на них посмотреть\n",
    "\n",
    "**Задание** Посчитайте векторы для случайных слов и выведите их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jt6LsI0NAPAm"
   },
   "outputs": [],
   "source": [
    "word_indices = np.random.choice(np.arange(len(data_test)), 1000, replace=False)\n",
    "words = [data_test[ind] for ind in word_indices]\n",
    "word_labels = [labels_test[ind] for ind in word_indices]\n",
    "\n",
    "model.eval()\n",
    "X_batch, y_batch = next(iterate_batches(words, word_labels, batch_size=1000))\n",
    "embeddings = <calc me>\n",
    "\n",
    "colors = plt.cm.tab20(y_batch) * 255\n",
    "\n",
    "visualize_embeddings(embeddings, words, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnQMDFTgKCY0"
   },
   "source": [
    "## Network visualization\n",
    "\n",
    "At each step, RNN produces some vector. The full layer applies only to the last output. But you can also look at intermediate states - how the network’s opinion changed about what this word refers to.\n",
    "\n",
    "** Task ** Write your visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86lffPwLKmqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DickfCNwJZFT"
   },
   "source": [
    "## Network improvement\n",
    "\n",
    "**Task** Replace SimpleRNN with LSTM. Compare quality.\n",
    "\n",
    "**Task** Add Dropout to LSTM (or later). A value of about 0.3 will be adequate.\n",
    "\n",
    "**Task** An important RNN is the Bidirectional RNN. In fact, these are two RNNs, one bypassing the sequence from left to right, the second - vice versa.\n",
    "\n",
    "As a result, for each point in time we have the vector $ h_t = [f_t; b_t] $ is the concatenation (or some other function of $ f_t $ and $ b_t $) of the states $ f_t $ and $ b_t $ of the forward and backward passage of the sequence. In sum, they cover the entire context.\n",
    "\n",
    "\n",
    "In our task, the Bidirectional option can help with the fact that the network will forget less about how the sequence began. That is, we will need to take $ f_N $ and $ b_N $ states: the first is the last state in the passage from left to right, i.e. output from the last character. The second is the last state at the back pass, i.e. output for the first character.\n",
    "\n",
    "Implement the Bidirectional Classifier. To do this, `LSTM` has a` bidirectional` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukwXJppHrDwS"
   },
   "source": [
    "# Referrence\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  \n",
    "[Understanding LSTM Networks, Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "[Recurrent Neural Networks Tutorial, Denny Britz](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)  \n",
    "[Vanishing Gradients & LSTMs, Harini Suresh](http://harinisuresh.com/2016/10/09/lstms/)\n",
    "[Non-Zero Initial States for Recurrent Neural Networks](https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html)\n",
    "[Explaining and illustrating orthogonal initialization for recurrent neural networks, Stephen Merity](http://smerity.com/articles/2016/orthogonal_init.html)\n",
    "[Comparative Study of CNN and RNN for Natural Language Processing, Yin, 2017](https://arxiv.org/abs/1702.01923)\n",
    "[cs224n \"Lecture 8: Recurrent Neural Networks and Language Models\"](https://www.youtube.com/watch?v=Keqep_PKrY8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 05 - RNNs Intro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
