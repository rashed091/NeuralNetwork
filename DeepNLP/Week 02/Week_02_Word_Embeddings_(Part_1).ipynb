{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWzU9hqUomdU"
   },
   "outputs": [],
   "source": [
    "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
    "!unzip quora.zip\n",
    "!pip install -q --upgrade nltk gensim bokeh pandas\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXl16AOdtlDk"
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErZ_TOu0vAOR"
   },
   "source": [
    "The notebook is largely based on [the first assignment from the relevant SHAD course](https://github.com/yandexdataschool/nlp_course/tree/master/week1_embeddings).*\n",
    "\n",
    "Everyone saw these pictures (I hope):\n",
    "![embeddings relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "*From [Vector Representations of Words, Tensorflow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
    "\n",
    "Today we will deal with such models.\n",
    "\n",
    "Let's start the morning with visualizations. We go to the site [http://rusvectores.org/ru/ru/ (http: //rusvectores.org/ru/) and see what the trained models for Russian can do.\n",
    "\n",
    "Note the sections * Related words * and * Calculator *, as well as the set of models that you can choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FkxsGQqZNjxj"
   },
   "source": [
    "## We train a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaOn69Bg1hH-"
   },
   "source": [
    "t’s just not fun to just look at other people's models, so we’ll gradually come closer to solving a specific task: [Quora Question Pairs at kaggle](https://www.kaggle.com/c/quora-question-pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-X7I7nc1gyS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quora_data = pd.read_csv('train.csv')\n",
    "\n",
    "quora_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p13HdkzWKtKe"
   },
   "source": [
    "Поучим на этих текстах Word2vec из gensim. \n",
    "\n",
    "Для начала объединим все тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mchv4fS_21OX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
    "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
    "\n",
    "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hZMFAmvK5b7"
   },
   "source": [
    "Для токенизации проще всего воспользоваться `nltk` (он быстрее `spacy`, но может быть хуже в отдельных случаях)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTxolf8nLM-n"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuJceE4JLRxK"
   },
   "source": [
    "**Задание** Приведите все тексты к нижнему регистру и токенизируйте их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7XbnSdt4REg"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = <do smth>\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in tokenized_texts), \\\n",
    "    \"please convert each line into a list of tokens\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in tokenized_texts), \\\n",
    "    \"please convert each line into a list of tokens\"\n",
    "\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(not is_latin(token) or token.islower() for tokens in tokenized_texts for token in tokens),\\\n",
    "    \"please lowercase each line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irl7RotC5C_B"
   },
   "outputs": [],
   "source": [
    "print([' '.join(row) for row in tokenized_texts[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kj4dC3iLdwH"
   },
   "source": [
    "Потренируем небольшую модель на полученных текстах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GNuiLio8M25"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized_texts, \n",
    "                 size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv  # define context as a 5-word window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JclToDJMNwTy"
   },
   "source": [
    "## Изучаем полученную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBVR2kY7LkCs"
   },
   "source": [
    "Ура, теперь можно делать то же, что было на `rusvectores`.\n",
    "\n",
    "Получить вектор для слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jk6Fgraj-j3c"
   },
   "outputs": [],
   "source": [
    "model.get_vector('anything')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHKRy7HyLuxH"
   },
   "source": [
    "Найти наиболее близкие слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "toyNzyTB-p70"
   },
   "outputs": [],
   "source": [
    "model.most_similar('bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A510z5gTL00E"
   },
   "source": [
    "Или даже так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2A_DF5E-ucq"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['coder', 'money'], negative=['brain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-_uKG4vNIJv"
   },
   "source": [
    "И так, конечно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_mzQgi4L474"
   },
   "outputs": [],
   "source": [
    "model.most_similar([model.get_vector('politician') - model.get_vector('power') + model.get_vector('honesty')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5BAEyMyL2kx"
   },
   "source": [
    "**Задание** Поищите аналогии самостоятельно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FYuh4DKN1Fd"
   },
   "source": [
    "## Визуализируем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdT0eIEiN4Ja"
   },
   "source": [
    "Посмотрим на проекции первых тысячи самых частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1e9yS0zBr6j"
   },
   "outputs": [],
   "source": [
    "words = sorted(model.vocab.keys(), \n",
    "               key=lambda word: model.vocab[word].count,\n",
    "               reverse=True)[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Yk5pgMXOESS"
   },
   "source": [
    "**Задание** Постройте матрицу из эмбеддингов этих слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQu724f2CAh0"
   },
   "outputs": [],
   "source": [
    "word_vectors = model.vectors[[model.vocab[word].index for word in words]]\n",
    "\n",
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), model.vectors.shape[1])\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7cgxin-OTvK"
   },
   "source": [
    "### PCA\n",
    "\n",
    "The simplest linear dimension reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
    "\n",
    "The PCA searches for axes on which the data will have the largest scatter when projected.\n",
    "\n",
    "![pca](https://i.stack.imgur.com/Q7HIP.gif)\n",
    "*From [https://stats.stackexchange.com/a/140579](https://stats.stackexchange.com/a/140579)*\n",
    "\n",
    "As a result, you can take the projections onto the first few components - and save as much information as possible, reducing the dimension.\n",
    "\n",
    "Beautiful visualizations can be found [here] (http://setosa.io/ev/principal-component-analysis/).\n",
    "\n",
    "** Task ** Use the PCA from sklearn, and then center and normalize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0fQKZw2Css4"
   },
   "outputs": [],
   "source": [
    "<imports>\n",
    "\n",
    "def get_pca_projection(word_vectors):\n",
    "    <code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_9VtLl9CviW"
   },
   "outputs": [],
   "source": [
    "word_vectors_pca = get_pca_projection(word_vectors)\n",
    "\n",
    "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
    "assert max(abs(1 - word_vectors_pca.std(0))) < 1e-5, \"points must have unit variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGyvhrk7Rlyt"
   },
   "source": [
    "Visualize what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1U58YxF3Cx0W"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBljy2hCC1qX"
   },
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOIU8uXnSItf"
   },
   "source": [
    "### TSNE\n",
    "\n",
    "A more interesting and complex (non-linear) method for visualizing high-dimensional spaces is TSNE. You can look at it in detail [here] (https://distill.pub/2016/misread-tsne/) (even more beautiful pictures!).\n",
    "\n",
    "** Task ** As with PCA, use TSNE from sklearn and normalize + center the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-nlN4_aDF9G"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    <your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_YFR_rYDK2n"
   },
   "outputs": [],
   "source": [
    "word_tsne = get_tsne_projection(word_vectors)\n",
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sz6uHYMbSjuE"
   },
   "source": [
    "## Phrased phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4GEypE4SokX"
   },
   "source": [
    "Now for tests it will be useful to use fixed embeddings. Let's load already trained model.\n",
    "(see which models and models are available at all, by calling `api.info ()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUDRtumXXF3S"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eypMhlOSXFWN"
   },
   "source": [
    "A simple and cheap way to calculate embedding of a phrase, when there are embeddingings of words - to average.\n",
    "\n",
    "So let's do this: tokenize and lower-case the phrase, average the embeddingings of the words for which they are counted.\n",
    "\n",
    "**Task** Write a function to count phrase embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F76HGRGDEPVq"
   },
   "outputs": [],
   "source": [
    "def get_phrase_embedding(model, phrase):    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    \n",
    "    <write me>\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i12XsqnIXfko"
   },
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
    "\n",
    "assert np.allclose(vector[::10],\n",
    "                   np.array([ 0.30757686, -0.05861897,  0.143751  , -0.11104885, -0.96929336,\n",
    "                             -0.21928601,  0.21652265,  0.14978765,  1.4842536 ,  0.017826  ],\n",
    "                              dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yl2nBZG0WAUx"
   },
   "source": [
    "\n",
    "Calculate the vector of all phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40LezFEJFwv3"
   },
   "outputs": [],
   "source": [
    "text_vectors = np.array([get_phrase_embedding(model, phrase) for phrase in tokenized_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBj8XMZvWFTv"
   },
   "source": [
    "And learn to look for coming!\n",
    "\n",
    "**Assignment** Find the closest questions to it by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjw7kTQ-FP11"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_nearest(model, text_vectors, texts, query, k=10):\n",
    "    <write me, please>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2yK0twNGWaQ"
   },
   "outputs": [],
   "source": [
    "results = find_nearest(model, text_vectors, texts, query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print('\\n'.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[1] == 'How do I get to the dark web?'\n",
    "assert results[4] == 'What can I do to save the world?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AutTfxbDGhku"
   },
   "outputs": [],
   "source": [
    "find_nearest(model, text_vectors, texts, query=\"How does Trump?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47Ff7heKGiGV"
   },
   "outputs": [],
   "source": [
    "find_nearest(model, text_vectors, texts, query=\"Why don't i ask a question myself?\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BhDEF11ZnTY"
   },
   "source": [
    "## Begin the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2D_LEV8cm0z3"
   },
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGge73gDid99"
   },
   "source": [
    "**Task** Let's collect tokens for the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJTjqdgiih7O"
   },
   "outputs": [],
   "source": [
    "tokenized_question1 = ...\n",
    "tokenized_question2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VBcipE7Ztkc"
   },
   "source": [
    "**Task** Calculate cosine proximity between questions.\n",
    "\n",
    "The easiest way is to implement the function of its calculation by hand:\n",
    "\n",
    "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItJLR_ENHGtI"
   },
   "outputs": [],
   "source": [
    "<do smth>\n",
    "\n",
    "cosine_similarities = <and calc the result>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgCVueJ4Z3DQ"
   },
   "source": [
    "We select the proximity threshold at which we will consider the texts to be the same.\n",
    "\n",
    "**Task** Implement the `accuracy` function that calculates the accuracy of the classification at a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41hRIb-KSA3F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracy(cosine_similarities, threshold):\n",
    "    return <accuracy>\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100, endpoint=False)\n",
    "plt.plot(thresholds, [accuracy(cosine_similarities, th) for th in thresholds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOxJSE38msmX"
   },
   "source": [
    "And run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3LVCJwqSgZz"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "res = minimize_scalar(lambda x: -accuracy(cosine_similarities, x), bounds=(0.5, 0.99), method='bounded')\n",
    "best_threshold = res.x\n",
    "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, accuracy(cosine_similarities, best_threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8yFlaffm34A"
   },
   "source": [
    "### Tf-idf weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cpeSa-4m64H"
   },
   "source": [
    "Instead of stupidly averaging the vectors, you can average them based on the weights - the familiar tf-idf will help in this.\n",
    "\n",
    "** Task ** Calculate weighted vector of questions. Use `TfidfVectorizer`\n",
    "\n",
    "`TfidfVectorizer` returns the matrix` (samples_count, words_count) `. And our embeddings have the dimension `(words_count, embedding_dim)`. So you can just multiply them. Then each phrase - the sequence of words $ w_1, \\ ldots, w_k $ - is transformed into the vector $ \\ sum_i \\ text {idf} (w_i) \\ cdot \\ text {embedding} (w_i) $. This vector should probably be normalized to the number of words $ k $.\n",
    "\n",
    "** Task ** In addition to tf-idf, you can add filtering of stop words and punctuation.\n",
    "Stop words can be taken from `nltk`:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoYjVExHd_OC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "<calculate tf-idf weighted vectors>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34Di8jXxoBxg"
   },
   "source": [
    "Посмотрим, что получается после фильтрации и векторизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k52NtVJKf1Wl"
   },
   "outputs": [],
   "source": [
    "for col in tfidf_question1[0].tocoo().col:\n",
    "    print(model.index2word[col], end=' ')\n",
    "\n",
    "print('\\n' + ' '.join(tokenized_question1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fMqY5tOoL9r"
   },
   "source": [
    "**Task** Calculate quality with new vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chf2qy7uhbPF"
   },
   "outputs": [],
   "source": [
    "cosine_similarities = <calc it somehow>\n",
    "\n",
    "res = minimize_scalar(lambda x: -accuracy(cosine_similarities, x), bounds=(0.8, 0.99), method='bounded')\n",
    "best_threshold = res.x\n",
    "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, accuracy(cosine_similarities, best_threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXi24B78sbsg"
   },
   "source": [
    "## Let's look inside learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GSDgSXfswp7"
   },
   "source": [
    "The key idea is that a word can be defined by the context in which it occurs:\n",
    "![contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
    "*From [cs224n, Lecture 2](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)*\n",
    "\n",
    "Watch how everything learns, we will be here: [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvqrFUS6vVhh"
   },
   "source": [
    "## Got a word by word machine translation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CXcr-ypzGXg"
   },
   "outputs": [],
   "source": [
    "!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n",
    "!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n",
    "!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n",
    "downloaded.GetContentFile('cc.ru.300.vec.zip')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n",
    "downloaded.GetContentFile('cc.uk.300.vec.zip')\n",
    "\n",
    "!unzip cc.ru.300.vec.zip\n",
    "!unzip cc.uk.300.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RqUeOXxws8y"
   },
   "source": [
    "\n",
    "Let's write a simple implementation of the machine translation model.\n",
    "\n",
    "The idea is based on the article [Word Translation Without Parallel Data] (https://arxiv.org/pdf/1710.04087.pdf). The authors in the repository have a lot more interesting: [https://github.com/facebookresearch/MUSE_ ((https://github.com/facebookresearch/MUSE)\n",
    "\n",
    "And we will translate from Ukrainian to Russian.\n",
    "\n",
    "! [] (https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/blue_cat_blue_whale.png)\n",
    "\n",
    "*blue kit* vs. *blue kit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjPj9FTRry0U"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rGx4TXWFJ65"
   },
   "source": [
    "\n",
    "Look at a couple of serpeni-august (being a translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FkHer36xyh4n"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RSDixWvylEP"
   },
   "outputs": [],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwmm3YQ1yl1U"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAsW7oxszE_I"
   },
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n",
    "\n",
    "\n",
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n",
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9z6ts7DC0XmN"
   },
   "source": [
    "### Учим маппинг из одного пространства эмбеддингов в другое\n",
    "\n",
    "У нас есть пары слов, соответствующих друг другу, и их эмбеддинги. Найдем преобразование из одного пространства в другое, чтобы приблизить известные нам слова:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{где} ||*||_F - \\text{норма Фробениуса}$$\n",
    "\n",
    "Эта функция очень похожа на линейную регрессию (без биаса).\n",
    "\n",
    "**Задание** Реализуйте её - воспользуйтесь `LinearRegression` из sklearn с `fit_intercept=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fraTOQtu1YWI"
   },
   "outputs": [],
   "source": [
    "mapping = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrzRk3ja1b_6"
   },
   "source": [
    "Проверим, куда перейдет `серпень`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Quax6HnF1aON"
   },
   "outputs": [],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ih1GLNZt1nZX"
   },
   "source": [
    "Должно получиться, что в топе содержатся разные месяцы, но август не первый.\n",
    "\n",
    "Будем мерять percision top-k с k = 1, 5, 10.\n",
    "\n",
    "**Задание** Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnmrLp9y2gNI"
   },
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(ru_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (_, ru) in enumerate(pairs):\n",
    "        <write code here>\n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1NIvhSH2olG"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ml_w1Tl2r7Y"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d9KQHMr2tx8"
   },
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "\n",
    "assert precision_top1 >= 0.635\n",
    "assert precision_top5 >= 0.813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNbDTP502urT"
   },
   "source": [
    "### Улучшаем маппинг\n",
    "\n",
    "Можно показать, что маппинг лучше строить ортогональным:\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, где: } W^TW = I$$\n",
    "\n",
    "Искать его можно через SVD:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "\n",
    "$$W^*=UV^T$$\n",
    "\n",
    "**Задание** Реализуйте эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9de8XZ_F3v53"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\" \n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"\n",
    "    <write code there>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WeCadzN382y"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qaMb0E3-f9"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Nn58crh4AH0"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
    "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqgcYk-c4DE5"
   },
   "source": [
    "### Пишем переводчик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwi70fP6FaAN"
   },
   "source": [
    "\n",
    "We implement a simple word-for-word translator - for each word we will look for its nearest neighbor in the common embedding space. If the word is not in the inserts - just copy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0etAHUks4JOr"
   },
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\") as in f:\n",
    "    uk_sentences = [line.rstrip().lower() for line in in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK_FJGmn4N7V"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    <implement it!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H47pbFyk4P6D"
   },
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAVWK7mE4RYU"
   },
   "outputs": [],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwffxpbmFwDh"
   },
   "source": [
    "## Referrence\n",
    "\n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n",
    "[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n",
    "[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n",
    "[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n",
    "[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n",
    "[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n",
    "[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n",
    "[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 02 - Word Embeddings (Part 1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
