{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqbkG4qoS0AB"
   },
   "source": [
    "# Text classification\n",
    "\n",
    "Let's start with the simplest - analysis of the tonality of the text. We will classify reviews from IMDB to positive / negative.\n",
    "\n",
    "Dataset taken from http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5R9e3pctHIV2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 25000\n",
      "Test size = 25000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../Data/IMDB_sentiment_dataset/train.tsv\", delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\"../Data/IMDB_sentiment_dataset/train.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print('Train size = {}'.format(len(train_df)))\n",
    "print('Test size = {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oHbJCPPwhjp"
   },
   "source": [
    "\n",
    "Look at the texts with your eyes? What are the clues, how to determine what kind of sentiment?\n",
    "\n",
    "The simplest, as always - to find keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DywUCyMLr_TD"
   },
   "outputs": [],
   "source": [
    "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
    "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
    "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
    "\n",
    "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
    "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
    "is_positive = positives_count > negatives_count\n",
    "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
    "\n",
    "accuracy = correct_count / len(test_df)\n",
    "\n",
    "print('Test accuracy = {:.2%}'.format(accuracy))\n",
    "if accuracy > 0.71:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oo8HRABxv1kW"
   },
   "source": [
    "**Task** Make up good keywords or phrases and type at least 71% accuracy on the test (and don't forget to look at the classification code!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaIrBClMUHZB"
   },
   "source": [
    "**Assignment** Does anyone like these? Personally, I do not. Write a regular calendar that will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09OkUmtde6ny"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(<smth>)\n",
    "\n",
    "print(train_df['review'].iloc[3])\n",
    "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO6D9NuMi4II"
   },
   "source": [
    "Применим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LTwQqs_hD-K"
   },
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
    "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGxzf4oXmXqw"
   },
   "source": [
    "\n",
    "It's time to go to the typewriter!\n",
    "\n",
    "How will we present the text? The easiest way is with a bag of words.\n",
    "\n",
    "Let's get a big-big dictionary - a list of all the words in the training set. Then each sentence can be represented as a vector in which it will be written, how many times each of the possible words has been encountered:\n",
    "\n",
    "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
    "\n",
    "\n",
    "A simple and enjoyable way to do this is to stuff the texts into the `CountVectorizer`.\n",
    "\n",
    "It has the following signature:\n",
    "\n",
    "```python\n",
    "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
    "```\n",
    "\n",
    "\n",
    "To begin with, pay attention to the parameters `lowercase = True` and` max_df = 1.0, min_df = 1, max_features = None` - they mean that by default all words will be converted to lower case and all words found in the texts will be included in the dictionary .\n",
    "\n",
    "If desired, it would be possible to remove too rare or too frequent words - until we do this.\n",
    "\n",
    "Let's look at a simple example of how it will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Odnum4iyGDr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "\n",
    "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
    "\n",
    "print(dummy_matrix.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3zC1ItWybVc"
   },
   "source": [
    "\n",
    "* How exactly does vectorizer define word boundaries? Note the parameter `token_pattern = r '(? U) \\ b \\ w \\ w + \\ b'` - how will it work? *\n",
    "\n",
    "Run it on real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ccd2gaCdQq2W"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_JC9n6C0bFR"
   },
   "source": [
    "\n",
    "Let's look at the words in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stV4ICO3mKsf"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oesbuQ9g0krj"
   },
   "source": [
    "Let's try to convert somebody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUWtDWcp0g7U"
   },
   "outputs": [],
   "source": [
    "vectorizer.transform([train_df['review'].iloc[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZCRef3pyCRC"
   },
   "source": [
    "What they wanted was a vector with a bow (i.e., bag-of-words) representation of the source text.\n",
    "\n",
    "And how can this information help? Well, all the same - some words are positive color, some - negative. Most are generally neutral, yes.\n",
    "\n",
    "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
    "\n",
    "I would like, probably, to choose the coefficients that will determine the level of color, right? It is necessary to select by the training sample, and not as we did before.\n",
    "\n",
    "For example, for sampling\n",
    "\n",
    "```\n",
    "1 The movie was excellent\n",
    "0 the movie was awful\n",
    "```\n",
    "\n",
    "It’s easy to pick odds on the eye: something like `+ 1` for` excellent`, `-1` for` awful` and zeros for everything else.\n",
    "\n",
    "Let's build a linear model that will do this. She will learn to build a separating hyperplane in the space of bow-vectors.\n",
    "\n",
    "Check out how the logistic regression can handle our super sample of a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6WVgK4LtUn2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "dummy_labels = [1, 0]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(dummy_data, dummy_labels)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9Y-kq-tv-XY"
   },
   "source": [
    "\n",
    "It turned out that it is necessary.\n",
    "\n",
    "Now run it on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvxHIIbSiUXq"
   },
   "outputs": [],
   "source": [
    "model.fit(train_df['review'], train_df['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d3BBV_uUu-O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, test_df):\n",
    "    preds = model.predict(test_df['review'])\n",
    "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
    "    \n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pnmmz3u71ctO"
   },
   "source": [
    "\n",
    "Progress!\n",
    "\n",
    "I want to somehow see what interested the classifier. Fortunately, this is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8W1Ngl-aVuYx"
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(classifier, vec=vectorizer, top=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgCE9tDk-aO"
   },
   "source": [
    "\n",
    "Let's look at specific examples of his work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-4sVWBOWJpo"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoZhtlYlW-xG"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[0] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[0], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNNUqZvplhAC"
   },
   "source": [
    "\n",
    "Let's look at examples of incorrect classification, finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf9ZzS8fXKFm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = model.predict(test_df['review'])\n",
    "incorrect_pred_index = np.random.choice(np.where(preds != test_df['is_positive'])[0])\n",
    "\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[incorrect_pred_index],\n",
    "                     vec=vectorizer, targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbFXKNrngP46"
   },
   "source": [
    "## We invent new signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dz7GSFzIlv4V"
   },
   "source": [
    "### Tf-idf\n",
    "\n",
    "Now we look at all words with the same weight - although some of them are more rare, some more frequent, and this frequency is useful, generally speaking, information.\n",
    "\n",
    "The easiest way to add statistical information about frequencies is to do * tf-idf * weighting:\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
    "\n",
    "*tf* - term-frequency - frequency of the word `t` in a specific document` d` (reviews in our case). This is exactly what we already thought.\n",
    "\n",
    "*idf* - inverse document-frequency - coefficient, which is greater, the smaller the number of documents met this word. It is considered something like this:\n",
    "\n",
    "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
    "\n",
    "where $n_d$ is the number of all documents, and $ n_{d (t)} $ is the number of documents with the word `t`.\n",
    "\n",
    "Using it is easy - you need to replace `CountVectorizer` with` TfidfVectorizer`.\n",
    "\n",
    "**Task** Try running `TfidfVectorizer`. Look at the mistakes that he learned to correct, and the mistakes that he began to make - compared to `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3DjjiJglvT3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xe_CJxQ0tFP9"
   },
   "source": [
    "### N-gram\n",
    "\n",
    "Until now, we looked at the texts as a bag of words - but it is obvious that there is a difference between the `good movie` and` not good movie`.\n",
    "\n",
    "Add information (at least some) about the sequences of words - we will also extract the digrams of words.\n",
    "\n",
    "In Vectorizers, this has the option `ngram_range = (n_1, n_2)` - it says that we need n_1 -... n_2-grams.\n",
    "\n",
    "**Task** Try an increased range and interpret the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDpdrT0HuKYN"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrBoThj6wl2F"
   },
   "source": [
    "### N-grams characters\n",
    "\n",
    "Character n-grams provide an easy way to learn useful roots and suffixes without being associated with this linguistics of yours - just statistics, only hardcore.\n",
    "\n",
    "For example, the word `badass` we can represent in the form of such a sequence of trigrams:\n",
    "\n",
    "`## b #ba bad ada das ass ss # s ##`\n",
    "\n",
    "So interpretable, is not it?\n",
    "\n",
    "It’s still as easy to implement as you need to put an analyzer = 'char'` in your favorite Vectorizer and choose the size of `ngram_range`.\n",
    "\n",
    "** Task ** File a classifier on n-grams of characters and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFaWmUrGyY3n"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9E1E1Bn8yvhq"
   },
   "outputs": [],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
    "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
    "                     targets=['positive'], target_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fZ6I8mN0VPU"
   },
   "source": [
    "## We connect linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkywIvbp4N-L"
   },
   "source": [
    "### Lemmatization and stemming\n",
    "\n",
    "If you look closely, you can find the forms of one word with different semantic coloring according to the classifier. Or not?\n",
    "\n",
    "** Assignment ** Find the word forms with different semantic coloring.\n",
    "\n",
    "Believe that they are, try something to do with it.\n",
    "\n",
    "For example, lemmatizing - we reduce all words to the initial form. The spacy library will help in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5CTTdtxI5yv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "\n",
    "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXs8Ia_bqeS0"
   },
   "outputs": [],
   "source": [
    "for token in docs[0]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVXJr_xxqlPK"
   },
   "source": [
    "Весь этот процесс очень долгий, поэтому я предподсчитал всё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZNz7E5JqrWz"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_docs.pkl', 'rb') as f:\n",
    "    train_docs = pickle.load(f)\n",
    "    \n",
    "with open('test_docs.pkl', 'rb') as f:\n",
    "    test_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pfup3O5r30m"
   },
   "source": [
    "** Task ** Make a classifier on lemmatized texts.\n",
    "\n",
    "An easier way to normalize words is to use stemming. It is a little dull, does not take into account the context, but sometimes it turns out to be even more effective than lemmatization - and, most importantly, faster.\n",
    "\n",
    "In essence, this is just a set of rules how to cut a word to get a stem (stem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cr0w_hVyrqFx"
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('become'))\n",
    "print(stemmer.stem('becomes'))\n",
    "print(stemmer.stem('became'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxGSEqHNsfHy"
   },
   "source": [
    "Task Try to classify bases instead of lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1NBpkwuspBX"
   },
   "source": [
    "### NER\n",
    "\n",
    "There are a lot of named entities in review texts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xki8Maf9MrVY"
   },
   "outputs": [],
   "source": [
    "displacy.render(docs[0], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGhcnuZSteHc"
   },
   "source": [
    "Generally speaking, why should any Depp have to carry a semantic coloring? However, it turns out that the classifier learns that some names are more often in positive reviews - or vice versa. It looks like retraining - why not try cutting out entities?\n",
    "\n",
    "**Task** Remove some of the entities from the texts, using the coordinates of the zapikennyh files. The description of entities can be viewed [here] (https://spacy.io/api/annotation#named-entities). Run the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2fHA70b0zEZ"
   },
   "source": [
    "## Turn on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1VCrM671XO5"
   },
   "source": [
    "We came here to do deep learning, and for some reason we are doing a model on logistic regression. How so?\n",
    "\n",
    "Let's try to launch a relatively standard model for text classification - a convolutional network on top of word embeddings.\n",
    "\n",
    "Understand what kind of beast we will be in the next classes, but for now we will just use it :)\n",
    "\n",
    "Each sentence must be represented by a set of words - and immediately the problems begin. First, how to limit the length of a sentence?\n",
    "\n",
    "Let us estimate according to the histogram, which length suits us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O477ZV1t1WIO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text)), bins='auto')\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXO4xi0u5m8l"
   },
   "source": [
    "In addition, you need to renumber the words somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIMGE7L-55fs"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
    "\n",
    "word2idx = {\n",
    "    '': 0,\n",
    "    '<unk>': 1\n",
    "}\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < <choose the limit>:\n",
    "        break\n",
    "        \n",
    "    word2idx[word] = len(word2idx)\n",
    "    \n",
    "print('Words count', len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTptlmd1yD3J"
   },
   "source": [
    "**Task** Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPP0cYdJ5VkE"
   },
   "outputs": [],
   "source": [
    "def convert(texts, word2idx, max_text_len):\n",
    "    data = np.zeros((len(texts), max_text_len), dtype=np.int)\n",
    "    \n",
    "    <fill the matrix>\n",
    "    return data\n",
    "\n",
    "X_train = convert(train_df.review, word2idx, <your limit>)\n",
    "X_test = convert(test_df.review, word2idx, <your limit>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYb-p5ioyLUZ"
   },
   "source": [
    "Поставим учиться модельку на keras.\n",
    "\n",
    "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
    "1. Определить модель, например:\n",
    "```python \n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
    "```\n",
    "2. Задать функцию потерь и оптимизатор:\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "3. Запустить обучение:\n",
    "```python\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, y_test))\n",
    "```\n",
    "\n",
    "\n",
    "NLP most often sets classification tasks, so you need to remember the following loss functions:\n",
    "\n",
    "**categorical_crossentropy** - for multi-class classification, one-hot-encoding vectors should be transmitted as labels\n",
    "**sparse_categorical_crossentropy** - similar to the previous one, but just the indices of the corresponding classes should be passed as labels\n",
    "**binary_crossentropy** - for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDjri3167vFf"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
    "    Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw93gTmq8gZl"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=10, \n",
    "          validation_data=(X_test, test_df.is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBGdVRQTyynD"
   },
   "source": [
    "**Task** Calculate the quality of the model on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-MHfe3by8JW"
   },
   "source": [
    "## A bit of math\n",
    "\n",
    "Let us recall how the logistic regression, which we used so much here, works.\n",
    "\n",
    "Useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZBLLlHt8s5g"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w, X, y = np.random.random(10), np.random.random((11, 10)), 2 * (np.random.randint(0, 2, 11) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VFuK-ws8wdO"
   },
   "source": [
    "Это обычная линейная функция\n",
    "\n",
    "$$h_w(X) = \\sigma(X w),$$\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Подсчитайте ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVd7ttkozkFJ"
   },
   "outputs": [],
   "source": [
    "def forward(X, w):\n",
    "    \"\"\" Функция, вычисляющая h(X, w)\n",
    "    X - матрица (n, m)\n",
    "    w - вектор (m,)\n",
    "    \"\"\"\n",
    "    <calc h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "It8mydxfzr-Q"
   },
   "source": [
    "Функция потерь \n",
    "$$J(w)  = \\frac{1}{m} \\left(-y^T \\text{log}(h_w) - (1-y)^T \\text{log}(1 - h_w)\\right)$$\n",
    "\n",
    "Для оптимизации с SGD нужно считать градиент по функции потерь\n",
    "$\\frac{\\partial J(w)}{\\partial w}$.\n",
    "\n",
    "**Задание** Подсчитайте их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjLlb4ux0oJ_"
   },
   "outputs": [],
   "source": [
    "def loss(X, w, y):\n",
    "    \"\"\" Функция потерь\n",
    "    X - матрица (n, m)\n",
    "    w - вектор (m,)\n",
    "    y - вектор (n,)\n",
    "    \"\"\"\n",
    "    <calc loss>\n",
    "\n",
    "\n",
    "def gradient(X, w, y):\n",
    "    \"\"\" Градиент функции потерь по w\n",
    "    X - матрица (n, m)\n",
    "    w - вектор (m,)\n",
    "    y - вектор (n,)\n",
    "    \"\"\"\n",
    "    <calc grad>\n",
    "\n",
    "\n",
    "print('loss = ', loss(X, w, y))\n",
    "print('gradient = ', gradient(X, w, y))\n",
    "\n",
    "assert gradient(X, w, y).shape == w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuxhtK1m9Jal"
   },
   "source": [
    "Очень часто при подсчёте градиента допускаются ошибки, проверьте правильность реализации подсчёта градиента с помощью конечных разностей: \n",
    "\n",
    "$$[\\nabla f(x)]_i \\approx \\frac{f(x + \\varepsilon \\cdot e_i) - f(x)}{\\varepsilon}$$\n",
    "\n",
    "где $e_i = (0, ... , 0, 1, 0, ..., 0)$ - i-й базисный орт, $\\varepsilon \\approx 10^{-8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2FgTg-T9TLO"
   },
   "outputs": [],
   "source": [
    "def grad_finite_diff(func, x, eps=1e-8):\n",
    "    \"\"\"\n",
    "    w - вектор (m,)\n",
    "    func - скалярная функция от векторного аргумента w, func(w) = число\n",
    "    eps - константа для проверки градиента\n",
    "    \"\"\"\n",
    "    x, fval, dnum = x.astype(np.float64), func(x), np.zeros_like(x)\n",
    "    \n",
    "    E = np.eye(x.shape[0])\n",
    "    deltas = np.array([func(x + eps * E[i]) for i in range(x.shape[0])])\n",
    "    return (deltas - fval) / eps\n",
    "\n",
    "mat_grad = gradient(X, w, y)\n",
    "num_grad = grad_finite_diff(lambda w: loss(X, w, y), w)\n",
    "\n",
    "err = np.max(np.abs(mat_grad - num_grad))\n",
    "print('err = ', err, 'ok' if err < 1e-6 else 'ошибка очень большая =(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zhjPhrM1WR-"
   },
   "source": [
    "В итоге получим такой класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6UOJICT020e"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        self._w = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            grad = gradient(X, self._w, y)\n",
    "            self._w -= self.lr * grad\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return forward(X, self._w)\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A-V-7ST9aIF"
   },
   "source": [
    "Посмотрим, как всё работает на простых датасетах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmO0F2cx9X-d"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "\n",
    "model = LogisticRegression(lr=0.1, num_iter=300000)\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
    "plt.legend()\n",
    "\n",
    "x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs = model.predict_prob(grid).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaoQgdAJ9ZaS"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(noise=0.1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
    "plt.legend()\n",
    "\n",
    "x1_min, x1_max = X[:,0].min(), X[:,0].max()\n",
    "x2_min, x2_max = X[:,1].min(), X[:,1].max()\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "probs = model.predict_prob(grid).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9coIbUWkn6xs"
   },
   "source": [
    "# Сдача задания\n",
    "\n",
    "Сдаем через [форму](https://goo.gl/forms/TWl14R42ddDPvZLU2).\n",
    "\n",
    "И можно заполнить [анонимный опрос](https://goo.gl/forms/RII9cRcwoaJjPr2G3)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
