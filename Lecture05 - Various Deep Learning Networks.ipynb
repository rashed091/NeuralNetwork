{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimization\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashed/.virtualenvs/dl/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# 28x28\n",
    "train_dataset = data.MNIST(root='dataset/', train=True,\n",
    "                           transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = data.MNIST(root='dataset', train=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 32\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = Net().to(device)\n",
    "\n",
    "summary(model, (1, 28, 28))\n",
    "        ax = axes.item(i)\n",
    "        ax.imshow(tensor[s].numpy())\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.set_title(\"Sample Index\\n{}\".format(s))\n",
    "        \n",
    "plot_mnist_sample(train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, train_dataset, test_dataset, model, num_epochs, batch_size, learning_rate):\n",
    "        self.train_loader = train_dataset\n",
    "        self.test_loader = test_dataset\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimization.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.train_state = {\n",
    "            'done_training': False,\n",
    "            'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1}\n",
    "\n",
    "    def update_train_state(self):\n",
    "        # Verbose\n",
    "        print(\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "            self.train_state['epoch_index'], self.train_state['learning_rate'],\n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1],\n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "    def compute_accuracy(self, outputs, target):\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == target).sum()\n",
    "        accuracy = 100 * correct / len(predicted)\n",
    "        return accuracy\n",
    "\n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over train dataset\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images = images.requires_grad_()\n",
    "                # zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # compute the output\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (i + 1)\n",
    "\n",
    "                # compute gradients using loss\n",
    "                loss.backward()\n",
    "\n",
    "                # use optimizer to take a gradient step\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(outputs, labels)\n",
    "                running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                # compute the output\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (i + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(outputs, labels)\n",
    "                running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.update_train_state()\n",
    "\n",
    "\n",
    "    def run_test_loop(self):\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            # compute the output\n",
    "            outputs = self.model(images)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = self.compute_accuracy(outputs, labels)\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "\n",
    "    def plot_performance(self):\n",
    "        # Figure size\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(self.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(self.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(self.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(self.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Save figure\n",
    "#         plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # Show plots\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (LeNet)\n",
    "\n",
    "In our first encounter with image data we applied a [Multilayer Perceptron](../chapter_deep-learning-basics/mlp-scratch.md) to pictures of clothing in the Fashion-MNIST data set. Both the height and width of each image were 28 pixels. We expanded the pixels in the image line by line to get a vector of length 784, and then used them as inputs to the fully connected layer. However, this classification method has certain limitations:\n",
    "\n",
    "1. The adjacent pixels in the same column of an image may be far apart in this vector. The patterns they create may be difficult for the model to recognize. In fact, the vectorial representation ignores position entirely - we could have permuted all $28 \\times 28$ pixels at random and obtained the same results.\n",
    "2. For large input images, using a fully connected layer can easily cause the model to become too large, as we discussed previously.\n",
    "\n",
    "As discussed in the previous sections, the convolutional layer attempts to solve both problems. On the one hand, the convolutional layer retains the input shape, so that the correlation of image pixels in the directions of both height and width can be recognized effectively. On the other hand, the convolutional layer repeatedly calculates the same kernel and the input of different positions through the sliding window, thereby avoiding excessively large parameter sizes.\n",
    "\n",
    "A convolutional neural network is a network with convolutional layers. In this section, we will introduce an early convolutional neural network used to recognize handwritten digits in images - [LeNet5](http://yann.lecun.com/exdb/lenet/). Convolutional networks were invented by Yann LeCun and coworkers at AT&T Bell Labs in the early 90s. LeNet showed that it was possible to use gradient descent to train the convolutional neural network for handwritten digit recognition. It achieved outstanding results at the time (only matched by Support Vector Machines at the time).\n",
    "\n",
    "## LeNet\n",
    "\n",
    "LeNet is divided into two parts: a block of convolutional layers and one of fully connected ones. Below, we will introduce these two modules separately. Before going into details, let's briefly review the model in pictures. To illustrate the issue of channels and the specific layers we will use a rather description (later we will see how to convey the same information more concisely).\n",
    "\n",
    "<img src=\"images/lenet.svg\" alt=\"\">\n",
    "\n",
    "\n",
    "The basic units in the convolutional block are a convolutional layer and a subsequent average pooling layer (note that max-pooling works better, but it had not been invented in the 90s yet). The convolutional layer is used to recognize the spatial patterns in the image, such as lines and the parts of objects, and the subsequent average pooling layer is used to reduce the dimensionality. The convolutional layer block is composed of repeated stacks of these two basic units. In the convolutional layer block, each convolutional layer uses a $5\\times 5$ window and a sigmoid activation function for the output (note that ReLU works better, but it had not been invented in the 90s yet). The number of output channels for the first convolutional layer is 6, and the number of output channels for the second convolutional layer is increased to 16. This is because the height and width of the input of the second convolutional layer is smaller than that of the first convolutional layer. Therefore, increasing the number of output channels makes the parameter sizes of the two convolutional layers similar. The window shape for the two average pooling layers of the convolutional layer block is $2\\times 2$ and the stride is 2. Because the pooling window has the same shape as the stride, the areas covered by the pooling window sliding on each input do not overlap. In other words, the pooling layer performs downsampling.\n",
    "\n",
    "The output shape of the convolutional layer block is (batch size, channel, height, width). When the output of the convolutional layer block is passed into the fully connected layer block, the fully connected layer block flattens each example in the mini-batch. That is to say, the input shape of the fully connected layer will become two dimensional: the first dimension is the example in the mini-batch, the second dimension is the vector representation after each example is flattened, and the vector length is the product of channel, height, and width.  The fully connected layer block has three fully connected layers. They have 120, 84, and 10 outputs, respectively. Here, 10 is the number of output classes.\n",
    "\n",
    "Next, we implement the LeNet model through the Sequential class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "              ReLU-2            [-1, 6, 28, 28]               0\n",
      "         MaxPool2d-3            [-1, 6, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 14, 14]           2,416\n",
      "              ReLU-5           [-1, 16, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 16, 7, 7]               0\n",
      "            Linear-7                   [-1, 10]           7,850\n",
      "================================================================\n",
      "Total params: 10,422\n",
      "Trainable params: 10,422\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.18\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashed/.virtualenvs/dl/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2) \n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.c1(x))\n",
    "        out = self.s2(out)\n",
    "        out = self.relu2(self.c3(out))\n",
    "        out = self.s4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "\n",
    "model = LeNet().to(device)\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(train_loader, test_loader, model, num_epochs, batch_size, 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f66630d51ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a45c87c6577c>\u001b[0m in \u001b[0;36mrun_train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# compute the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-14c861f089ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "AlexNet was introduced in 2012, named after Alex Krizhevsky, the first author of the eponymous [paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks). AlexNet uses an 8-layer convolutional neural network and won the ImageNet Large Scale Visual Recognition Challenge 2012 with a large margin. This network proved, for the first time, that the features obtained by learning can transcend manually-design features, breaking the previous paradigm in computer vision. The architectures of AlexNet and LeNet are *very similar*, as the diagram below illustrates. Note that we provide a slightly streamlined version of AlexNet which removes the quirks that were needed in 2012 to make the model fit on two small GPUs.\n",
    "\n",
    "<img src=\"images/alexnet-all.svg\" alt=\"\">\n",
    "\n",
    "The design philosophies of AlexNet and LeNet are very similar, but there are also significant differences.\n",
    "First, AlexNet is much deeper than the comparatively small LeNet5. AlexNet consists of eight layers, five convolutional layers, two fully connected hidden layers, and one fully connected output layer. Second, AlexNet used the ReLU instead of the sigmoid as its activation function. This improved convergence during training significantly. Let's delve into the details below.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "In AlexNet's first layer, the convolution window shape is $11\\times11$. Since most images in ImageNet are more than ten times higher and wider than the MNIST images, objects in ImageNet images take up more pixels. Consequently, a larger convolution window is needed to capture the object. The convolution window shape in the second layer is reduced to $5\\times5$, followed by $3\\times3$. In addition, after the first, second, and fifth convolutional layers, the network adds maximum pooling layers with a window shape of $3\\times3$ and a stride of 2. Moreover, AlexNet has ten times more convolution channels than LeNet.\n",
    "\n",
    "After the last convolutional layer are two fully connected layers with 4096 outputs. These two huge fully connected layers produce model parameters of nearly 1 GB. Due to the limited memory in early GPUs, the original AlexNet used a dual data stream design, so that one GPU only needs to process half of the model. Fortunately, GPU memory has developed tremendously over the past few years, so we usually do not need this special design anymore (our model deviates from the original paper in this aspect).\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Second, AlexNet changed the sigmoid activation function to a simpler ReLU activation function. On the one hand, the computation of the ReLU activation function is simpler. For example, it does not have the exponentiation operation found in the sigmoid activation function. On the other hand, the ReLU activation function makes model training easier when using different parameter initialization methods. This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that back propagation cannot continue to update some of the model parameters. In contrast, the gradient of the ReLU activation function in the positive interval is always 1. Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, so that the model cannot be effectively trained.\n",
    "\n",
    "### Capacity Control and Preprocessing\n",
    "\n",
    "AlexNet controls the model complexity of the fully connected layer by [dropout](../chapter_deep-learning-basics/dropout.md) section), while LeNet only uses weight decay. To augment the data even further, the training loop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and color changes. This makes the model more robust and the larger sample size effectively reduces overfitting. We will discuss preprocessing in detail in a [subsequent section](../chapter_computer-vision/image-augmentation.md).\n",
    "\n",
    "### Training\n",
    "The network takes 90 epochs in five or six days to train on two GTX 580 GPUs. SGD with learning rate 0.01, momentum 0.9 and weight decay 0.0005 is used. Learning rate is divided by 10 once the the accuracy plateaus. The leaning rate is descreased 3 times during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "              ReLU-2           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
      "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
      "              ReLU-5          [-1, 256, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-10          [-1, 384, 13, 13]               0\n",
      "           Conv2d-11          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-12          [-1, 384, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 384, 6, 6]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 384, 6, 6]               0\n",
      "           Linear-15                 [-1, 4096]      56,627,200\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "           Linear-17                 [-1, 4096]      16,781,312\n",
      "             ReLU-18                 [-1, 4096]               0\n",
      "           Linear-19                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 81,695,208\n",
      "Trainable params: 81,695,208\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.46\n",
      "Params size (MB): 311.64\n",
      "Estimated Total Size (MB): 323.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(384 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = AlexNet()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Network\n",
    "\n",
    "Like AlexNet and LeNet, the VGG Network is composed of convolutional layer modules attached to fully connected layers. Several `vgg_block` modules are connected in series in the convolutional layer module, the hyper-parameter of which is defined by the variable `conv_arch`. This variable specifies the numbers of convolutional layers and output channels in each VGG block. The fully connected module is the same as that of AlexNet.\n",
    "\n",
    "<img src=\"images/vgg.svg\" alt=\"\">\n",
    "\n",
    "\n",
    "The VGG network proposed by Simonyan and Ziserman has 5 convolutional blocks, among which the former two use a single convolutional layer, while the latter three use a double convolutional layer. The first block has 64 output channels, and the latter blocks double the number of output channels, until that number reaches 512. Since this network uses 8 convolutional layers and 3 fully connected layers, it is often called VGG-11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks Using Blocks (VGG)\n",
    "\n",
    "AlexNet adds three convolutional layers to LeNet. Beyond that, the authors of AlexNet made significant adjustments to the convolution windows, the number of output channels, nonlinear activation, and regularization. Although AlexNet proved that deep convolutional neural networks can achieve good results, it does not provide simple rules to guide subsequent researchers in the design of new networks. In the following sections, we will introduce several different concepts used in deep network design.\n",
    "\n",
    "Progress in this field mirrors that in chip design where engineers went from placing transistors (neurons) to logical elements (layers) to logic blocks  (the topic of the current section). The idea of using blocks was first proposed by the [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) (VGG) at Oxford University. This led to the VGG network, which we will be discussing below. When using a modern deep learning framework repeated structures can be expressed as *code* with for loops and subroutines. Just like we would use a for loop to count from 1 to 10, we'll use code to combine layers.\n",
    "\n",
    "### VGG Blocks\n",
    "\n",
    "The basic building block of a ConvNet is the combination of a convolutional layer (with padding to keep the resolution unchanged), followed by a nonlinearity such as a ReLu. A VGG block is given by a sequence of such layers, followed by maximum pooling. Throughout their design [Simonyan and Ziserman, 2014](https://arxiv.org/abs/1409.1556) used convolution windows of size 3 and maximum pooling with stride and window width 2, effectively halving the resolution after each block. We use the `vgg_block` function to implement this basic VGG block. This function takes the number of convolutional layers `num_convs` and the number of output channels `num_channels` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def vgg_block(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "model = VGG(vgg_block(cfg['A']))\n",
    "# summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1\n",
    "learning_rate = 0.0002\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./images\"\n",
    "img_data = dset.ImageFolder(img_dir, transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "\n",
    "img_batch = data.DataLoader(img_data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    for img,label in img_batch:\n",
    "        img = Variable(img).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = loss_func(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 ==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network in Network (NiN)\n",
    "\n",
    "LeNet, AlexNet, and VGG all share a common design pattern: extract the spatial features through a sequence of convolutions and pooling layers and then post-process the representations via fully connected layers. The improvements upon LeNet by AlexNet and VGG mainly lie in how these later networks widen and deepen these two modules. An alternative is to use fully connected layers much earlier in the process. However, a careless use of a dense layer would destroy the spatial structure of the data entirely, since fully connected layers mangle all inputs. Network in Network (NiN) blocks offer an alternative. They were proposed by [Lin, Chen and Yan, 2013](https://arxiv.org/pdf/1312.4400.pdf) based on a very simple insight - to use an MLP on the channels for each pixel separately.\n",
    "\n",
    "### NiN Blocks\n",
    "\n",
    "We know that the inputs and outputs of convolutional layers are usually four-dimensional arrays (example, channel, height, width), while the inputs and outputs of fully connected layers are usually two-dimensional arrays (example, feature). This means that once we process data by a fully connected layer it's virtually impossible to recover the spatial structure of the representation. But we could apply a fully connected layer at a pixel level: Recall the $1\\times 1$ convolutional layer described in the section discussing [channels](channels.md). This somewhat unusual convolution can be thought of as a fully connected layer processing channel activations on a per pixel level. Another way to view this is to think of each element in the spatial dimension (height and width) as equivalent to an example, and the channel as equivalent to a feature. NiNs use the $1\\times 1$ convolutional layer instead of a fully connected layer. The spatial information can then be naturally passed to the subsequent layers. The figure below illustrates the main structural differences between NiN and AlexNet, VGG, and other networks.\n",
    "\n",
    "<img src=\"images/nin-compare.svg\" alt=\"\">\n",
    "\n",
    "The NiN block is the basic block in NiN. It concatenates a convolutional layer and two $1\\times 1$ convolutional layers that act as fully connected layers (with ReLU in between). The convolution width of the first layer is typically set by the user. The subsequent widths are fixed to $1 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NiN Model\n",
    "\n",
    "NiN was proposed shortly after the release of AlexNet. Their convolutional layer settings share some similarities. NiN uses convolutional layers with convolution window shapes of $11\\times 11$, $5\\times 5$, and $3\\times 3$, and the corresponding numbers of output channels are the same as in AlexNet. Each NiN block is followed by a maximum pooling layer with a stride of 2 and a window shape of $3\\times 3$.\n",
    "\n",
    "In addition to using NiN blocks, NiN’s design is significantly different from AlexNet by avoiding dense connections entirely: Instead, NiN uses a NiN block with a number of output channels equal to the number of label classes, and then uses a global average pooling layer to average all elements in each channel for direct use in classification. Here, the global average pooling layer, i.e. the window shape, is equal to the average pooling layer of the input spatial dimension shape. The advantage of NiN's design is that it can significantly reduce the size of model parameters, thus mitigating overfitting. In other words, short of the average pooling all operations are convolutions. However, this design sometimes results in an increase in model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          11,712\n",
      "              ReLU-2           [-1, 96, 55, 55]               0\n",
      "            Conv2d-3           [-1, 96, 55, 55]           9,312\n",
      "              ReLU-4           [-1, 96, 55, 55]               0\n",
      "            Conv2d-5           [-1, 96, 55, 55]           9,312\n",
      "              ReLU-6           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-7           [-1, 96, 27, 27]               0\n",
      "            Conv2d-8          [-1, 256, 27, 27]         614,656\n",
      "              ReLU-9          [-1, 256, 27, 27]               0\n",
      "           Conv2d-10          [-1, 256, 27, 27]          65,792\n",
      "             ReLU-11          [-1, 256, 27, 27]               0\n",
      "           Conv2d-12          [-1, 256, 27, 27]          65,792\n",
      "             ReLU-13          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-14          [-1, 256, 13, 13]               0\n",
      "           Conv2d-15          [-1, 384, 15, 15]         885,120\n",
      "             ReLU-16          [-1, 384, 15, 15]               0\n",
      "           Conv2d-17          [-1, 384, 15, 15]         147,840\n",
      "             ReLU-18          [-1, 384, 15, 15]               0\n",
      "           Conv2d-19          [-1, 384, 15, 15]         147,840\n",
      "             ReLU-20          [-1, 384, 15, 15]               0\n",
      "        MaxPool2d-21            [-1, 384, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [-1, 384, 4, 4]               0\n",
      "           Linear-23                 [-1, 1000]       6,145,000\n",
      "================================================================\n",
      "Total params: 8,102,376\n",
      "Trainable params: 8,102,376\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 26.85\n",
      "Params size (MB): 30.91\n",
      "Estimated Total Size (MB): 57.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(inplace=True))\n",
    "    return blk\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(NiN, self).__init__()\n",
    "        self.features = nn.Sequential(nin_block(1, 96, 11, 4, 2), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                     nin_block(96, 256, 5, 1, 2), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                     nin_block(256, 384, 3, 1, 2), nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.linear = nn.Linear(384 * 4 * 4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "model = NiN()\n",
    "summary(model, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Training\n",
    "\n",
    "As before we use Fashion-MNIST to train the model. NiN's training is similar to that for AlexNet and VGG, but it often uses a larger learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* NiN uses blocks consisting of a convolutional layer and multiple $1\\times 1$ convolutional layer. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.\n",
    "* NiN removes the fully connected layers and replaces them with global average pooling (i.e. summing over all locations) after reducing the number of channels to the desired number of outputs (e.g. 10 for Fashion-MNIST).\n",
    "* Removing the dense layers reduces overfitting. NiN has dramatically fewer parameters.\n",
    "* The NiN design influenced many subsequent convolutional neural networks designs.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Tune the hyper-parameters to improve the classification accuracy.\n",
    "1. Why are there two $1\\times 1$ convolutional layers in the NiN block? Remove one of them, and then observe and analyze the experimental phenomena.\n",
    "1. Calculate the resource usage for NiN\n",
    "    * What is the number of parameters?\n",
    "    * What is the amount of computation?\n",
    "    * What is the amount of memory needed during training?\n",
    "    * What is the amount of memory needed during inference?\n",
    "1. What are possible problems with reducing the $384 \\times 5 \\times 5$ representation to a $10 \\times 5 \\times 5$ representation in one step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet (Inception Module v1)\n",
    "\n",
    "Whenever I discuss or show [GoogleNet architecture](http://arxiv.org/pdf/1409.4842v1.pdf), one question always comes up - <br /><br />\n",
    "**<center>\"Why 1x1 convolution ? Is it not redundant ?</center>**\n",
    "\n",
    "<center>\n",
    "<img align=\"center\" alt=\"Convolution with Kernel of size 3x3\" src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif\" />\n",
    "<img align=\"center\" alt=\"Convolution with Kernel of size 1x1\" src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed_small.gif\" />\n",
    "</center>\n",
    "<center>\n",
    "left : **Convolution with kernel of size 3x3**               right : **Convolution with kernel of size 1x1**\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "## Simple Answer\n",
    "\n",
    "Most simplistic explanation would be that 1x1 convolution leads to dimension reductionality. For example, an image of 200 x 200 with 50 features on convolution with 20 filters of 1x1 would result in size of 200 x 200 x 20.\n",
    "But then again, is this is the best way to do dimensionality reduction in the convoluational neural network? What about the efficacy vs efficiency?\n",
    "\n",
    "## Complex Answer\n",
    "\n",
    "### Feature transformation\n",
    "Although 1x1 convolution is a 'feature pooling' technique, there is more to it than just sum pooling of features across various channels/feature-maps of a given layer. \n",
    "1x1 convolution acts like coordinate-dependent transformation in the filter space[[1](https://plus.google.com/118431607943208545663/posts/2y7nmBuh2ar)]. It is important to note here that this transformation is strictly linear, but in most of application of 1x1 convolution, it is succeeded by a non-linear activation layer like ReLU. This transformation is learned through the (stochastic) gradient descent. But an important distinction is that it suffers with less over-fitting due to smaller kernel size (1x1).\n",
    "\n",
    "### Deeper Network\n",
    "\n",
    "One by One convolution was first introduced in this paper titled [Network in Network](http://arxiv.org/pdf/1312.4400v3.pdf). In this paper, the author's goal was to generate a deeper network without simply stacking more layers. It replaces few filters with a smaller perceptron layer with mixture of 1x1 and 3x3 convolutions. In a way, it can be seen as \"going wide\" instead of \"deep\", but it should be noted that in machine learning terminology, 'going wide' is often meant as adding more data to the training. Combination of 1x1 (x F) convolution is mathematically equivalent to a multi-layer perceptron.[[2](https://www.reddit.com/r/MachineLearning/comments/3oln72/1x1_convolutions_why_use_them/cvyxood)]. \n",
    "\n",
    "**Inception Module**\n",
    "\n",
    "In GoogLeNet architecture, 1x1 convolution is used for two purposes \n",
    "\n",
    "  * To make network deep by adding an \"inception module\" like Network in Network paper, as described above.\n",
    "  * To reduce the dimensions inside this \"inception module\".\n",
    "  * To add more non-linearity by having ReLU immediately after every 1x1 convolution.\n",
    "\n",
    "Here is the scresnshot from the paper, which elucidates above points :\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/inception_1x1.png\" alt=\"1x1 convolutions in GoogLeNet\"/>\n",
    "</center>\n",
    "**<center>1x1 convolutions in GoogLeNet</center>**\n",
    "\n",
    "It can be seen from the image on the right, that 1x1 convolutions (in yellow), are specially used before 3x3 and 5x5 convolution to reduce the dimensions. It should be noted that a two step convolution operation can always to combined into one, but in this case and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined.\n",
    "\n",
    "\n",
    "In designing such a network, it is important to note that initial convolution kernel should be of size larger than 1x1 to have a receptive field capable of capturing locally spatial information. According to the NIN paper, 1x1 convolution is equivalent to cross-channel parametric pooling layer. From the paper - \"This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information\".\n",
    "\n",
    "Cross channel information learning (cascaded 1x1 convolution) is biologically inspired because human visual cortex have receptive fields (kernels) tuned to different orientation. For e.g \n",
    "\n",
    "![different orientation tuned receptive field profiles in the human visual cortex](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/RotBundleFiltersListPlot3D.gif)\n",
    "\n",
    "\n",
    "\n",
    "**Different orientation tuned receptive field profiles in the human visual cortex** [Source](http://bmia.bmt.tue.nl/education/courses/fev/course/notebooks/Convolution.html)\n",
    "\n",
    "\n",
    "\n",
    "## More Uses\n",
    "\n",
    "  * 1x1 Convolution can be combined with Max pooling\n",
    "\n",
    "  ![Pooling with 1x1 convolution](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/numerical_max_pooling.gif)\n",
    "   **Pooling with 1x1 convolution**\n",
    "  <br />\n",
    "\n",
    "  * 1x1 Convolution with higher strides leads to even more redution in data by decreasing resolution, while losing very little non-spatially correlated information.\n",
    "\n",
    "  ![1x1 convolution with strides](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/no_padding_strides.gif)\n",
    "   **1x1 convolution with strides**\n",
    "   <br />\n",
    "\n",
    "  * Replace fully connected layers with 1x1 convolutions as Yann LeCun believes they are the same -\n",
    "> In Convolutional Nets, there is no such thing as \"fully-connected layers\". There are only convolution layers with 1x1 convolution kernels and a full connection table.\n",
    "-- [Yann LeCun](https://www.facebook.com/yann.lecun/posts/10152820758292143)\n",
    "  <br />\n",
    "\n",
    "*Convolution gif images generated using [this wonderful code](https://github.com/vdumoulin/conv_arithmetic), more images on 1x1 convolutions and 3x3 convolutions can be* [found here](http://gpgpu.cs-i.brandeis.edu/convolution_images/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "       BasicConv2d-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "       BasicConv2d-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8          [-1, 192, 56, 56]         110,592\n",
      "       BatchNorm2d-9          [-1, 192, 56, 56]             384\n",
      "      BasicConv2d-10          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 192, 28, 28]               0\n",
      "           Conv2d-12           [-1, 64, 28, 28]          12,288\n",
      "      BatchNorm2d-13           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-14           [-1, 64, 28, 28]               0\n",
      "           Conv2d-15           [-1, 96, 28, 28]          18,432\n",
      "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-17           [-1, 96, 28, 28]               0\n",
      "           Conv2d-18          [-1, 128, 28, 28]         110,592\n",
      "      BatchNorm2d-19          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21           [-1, 16, 28, 28]           3,072\n",
      "      BatchNorm2d-22           [-1, 16, 28, 28]              32\n",
      "      BasicConv2d-23           [-1, 16, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-25           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-26           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-27          [-1, 192, 28, 28]               0\n",
      "           Conv2d-28           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-29           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-30           [-1, 32, 28, 28]               0\n",
      "        Inception-31          [-1, 256, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 192, 28, 28]         221,184\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "      BasicConv2d-40          [-1, 192, 28, 28]               0\n",
      "           Conv2d-41           [-1, 32, 28, 28]           8,192\n",
      "      BatchNorm2d-42           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-43           [-1, 32, 28, 28]               0\n",
      "           Conv2d-44           [-1, 96, 28, 28]          27,648\n",
      "      BatchNorm2d-45           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-46           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48           [-1, 64, 28, 28]          16,384\n",
      "      BatchNorm2d-49           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-50           [-1, 64, 28, 28]               0\n",
      "        Inception-51          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-52          [-1, 480, 14, 14]               0\n",
      "           Conv2d-53          [-1, 192, 14, 14]          92,160\n",
      "      BatchNorm2d-54          [-1, 192, 14, 14]             384\n",
      "      BasicConv2d-55          [-1, 192, 14, 14]               0\n",
      "           Conv2d-56           [-1, 96, 14, 14]          46,080\n",
      "      BatchNorm2d-57           [-1, 96, 14, 14]             192\n",
      "      BasicConv2d-58           [-1, 96, 14, 14]               0\n",
      "           Conv2d-59          [-1, 208, 14, 14]         179,712\n",
      "      BatchNorm2d-60          [-1, 208, 14, 14]             416\n",
      "      BasicConv2d-61          [-1, 208, 14, 14]               0\n",
      "           Conv2d-62           [-1, 16, 14, 14]           7,680\n",
      "      BatchNorm2d-63           [-1, 16, 14, 14]              32\n",
      "      BasicConv2d-64           [-1, 16, 14, 14]               0\n",
      "           Conv2d-65           [-1, 48, 14, 14]           6,912\n",
      "      BatchNorm2d-66           [-1, 48, 14, 14]              96\n",
      "      BasicConv2d-67           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-68          [-1, 480, 14, 14]               0\n",
      "           Conv2d-69           [-1, 64, 14, 14]          30,720\n",
      "      BatchNorm2d-70           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-71           [-1, 64, 14, 14]               0\n",
      "        Inception-72          [-1, 512, 14, 14]               0\n",
      "           Conv2d-73            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
      "      BasicConv2d-75            [-1, 128, 4, 4]               0\n",
      "           Linear-76                 [-1, 1024]       2,098,176\n",
      "           Linear-77                 [-1, 1000]       1,025,000\n",
      "     InceptionAux-78                 [-1, 1000]               0\n",
      "           Conv2d-79          [-1, 160, 14, 14]          81,920\n",
      "      BatchNorm2d-80          [-1, 160, 14, 14]             320\n",
      "      BasicConv2d-81          [-1, 160, 14, 14]               0\n",
      "           Conv2d-82          [-1, 112, 14, 14]          57,344\n",
      "      BatchNorm2d-83          [-1, 112, 14, 14]             224\n",
      "      BasicConv2d-84          [-1, 112, 14, 14]               0\n",
      "           Conv2d-85          [-1, 224, 14, 14]         225,792\n",
      "      BatchNorm2d-86          [-1, 224, 14, 14]             448\n",
      "      BasicConv2d-87          [-1, 224, 14, 14]               0\n",
      "           Conv2d-88           [-1, 24, 14, 14]          12,288\n",
      "      BatchNorm2d-89           [-1, 24, 14, 14]              48\n",
      "      BasicConv2d-90           [-1, 24, 14, 14]               0\n",
      "           Conv2d-91           [-1, 64, 14, 14]          13,824\n",
      "      BatchNorm2d-92           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-93           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-94          [-1, 512, 14, 14]               0\n",
      "           Conv2d-95           [-1, 64, 14, 14]          32,768\n",
      "      BatchNorm2d-96           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-97           [-1, 64, 14, 14]               0\n",
      "        Inception-98          [-1, 512, 14, 14]               0\n",
      "           Conv2d-99          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108           [-1, 24, 14, 14]          12,288\n",
      "     BatchNorm2d-109           [-1, 24, 14, 14]              48\n",
      "     BasicConv2d-110           [-1, 24, 14, 14]               0\n",
      "          Conv2d-111           [-1, 64, 14, 14]          13,824\n",
      "     BatchNorm2d-112           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-113           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-114          [-1, 512, 14, 14]               0\n",
      "          Conv2d-115           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-116           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-117           [-1, 64, 14, 14]               0\n",
      "       Inception-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          57,344\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "     BasicConv2d-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 144, 14, 14]          73,728\n",
      "     BatchNorm2d-123          [-1, 144, 14, 14]             288\n",
      "     BasicConv2d-124          [-1, 144, 14, 14]               0\n",
      "          Conv2d-125          [-1, 288, 14, 14]         373,248\n",
      "     BatchNorm2d-126          [-1, 288, 14, 14]             576\n",
      "     BasicConv2d-127          [-1, 288, 14, 14]               0\n",
      "          Conv2d-128           [-1, 32, 14, 14]          16,384\n",
      "     BatchNorm2d-129           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-130           [-1, 32, 14, 14]               0\n",
      "          Conv2d-131           [-1, 64, 14, 14]          18,432\n",
      "     BatchNorm2d-132           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-133           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-134          [-1, 512, 14, 14]               0\n",
      "          Conv2d-135           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-136           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-137           [-1, 64, 14, 14]               0\n",
      "       Inception-138          [-1, 528, 14, 14]               0\n",
      "          Conv2d-139            [-1, 128, 4, 4]          67,584\n",
      "     BatchNorm2d-140            [-1, 128, 4, 4]             256\n",
      "     BasicConv2d-141            [-1, 128, 4, 4]               0\n",
      "          Linear-142                 [-1, 1024]       2,098,176\n",
      "          Linear-143                 [-1, 1000]       1,025,000\n",
      "    InceptionAux-144                 [-1, 1000]               0\n",
      "          Conv2d-145          [-1, 256, 14, 14]         135,168\n",
      "     BatchNorm2d-146          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-147          [-1, 256, 14, 14]               0\n",
      "          Conv2d-148          [-1, 160, 14, 14]          84,480\n",
      "     BatchNorm2d-149          [-1, 160, 14, 14]             320\n",
      "     BasicConv2d-150          [-1, 160, 14, 14]               0\n",
      "          Conv2d-151          [-1, 320, 14, 14]         460,800\n",
      "     BatchNorm2d-152          [-1, 320, 14, 14]             640\n",
      "     BasicConv2d-153          [-1, 320, 14, 14]               0\n",
      "          Conv2d-154           [-1, 32, 14, 14]          16,896\n",
      "     BatchNorm2d-155           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-156           [-1, 32, 14, 14]               0\n",
      "          Conv2d-157          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-158          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-159          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-160          [-1, 528, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          67,584\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-163          [-1, 128, 14, 14]               0\n",
      "       Inception-164          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-165            [-1, 832, 7, 7]               0\n",
      "          Conv2d-166            [-1, 256, 7, 7]         212,992\n",
      "     BatchNorm2d-167            [-1, 256, 7, 7]             512\n",
      "     BasicConv2d-168            [-1, 256, 7, 7]               0\n",
      "          Conv2d-169            [-1, 160, 7, 7]         133,120\n",
      "     BatchNorm2d-170            [-1, 160, 7, 7]             320\n",
      "     BasicConv2d-171            [-1, 160, 7, 7]               0\n",
      "          Conv2d-172            [-1, 320, 7, 7]         460,800\n",
      "     BatchNorm2d-173            [-1, 320, 7, 7]             640\n",
      "     BasicConv2d-174            [-1, 320, 7, 7]               0\n",
      "          Conv2d-175             [-1, 32, 7, 7]          26,624\n",
      "     BatchNorm2d-176             [-1, 32, 7, 7]              64\n",
      "     BasicConv2d-177             [-1, 32, 7, 7]               0\n",
      "          Conv2d-178            [-1, 128, 7, 7]          36,864\n",
      "     BatchNorm2d-179            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-180            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-181            [-1, 832, 7, 7]               0\n",
      "          Conv2d-182            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-183            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-184            [-1, 128, 7, 7]               0\n",
      "       Inception-185            [-1, 832, 7, 7]               0\n",
      "          Conv2d-186            [-1, 384, 7, 7]         319,488\n",
      "     BatchNorm2d-187            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-188            [-1, 384, 7, 7]               0\n",
      "          Conv2d-189            [-1, 192, 7, 7]         159,744\n",
      "     BatchNorm2d-190            [-1, 192, 7, 7]             384\n",
      "     BasicConv2d-191            [-1, 192, 7, 7]               0\n",
      "          Conv2d-192            [-1, 384, 7, 7]         663,552\n",
      "     BatchNorm2d-193            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-194            [-1, 384, 7, 7]               0\n",
      "          Conv2d-195             [-1, 48, 7, 7]          39,936\n",
      "     BatchNorm2d-196             [-1, 48, 7, 7]              96\n",
      "     BasicConv2d-197             [-1, 48, 7, 7]               0\n",
      "          Conv2d-198            [-1, 128, 7, 7]          55,296\n",
      "     BatchNorm2d-199            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-200            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-201            [-1, 832, 7, 7]               0\n",
      "          Conv2d-202            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-203            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-204            [-1, 128, 7, 7]               0\n",
      "       Inception-205           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-206           [-1, 1024, 1, 1]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 13,004,888\n",
      "Trainable params: 13,004,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 94.25\n",
      "Params size (MB): 49.61\n",
      "Estimated Total Size (MB): 144.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False, init_weights=True):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        if self.training and self.aux_logits:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if self.training and self.aux_logits:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "#         if self.training and self.aux_logits:\n",
    "#             return _GoogLeNetOuputs(x, aux2, aux1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 2048\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 2048\n",
    "        x = self.fc2(x)\n",
    "        # N x 1024\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "model = GoogLeNet()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks (ResNet)\n",
    "\n",
    "As we design increasingly deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network. Even more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different. To make some progress we need a bit of theory. \n",
    "\n",
    "## Function Classes\n",
    "\n",
    "Consider $\\mathcal{F}$, the class of functions that a specific network architecture (together with learning rates and other hyperparameter settings) can reach. That is, for all $f \\in \\mathcal{F}$ there exists some set of parameters $W$ that can be obtained through training on a suitable dataset. Let's assume that $f^*$ is the function that we really would like to find. If it's in $\\mathcal{F}$, we're in good shape but typically we won't be quite so lucky. Instead, we will try to find some $f^*_\\mathcal{F}$ which is our best bet within $\\mathcal{F}$. For instance, we might try finding it by solving the following optimization problem:\n",
    "\n",
    "$$f^*_\\mathcal{F} := \\mathop{\\mathrm{argmin}}_f L(X, Y, f) \\text{ subject to } f \\in \\mathcal{F}$$\n",
    "\n",
    "It is only reasonable to assume that if we design a different and more powerful architecture $\\mathcal{F}'$ we should arrive at a better outcome. In other words, we would expect that $f^*_{\\mathcal{F}'}$ is 'better' than $f^*_{\\mathcal{F}}$. However, if $\\mathcal{F} \\not\\subseteq \\mathcal{F}'$ there is no guarantee that this should even happen. In fact, $f^*_{\\mathcal{F}'}$ might well be worse. This is a situation that we often encounter in practice - adding layers doesn't only make the network more expressive, it also changes it in sometimes not quite so predictable ways. The picture below illustrates this in slightly abstract terms. \n",
    "\n",
    "<img src=\"images/functionclasses.svg\" alt=\"\">\n",
    "\n",
    "\n",
    "Only if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. This is the question that He et al, 2016 considered when working on very deep computer vision models. At the heart of ResNet is the idea that every additional layer should contain the identity function as one of its elements. This means that if we can train the newly-added layer into an identity mapping $f(\\mathbf{x}) = \\mathbf{x}$, the new model will be as effective as the original model. As the new model may get a better solution to fit the training data set, the added layer might make it easier to reduce training errors. Even better, the identity function rather than the null $f(\\mathbf{x}) = 0$ should be the the simplest function within a layer. \n",
    "\n",
    "These considerations are rather profound but they led to a surprisingly simple solution, a residual block. With it, [He et al, 2015](https://arxiv.org/abs/1512.03385) won the ImageNet Visual Recognition Challenge in 2015. The design had a profound influence on how to build deep neural networks.\n",
    "\n",
    "\n",
    "### Residual Blocks\n",
    "\n",
    "Let us focus on a local neural network, as depicted below. Denote the input by $\\mathbf{x}$. We assume that the ideal mapping we want to obtain by learning is $f(\\mathbf{x})$, to be used as the input to the activation function. The portion within the dotted-line box in the left image must directly fit the mapping $f(\\mathbf{x})$. This can be tricky if we don't need that particular layer and we would much rather retain the input $\\mathbf{x}$. The portion within the dotted-line box in the right image now only needs to parametrize the *deviation* from the identity, since we return $\\mathbf{x} + f(\\mathbf{x})$. In practice, the residual mapping is often easier to optimize. We only need to set $f(\\mathbf{x}) = 0$. The right image in the figure below illustrates the basic Residual Block of ResNet. Similar architectures were later proposed for sequence models which we will study later. \n",
    "\n",
    "<img src=\"images/residual-block.svg\" alt=\"\">\n",
    "\n",
    "ResNet follows VGG's full $3\\times 3$ convolutional layer design. The residual block has two $3\\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. This kind of design requires that the output of the two convolutional layers be of the same shape as the input, so that they can be added together. If we want to change the number of channels or the the stride, we need to introduce an additional $1\\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. Let's have a look at the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity, and whenever `use_1x1conv=True`, one where we adjust channels and resolution by means of a $1 \\times 1$ convolution before adding. The diagram below illustrates this:\n",
    "\n",
    "<img src=\"images/ResNetBlock.svg\" alt=\"\">\n",
    "\n",
    "Now let us look at a situation where the input and output are of the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Learning\n",
    "It is possible to fit an desired underlying mapping $H(x)$ by a few stacked nonlinear layers, so they can also fit an another underlying mapping $F(x)=H(x)−x$. As a result, it is possible to reformulate it to $H(x)=F(x)+x$, which consists of the Residual Function $F(x)$ and input $x$. The connection of the input to the output is called a skipt connection or identity mapping. The general idea is that if multiple nonlinear layers can approximate the complicated function $H(x)$, then it is possible for them to approximate the residual function $F(x)$. Therefore the stacked layers are not used to fit $H(x)$, instead these layers approximate the residual function $F(x)$. Both forms should be able to fit the underlying mapping.\n",
    "\n",
    "<img src=\"images/residual_building_block.png\" alt=\"\">\n",
    "\n",
    "One reason for the degradation problem could be the difficulties in approximating identity mappings by nonlinear layers. The reformulation used identity mapping as a reference and let the residual function represent the perturbations. The identity mapping can be generated by the solver through driving the weights of the residual function to zero if need be.\n",
    "\n",
    "### Implementation\n",
    "Residual learning is implented to every few stacked layers. Figure 2 shows an example of 2 layers. As an example, formulation (1) can be defined as:\n",
    "\n",
    "(1)                                $$F(x)=W_2σ(W_1x)+x $$\n",
    "\n",
    "Where W1 and W2 are the weights for the convolutinoal layers and σ is the activation function, in this case a RELU function. The operation F+x is realized by a shortcut connection and element-wise addition. The addition is followed by an activation function σ.\n",
    "\n",
    "The resulting formulation for a residual block is:\n",
    "\n",
    "(2)                                $$y(x)=σ(W_2σ(W_1x)+x) .$$\n",
    "\n",
    "After each convolution (weight) layer a batch normalization method (BN) is adopted. The training of the network is achiebed by stochastic gradient descent (SGD) with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus. The weight decay rate is 0.0001 and has a value of 0.9. (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Network\n",
    "To convert the plain model to the residual version, shortcut connections are added, as demonstrated in the figure 3 (right). The solid line shortcuts are identity mapping. When the dimensions increases there are 2 options (dotted line shortcut):\n",
    "\n",
    "1. The shortcut still performs identity mapping with zero padding to increasing the dimensions or\n",
    "\n",
    "2. the shortcut is used to match dimensions utilizing 1x1 convolution.\n",
    "\n",
    "In both options, when the shortcut go across feature maps of different sizes, they used a stride of 2. Generally the second option is used.(1)\n",
    "\n",
    "<img src=\"images/residual_block.png\" alt=\"\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]              32\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "            Conv2d-3           [-1, 16, 28, 28]           2,320\n",
      "       BatchNorm2d-4           [-1, 16, 28, 28]              32\n",
      "              ReLU-5           [-1, 16, 28, 28]               0\n",
      "            Conv2d-6           [-1, 16, 28, 28]           2,320\n",
      "       BatchNorm2d-7           [-1, 16, 28, 28]              32\n",
      "              ReLU-8           [-1, 16, 28, 28]               0\n",
      "          ResBlock-9           [-1, 16, 28, 28]               0\n",
      "           Conv2d-10           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-11           [-1, 16, 28, 28]              32\n",
      "             ReLU-12           [-1, 16, 28, 28]               0\n",
      "           Conv2d-13           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-14           [-1, 16, 28, 28]              32\n",
      "             ReLU-15           [-1, 16, 28, 28]               0\n",
      "         ResBlock-16           [-1, 16, 28, 28]               0\n",
      "           Conv2d-17           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-18           [-1, 16, 28, 28]              32\n",
      "             ReLU-19           [-1, 16, 28, 28]               0\n",
      "           Conv2d-20           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-21           [-1, 16, 28, 28]              32\n",
      "             ReLU-22           [-1, 16, 28, 28]               0\n",
      "         ResBlock-23           [-1, 16, 28, 28]               0\n",
      "           Conv2d-24           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-25           [-1, 16, 28, 28]              32\n",
      "             ReLU-26           [-1, 16, 28, 28]               0\n",
      "           Conv2d-27           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-28           [-1, 16, 28, 28]              32\n",
      "             ReLU-29           [-1, 16, 28, 28]               0\n",
      "         ResBlock-30           [-1, 16, 28, 28]               0\n",
      "           Conv2d-31           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-32           [-1, 16, 28, 28]              32\n",
      "             ReLU-33           [-1, 16, 28, 28]               0\n",
      "           Conv2d-34           [-1, 16, 28, 28]           2,320\n",
      "      BatchNorm2d-35           [-1, 16, 28, 28]              32\n",
      "             ReLU-36           [-1, 16, 28, 28]               0\n",
      "         ResBlock-37           [-1, 16, 28, 28]               0\n",
      "        AvgPool2d-38             [-1, 16, 1, 1]               0\n",
      "             ReLU-39             [-1, 16, 1, 1]               0\n",
      "           Linear-40                   [-1, 10]             170\n",
      "================================================================\n",
      "Total params: 23,722\n",
      "Trainable params: 23,722\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.54\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 3.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels, kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.batch1 = nn.BatchNorm2d(nb_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels, kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.batch2 = nn.BatchNorm2d(nb_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.batch1(self.conv1(x))\n",
    "        y = self.relu1(y)\n",
    "        y = self.batch2(self.conv2(y))\n",
    "        y += x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size, nb_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, nb_channels, kernel_size=1)\n",
    "        \n",
    "        self.resblocks = nn.Sequential(*(ResBlock(nb_channels, kernel_size) for _ in range(nb_blocks)))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg = nn.AvgPool2d(kernel_size=28)\n",
    "        self.fc = nn.Linear(nb_channels, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv0(x))\n",
    "        x = self.resblocks(x)\n",
    "        x = self.relu(self.avg(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet(16, 3, 5)\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(train_loader, test_loader, model, num_epochs, batch_size, 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Networks\n",
    "Before talking about Highway Networks, Let’s start with plain network which consists of $L$ layers where the $l-th$ layer (with omitting the symbol for layer):\n",
    "\n",
    "\n",
    "$$y=H(x, W_H)$$\n",
    "\n",
    "\n",
    "where $x$ is input, $W_H$ is the weight, $H$ is the transform function followed by an activation function and $y$ is the output. And for $i-th$ unit:\n",
    "\n",
    "$$y_i=H_i(x)$$\n",
    "\n",
    "We compute the $y_i$ and pass it to next layer.\n",
    "\n",
    "\n",
    "The plain networks are adopted from the VGG nets (Figure 3(left)). The convolutional layers have mostly 3x3 filters and the design follows two rules:\n",
    "1. For the same output feature map size, the layers have the same number of filters, and\n",
    "\n",
    "2. if the feature map size is halved, the number of filters is doubled in order to preserve the time complexity per layer.\n",
    "\n",
    "The downsampling operation is performed by the convolutional layers that have a stride of 2, hence no pooling layers. The network ends with a global average pooling layer and a 1000-way fully connected layer with softmax function.\n",
    "\n",
    "Figure 3 (middle) shows a plain model with 34 layers. (1)\n",
    "\n",
    "<img src=\"images/residualnet_34.png\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/highway.png\" alt=\"\">\n",
    "\n",
    "\n",
    "In highway network, two non-linear transforms $T$ and $C$ are introduced:\n",
    "\n",
    "$$y= H(x, W_H).T(x, W_T) + x . C(x, W_C)$$\n",
    "\n",
    "where $T$ is the Transform Gate and $C$ is the Carry Gate. In particular, $C = 1 - T$:\n",
    "\n",
    "$$y= H(x, W_H).T(x, W_T) + x . (1 - T(x, W_T))$$\n",
    "\n",
    "We can have below conditions for particular T values:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}y =\n",
    "\\begin{cases}\n",
    "x & \\text{if } T(x, W_T) = 0 \\\\\n",
    "H(x, W_H) & \\text{if } T(x, W_T) = 1\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "When $T=0$, we pass the input as output directly which creates an information highway. That’s why it is called Highway Network !!!. When $T=1$, we use the non-linear activated transformed input as output. Here, in contrast to the $i-th$ unit in plain network, authors introduce the block concept. For $i-th$ block, there is a block state $H_i(x)$, and and transform gate output $T_i(x)$. And the corresponding block output $y_i$:\n",
    "\n",
    "$$y_i= H_i(x, W_H)*T_i(x, W_T) + x_i . (1 - T_i(x))$$\n",
    "\n",
    "which is connected to the next layer. Formally, $T(x)$ is the sigmoid function:\n",
    "\n",
    "$$T(x) = \\sigma(W^T_Tx + b_T)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 784]         615,440\n",
      "           Sigmoid-2                  [-1, 784]               0\n",
      "            Linear-3                  [-1, 784]         615,440\n",
      "              ReLU-4                  [-1, 784]               0\n",
      "            Linear-5                  [-1, 784]         615,440\n",
      "           Sigmoid-6                  [-1, 784]               0\n",
      "            Linear-7                  [-1, 784]         615,440\n",
      "              ReLU-8                  [-1, 784]               0\n",
      "            Linear-9                  [-1, 784]         615,440\n",
      "          Sigmoid-10                  [-1, 784]               0\n",
      "           Linear-11                  [-1, 784]         615,440\n",
      "             ReLU-12                  [-1, 784]               0\n",
      "           Linear-13                   [-1, 10]           7,850\n",
      "================================================================\n",
      "Total params: 3,700,490\n",
      "Trainable params: 3,700,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 14.12\n",
      "Estimated Total Size (MB): 14.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, size=784, num_layers=3):\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in range(self.num_layers):\n",
    "            T = self.sigmoid(self.gate[layer](x))\n",
    "            H = self.relu(self.nonlinear[layer](x))\n",
    "            x = T * H + (1 - T) * x\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "model = Highway()\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densely Connected Networks (DenseNet)\n",
    "\n",
    "ResNet significantly changed the view of how to parametrize the functions in deep networks. DenseNet is to some extent the logical extension of this. To understand how to arrive at it, let's take a small detour to theory. Recall the Taylor expansion for functions. For scalars it can be written as\n",
    "\n",
    "$$f(x) = f(0) + f'(x) x + \\frac{1}{2} f''(x) x^2 + \\frac{1}{6} f'''(x) x^3 + o(x^3)$$\n",
    "\n",
    "## Function Decomposition\n",
    "\n",
    "The key point is that it decomposes the function into increasingly higher order terms. In a similar vein, ResNet decomposes functions into \n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})$$\n",
    "\n",
    "That is, ResNet decomposes $f$ into a simple linear term and a more complex nonlinear one. What if we want to go beyond two terms? A solution was proposed by [Huang et al, 2016](https://arxiv.org/abs/1608.06993) in the form of DenseNet, an architecture that reported record performance on the ImageNet dataset. \n",
    "\n",
    "<img src=\"images/densenet.svg\" >\n",
    "\n",
    "The key difference between ResNet and DenseNet is that in the latter case outputs are *concatenated* rather than added. As a result we perform a mapping from $\\mathbf{x}$ to its values after applying an increasingly complex sequence of functions.\n",
    "\n",
    "$$\\mathbf{x} \\to \\left[\\mathbf{x}, f_1(\\mathbf{x}), f_2(\\mathbf{x}, f_1(\\mathbf{x})), f_3(\\mathbf{x}, f_1(\\mathbf{x}), f_2(\\mathbf{x}, f_1(\\mathbf{x})), \\ldots\\right]$$\n",
    "\n",
    "<img src=\"images/DenseNetDense.svg\" >\n",
    "\n",
    "In the end, all these functions are combined in an MLP to reduce the number of features again. In terms of implementation this is quite simple - rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The last layer of such a chain is densely connected to all previous layers. The main components that compose a DenseNet are dense blocks and transition layers. The former defines how the inputs and outputs are concatenated, while the latter controls the number of channels so that it is not too large.\n",
    "\n",
    "\n",
    "<img src=\"images/densenet2.jpg\" width=\"60%\">\n",
    "\n",
    "<img src=\"images/densenet3.jpg\" width=\"80%\">\n",
    "\n",
    "\n",
    "## Dense Blocks\n",
    "\n",
    "DenseNet uses the modified \"batch normalization, activation, and convolution\" architecture of ResNet (see the exercise in the [previous section](resnet.md)). First, we implement this architecture in the `conv_block` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.norm(self.conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense block consists of multiple `ConvBlock` units, each using the same number of output channels. In the forward computation, however, we concatenate the input and output of each block on the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, nb_convs, nb_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.net = nn.Sequential(*(ConvBlock(nb_channels, nb_channels) for _ in range(nb_convs)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel\n",
    "            # dimension\n",
    "            X = torch.cat(X, Y, dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we define a convolution block with two blocks of 10 output channels. When using an input with 3 channels, we will get an output with the $3+2\\times 10=23$ channels. The number of convolution block channels controls the increase in the number of output channels relative to the number of input channels. This is also referred to as the growth rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transition Layers\n",
    "\n",
    "Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A transition layer is used to control the complexity of the model. It reduces the number of channels by using the $1\\times 1$ convolutional layer and halves the height and width of the average pooling layer with a stride of 2, further reducing the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(self.relu(self.norm(self.conv(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a transition layer with 10 channels to the output of the dense block in the previous example.  This reduces the number of output channels to 10, and halves the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051f8ed7f0c93aa1aa30fb278ee333bc108b14f52b0c39d794e507a7c646268f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
